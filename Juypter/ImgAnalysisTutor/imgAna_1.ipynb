{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiRecentPhase1 - \n",
    "\n",
    "This sample demostrates creating a Streams Python application to perform some analytics and viewing the results.\n",
    "The data source is live SSE feed, provided by Wikipedia, that publishes updates as they occur in the wikipedia universe. \n",
    "\n",
    "## Continious processing with Streams.\n",
    "\n",
    "The [WikiRecentPhase0](./WikiRecentPhase0.ipynb) illustrated accessing continious streams events from a notebook. \n",
    "Data is collected, averages are calculated as long as the notebook open, rather inconvient to keep for the \n",
    "workstation user. Running a notebook continuously is feaseable but fraught with issues.  Better, have the server stage the events in order they can be rendered or processed as demand. ~~Best, stage the events and do the aggregation.~~\n",
    "This processes events as long as browser is open, \n",
    "\n",
    "## Overview \n",
    "**About the sample** \n",
    "The appliction recieves wikipedia updates via a SSE feed that transmits updates as the occur on the wikipedia site. \n",
    "The updates a filters and rendered and viewed using Pandas.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "The Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to retrieve the results.\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Submit the appplication that has a filter. \n",
    "\n",
    "- rough cut look at the data\n",
    "- plot the 'type' data\n",
    "- look at the filtered data\n",
    "\n",
    "##  Collect in buffer / Aggregate\n",
    "- Last 1000 in local buffer & aggregate\n",
    "- Render\n",
    "- Push code to server.\n",
    "\n",
    "## Links\n",
    "[Topology](https://streamsxtopology.readthedocs.io/en/latest/index.html) documentation.\n",
    "[Widgets](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Basics.html)\n",
    "\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Setup\n",
    "### 1.1 Add credentials for the IBM Streams service\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verify `streamsx` package version\n",
    "\n",
    "Run the cell below to check which version of the `streamsx` package is installed.  \n",
    "\n",
    "If you need to upgrade,\n",
    "- Use `!pip install --user --upgrade streamsx` to upgrade the package. \n",
    "- Or, use  `!pip install --user streamsx==somever` to install a specific version of the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install components\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.topology.context\n",
    "print(\"streamsx package version: \" + streamsx.topology.context.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ? Explain this or just say it's imports were using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load common_imports.py\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "\n",
    "import streamsx.topology.context\n",
    "print(\"streamsx package version: \" + streamsx.topology.context.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions for Jupyter \n",
    "Make interacting with the Streams data friendlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    .\"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt\n",
    "\n",
    "\n",
    "def display_view_stop(eventView, period=2):\n",
    "    \"\"\"Wrapper for streamsx.rest_primitives.View.display() to have button. \"\"\"\n",
    "    button =  widgets.Button(description=\"Stop Updating\")\n",
    "    display(button)\n",
    "    eventView.display(period=period) \n",
    "    def on_button_clicked(b):\n",
    "        eventView.stop_data_fetch()\n",
    "        b.description = \"Stopped\"\n",
    "    button.on_click(on_button_clicked)\n",
    "\n",
    "def view_events(views):\n",
    "    \"\"\"\n",
    "    Build interface to display a list of views and \n",
    "    display view when selected from list.\n",
    "     \n",
    "    \"\"\"\n",
    "    view_names = [view.name for view in views]\n",
    "    nameView = dict(zip(view_names, views))    \n",
    "    select = widgets.RadioButtons(\n",
    "        options = view_names,\n",
    "        value = None,\n",
    "        description = 'Select view to display',\n",
    "        disabled = False\n",
    "    )\n",
    "    def on_change(b):\n",
    "        if (b['name'] == 'label'):\n",
    "            clear_output(wait=True)\n",
    "            [view.stop_data_fetch() for view in views ]\n",
    "            display(select)\n",
    "            display_view_stop(nameView[b['new']], period=2)\n",
    "    select.observe(on_change)\n",
    "    display(select)\n",
    "\n",
    "def find_job(instance, job_name=None):\n",
    "    \"\"\"locate job within instance\"\"\"\n",
    "    for job in instance.get_jobs():    \n",
    "        if job.applicationName.split(\"::\")[-1] == job_name:\n",
    "            return job\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_view(instance, job_name=None, view_name=\"view\"):\n",
    "    job = find_job(instance, job_name)\n",
    "    return job.get_views(view_name)\n",
    "    \n",
    "\n",
    "def display_views(instance, job_name):\n",
    "    \"Locate/promote and display all views of a job\"\n",
    "    job = find_job(instance, job_name=job_name)\n",
    "    if job is None:\n",
    "        print(\"Failed to locate job\")\n",
    "    else:\n",
    "        views = job.get_views()\n",
    "        view_events(views)\n",
    "        \n",
    "def list_cancel_jobs(_instance=None):\n",
    "    \"\"\"\n",
    "    Interactive selection of jobs to cancel.\n",
    "    \n",
    "    Prompts with SelectMultiple widget, if thier are no jobs, your presente with a blank list.\n",
    "    \n",
    "    \"\"\"\n",
    "    active_jobs = { \"{}:{}\".format(job.name, job.health):job for job in _instance.get_jobs()}\n",
    "\n",
    "    selectMultiple_jobs = widgets.SelectMultiple(\n",
    "        options=active_jobs.keys(),\n",
    "        value=[],\n",
    "        rows=len(active_jobs),\n",
    "        description=\"Select job(s) to cancel\",\n",
    "        layout=Layout(width='60%')\n",
    "    )\n",
    "    cancel_jobs = widgets.ToggleButton(\n",
    "        value=False,\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Delete selected jobs',\n",
    "        icon=\"stop\"\n",
    "    )\n",
    "    def on_value_change(change):\n",
    "        for job in selectMultiple_jobs.value:\n",
    "            print(\"canceling job:\", job, active_jobs[job].cancel())\n",
    "        cancel_jobs.disabled = True\n",
    "        selectMultiple_jobs.disabled = True\n",
    "\n",
    "    cancel_jobs.observe(on_value_change, names='value')\n",
    "\n",
    "    return HBox([selectMultiple_jobs, cancel_jobs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the server :  ICP4D or Cloud instance -\n",
    "Attempt to import if fails the cfg will not be defined we know were using \n",
    "Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICP4D  - injected by 'Connected to instance' menu item\n",
    "from streamsx.topology import context\n",
    "try:\n",
    "    from icpd_core import icpd_util\n",
    "    cfg = icpd_util.get_service_instance_details(name='sample-icp1')\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    instance = rest.Instance.of_service(cfg)\n",
    "    print(\"Within ICP4D\")\n",
    "except ImportError:\n",
    "    cfg = None\n",
    "    print(\"Outside ICP4D\")\n",
    "# ICP4D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable 'InsecureRequestWarning'  - must be put after startup\n",
    "if cfg is not None:\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT ICP4D : cloud access - \n",
    "if cfg is None:\n",
    "    import credential   # remove @ ICP4D\n",
    "    import common\n",
    "    # TODO * check if instance is up\n",
    "    # - link up to first cell (can you do a test and execute)\n",
    "    sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', vcap_services={'streaming-analytics':[{'name':'Streaming3Turbine','credentials':credential.streaming3Turbine}]})\n",
    "    instance = sc.get_instances()[0]\n",
    "    # Render the views....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List jobs to cancel....\n",
    "This page will submit a job named 'WikiPhase1'. If it's running you'll want to cancel it before submitting a new version. If it is running, no need to cancel/submit you can just procede to the [Viewing data section](#viewingData).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cancel_jobs(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing the Streams application\n",
    "- get data from wiki usng SSE\n",
    "- filter data, seperate out the humans and RObots\n",
    "- setup views : allEvents, allHumans, paredHumans, paredAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive messages updates from Wikipedia\n",
    "As updates are made to Wikipidia pages the changs are sent over and SSE feed. The get_events() function recieves the events and acting as a [source](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Topology.source) pushes them onto the Streams streasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"fetch recent changes from wikievents site using SSE\"\"\"\n",
    "    for change in EventSource('https://stream.wikimedia.org/v2/stream/recentchange'):\n",
    "        if len(change.data):\n",
    "            try:\n",
    "                obj = json.loads(change.data)\n",
    "            except json.JSONDecodeError as err:\n",
    "                print(\"JSON l1 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            except json.decoder.JSONDecodeError as err:\n",
    "                print(\"JSON l2 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            else:\n",
    "                yield(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter messages\n",
    "The [filter](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.filter) is used to break out messages not generated by robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View into the live stream\n",
    "The [view](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.view) enables access to live stream at runtime. We spread them liberaly throughout the application to observe how the processing is procedeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='composeBuildSubmit'></a>\n",
    "## Compose, build and submit the Streams application.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WikiPhase1(jobName=None):\n",
    "    \"\"\"\n",
    "    Compose, build and submit topology. \n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    topo.add_pip_package('sseclient')\n",
    "\n",
    "    ## Receive wiki data - send frequently, only one\n",
    "    source = topo.source(get_events, name=\"WikiFeed\")\n",
    "    allEvents = source.view(buffer_time=1.0, sample_size=1, name=\"allEvents\", description=\"All wiki events\")\n",
    "    \n",
    "      ## Filter out bots only humans\n",
    "    allHumans_ = source.filter(lambda x: x['bot'] is False, name='humansOnlyFilter')\n",
    "    allHumans = allHumans_.view(buffer_time=1.0, sample_size=5, name=\"allHumans\", description=\"All human events\")\n",
    "\n",
    "    ## Pare/Reduce the information we move around about humans.\n",
    "    # paredHumans_ = allHumans_.map(lambda x : {'timestamp':x['timestamp']},name=\"pared fields\")\n",
    "    paredHumans_ = allHumans_.map(lambda x : {'timestamp':x['timestamp'],'type':x['type'],'wiki':x['wiki'],'user':x['user'],'title':x['title']}, name=\"paredFields\")\n",
    "    paredHumans = paredHumans_.view(buffer_time=1.0, sample_size=200, name=\"paredHumans\", description=\"Human events pared\")\n",
    "\n",
    "   ## Pare/Reduce the complete set of data. \n",
    "    paredAll_ = source.map(lambda x : {'timestamp':x['timestamp'],'type':x['type'],'wiki':x['wiki'],'bot':x['bot'],'user':x['user'],'title':x['title']}, \n",
    "                        name=\"paredAll\")\n",
    "    paredAll = paredAll_.view(buffer_time=1.0, sample_size=200, name=\"paredAll\", description=\"All events pared\")\n",
    "    \n",
    "    \n",
    "    return ({\"topo\":topo})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting job : ICP or Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg=icpd_util.get_service_instance_details(name='sample-icp1')\n",
    "if cfg is not None:\n",
    "    resp = WikiPhase1(jobName=\"WikiPhase1\")\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "\n",
    "    submission_result = context.submit(\"DISTRIBUTED\",\n",
    "                                   resp['topo'], \n",
    "                                   config=cfg)\n",
    "    # The submission_result object contains information about the running application, or job\n",
    "    if submission_result.job:\n",
    "        print(\"JobId: \", submission_result['id'] , \"Name: \", submission_result['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg is None:\n",
    "    resp = WikiPhase1(jobName=\"WikiPhase1\")\n",
    "    submitStatus = common.submitProcess(topology=resp['topo'],\n",
    "                                        streamsService=\"Streaming3Turbine\",\n",
    "                                        buildType=\"DISTRIBUTED\",\n",
    "                                        serviceType=\"STREAMING_ANALYTIC\",\n",
    "                                        jobName=\"WikiPhase1\",\n",
    "                                        cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the application look like.....\n",
    "\n",
    "Graph of application running, since we're using live data the values on the edges between the nodes\n",
    "will be different but the nodes should be the same. Notice that the count of tuples input port of botFilter is \n",
    "diffent from the output port, only tuples match the critera appear on the output port.\n",
    "\n",
    "**TO Decide**\n",
    "\n",
    "The graph should stay but not telling how to access the console has been squashed, do\n",
    "not show things that are not 'blessed'.\n",
    "\n",
    "I can make this into a multipart gif, which gives a better idea of what is going on. But, not having a the console that they can go to see the live may not be a good idea.\n",
    "\n",
    "![graph of application](images/stillPhase1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stillPhase1.jpg](attachment:stillPhase1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viewingData'></a>\n",
    "## Viewing data \n",
    "\n",
    "The running application has number of views to see what what data is moving through the stream. The following \n",
    "cell will fetch the views' queue and dipslay it's data when selected. \n",
    "\n",
    "|view name | description of data is the view |\n",
    "|---------|-------------|\n",
    "|allEvents  | all fields of all events  |\n",
    "|allHumans | all fields of events where field 'bot' is **False** |\n",
    "|paredAll | subset of fields for all events |\n",
    "|paredHumans | subset of fields of all events where field 'bot is **False**|\n",
    "\n",
    "You want to stop the the fetching the view data when done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the views.....\n",
    "display_views(instance, job_name=\"WikiPhase1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View notes:\n",
    "- constructing the view object : https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html?highlight=view#streamsx.topology.topology.Stream.view\n",
    "- methods on the view object : https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html?highlight=view#streamsx.topology.topology.View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph frequency of 'type' events and 'bots'\n",
    "\n",
    "What bots in relation to type. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tally the the bots/types\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "\n",
    "@catchInterrupt\n",
    "def tally_bot(view, ele_window=20):\n",
    "    nam_list = ['new', 'edit', 'log','categorize']\n",
    "    cntbot = {key:0 for key in nam_list}\n",
    "    cntnobot = {key:0 for key in nam_list}\n",
    "    view.start_data_fetch()\n",
    "\n",
    "    while True:\n",
    "        listTuples= view.fetch_tuples(max_tuples=20, timeout=4)\n",
    "        cntbot = Counter({key:0 for key in nam_list})\n",
    "        cntnobot = Counter({key:0 for key in nam_list})\n",
    "        for evt in listTuples:\n",
    "            if evt['bot']:\n",
    "                cntbot[evt['type']] += 1\n",
    "            else:\n",
    "                cntnobot[evt['type']] += 1\n",
    "        bot_list = [cntbot[key] for key in nam_list]\n",
    "        nobot_list = [cntnobot[key] for key in nam_list]\n",
    "        \n",
    "        df = pd.DataFrame({'bot': bot_list, ' nobot': nobot_list}, index=nam_list)\n",
    "        df.plot.bar(rot=0, stacked=True)\n",
    "        plt.ylim(0.0, ele_window)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "view = get_view(instance, job_name=\"WikiPhase1\", view_name=\"paredAll\")\n",
    "tally_bot(view=view[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph frequency of *bot*less 'type'  : Avg vs instant\n",
    "\n",
    "Display the last set of counts and the average of the last 20 sets.\n",
    "\n",
    "Offload the aggregating of the data to the server. We can connect to the server at\n",
    "anytime to the flow event types. Curently the window is set to 200 which tranlates to\n",
    "approximatly a window of 20 seconds. Setting the value to 1200 will 120 seconds of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate colllections of rows - support\n",
    "\n",
    "class chunking_average:\n",
    "    def __init__(self, init_base, mean_elements=20):\n",
    "        self.deques = {key:deque([0],maxlen=mean_elements) for key in init_base.keys()}\n",
    "\n",
    "    def aggregate(self, chunk):\n",
    "        for key in self.deques.keys():\n",
    "            if self.deques[key] and chunk[key]: self.deques[key].append(chunk[key])\n",
    "        return {key:mean(self.deques[key]) for key in self.deques.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tally the the types\n",
    "\n",
    "@catchInterrupt\n",
    "def tally_types(view, ele_window=20):\n",
    "    global resp\n",
    "    nam_list = ['new', 'edit', 'log','categorize']\n",
    "    cnt = {key:0 for key in nam_list}\n",
    "    run_avg = chunking_average(cnt)\n",
    "    view.start_data_fetch()\n",
    "\n",
    "\n",
    "    while True:\n",
    "        listTuples= view.fetch_tuples(max_tuples=20, timeout=3)\n",
    "        cnt = Counter({key:0 for key in nam_list})\n",
    "        for evt in listTuples:cnt[evt['type']] += 1\n",
    "        avg = run_avg.aggregate(cnt)\n",
    "        evt_list = [cnt[key] for key in nam_list]\n",
    "        avg_list = [avg[key] for key in nam_list]\n",
    "        df = pd.DataFrame({'count': evt_list, 'running avg': avg_list}, index=nam_list)\n",
    "        df.plot.bar(rot=0)\n",
    "        plt.ylim(0.0, ele_window)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "view = get_view(instance, job_name=\"WikiPhase1\", view_name=\"paredHumans\")\n",
    "tally_types(view=view[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
