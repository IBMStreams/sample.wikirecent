{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis : Facial Analyis + Emotion\n",
    "\n",
    "Use the face finder to to find and image to apply \n",
    "\n",
    "- TODO : switch to this : https://gist.github.com/ddebrunner/21db521909accd2ec364861964e18ae3\n",
    "- TODO : use the : __enter__ construct\n",
    "\n",
    "## AKA - WikiRecentPhase5\n",
    "### Use learning models from IBM's [Model Asset Exchange](https://developer.ibm.com/exchanges/models/)\n",
    "\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "Use the  [IBM emotion classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/) to analyise images that are being added to wikiepedia. We've dervived the links to images in the last phase now score\n",
    "them, possibly humans photo and classify the emotional state. \n",
    "\n",
    "Built on previous pages, the further back you go the more detail  \n",
    " - WikiRecentPhase0 - connect to live wiki updates\n",
    " - WikiRecentPhase1 - filter out bot processing\n",
    " - WikiRecentPhase2 - derive the current: editor, title, language\n",
    " - WikiRecentPhase3 - extract image, if it exists\n",
    " - WikiRecentPhase4 - find all faces in image\n",
    " - WikiRecentPhase5 - emotional analysis on faces\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install components\n",
    "!pip install sseclient\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "import pandas as pd\n",
    "import pandas\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import io\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from  functools import lru_cache\n",
    "import requests\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    \"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt\n",
    "\n",
    "#\n",
    "# Support for locating/rendering views.\n",
    "def display_view_stop(eventView, period=2):\n",
    "    \"\"\"Wrapper for streamsx.rest_primitives.View.display() to have button. \"\"\"\n",
    "    button =  widgets.Button(description=\"Stop Updating\")\n",
    "    display(button)\n",
    "    eventView.display(period=period) \n",
    "    def on_button_clicked(b):\n",
    "        eventView.stop_data_fetch()\n",
    "        b.description = \"Stopped\"\n",
    "    button.on_click(on_button_clicked)\n",
    "\n",
    "def view_events(views):\n",
    "    \"\"\"\n",
    "    Build interface to display a list of views and \n",
    "    display view when selected from list.\n",
    "     \n",
    "    \"\"\"\n",
    "    view_names = [view.name for view in views]\n",
    "    nameView = dict(zip(view_names, views))    \n",
    "    select = widgets.RadioButtons(\n",
    "        options = view_names,\n",
    "        value = None,\n",
    "        description = 'Select view to display',\n",
    "        disabled = False\n",
    "    )\n",
    "    def on_change(b):\n",
    "        if (b['name'] == 'label'):\n",
    "            clear_output(wait=True)\n",
    "            [view.stop_data_fetch() for view in views ]\n",
    "            display(select)\n",
    "            display_view_stop(nameView[b['new']], period=2)\n",
    "    select.observe(on_change)\n",
    "    display(select)\n",
    "\n",
    "def find_job(instance, job_name=None):\n",
    "    \"\"\"locate job within instance\"\"\"\n",
    "    for job in instance.get_jobs():    \n",
    "        if job.applicationName.split(\"::\")[-1] == job_name:\n",
    "            return job\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_view(instance, job_name=None, view_name=\"view\"):\n",
    "    job = find_job(instance, job_name)\n",
    "    return job.get_views(view_name)    \n",
    "    \n",
    "def display_views(instance, job_name):\n",
    "    \"Locate/promote and display all views of a job\"\n",
    "    job = find_job(instance, job_name=job_name)\n",
    "    if job is None:\n",
    "        print(\"Failed to locate job\")\n",
    "    else:\n",
    "        views = job.get_views()\n",
    "        view_events(views)\n",
    "\n",
    "def list_jobs(_instance=None, cancel=False):\n",
    "    \"\"\"\n",
    "    Interactive selection of jobs to cancel.\n",
    "    \n",
    "    Prompts with SelectMultiple widget, if thier are no jobs, your presente with a blank list.\n",
    "    \n",
    "    \"\"\"\n",
    "    active_jobs = { \"{}:{}\".format(job.name, job.health):job for job in _instance.get_jobs()}\n",
    "\n",
    "    selectMultiple_jobs = widgets.SelectMultiple(\n",
    "        options=active_jobs.keys(),\n",
    "        value=[],\n",
    "        rows=len(active_jobs),\n",
    "        description = \"Cancel jobs(s)\" if cancel else \"Active job(s):\",\n",
    "        layout=Layout(width='60%')\n",
    "    )\n",
    "    cancel_jobs = widgets.ToggleButton(\n",
    "        value=False,\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Delete selected jobs',\n",
    "        icon=\"stop\"\n",
    "    )\n",
    "    def on_value_change(change):\n",
    "        for job in selectMultiple_jobs.value:\n",
    "            print(\"canceling job:\", job, active_jobs[job].cancel())\n",
    "        cancel_jobs.disabled = True\n",
    "        selectMultiple_jobs.disabled = True\n",
    "\n",
    "    cancel_jobs.observe(on_value_change, names='value')\n",
    "    if cancel:\n",
    "        return HBox([selectMultiple_jobs, cancel_jobs])\n",
    "    else:\n",
    "        return HBox([selectMultiple_jobs])\n",
    "    \n",
    "# Notebook support / common\n",
    "def render_image(image_url=None, output_region=None):\n",
    "    \"\"\"Write the image into a output region.\n",
    "    \n",
    "    Args::\n",
    "        url: image\n",
    "        output_region: output region\n",
    "        \n",
    "    .. note:: The creation of the output 'stage', if this is not done the image is rendered in the page and\n",
    "        the output region. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        stage = widgets.Output(layout={'border': '1px solid green'})\n",
    "    except:\n",
    "        print(\"Error on request : \", image_url)\n",
    "    else:\n",
    "        if response.status_code == 200:\n",
    "            with output_region:\n",
    "                stage.append_display_data(widgets.Image(\n",
    "                    value=response.content,\n",
    "                    #format='jpg',\n",
    "                    width=300,\n",
    "                    height=400,\n",
    "                ))\n",
    "            output_region.clear_output(wait=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the service: Cloud/ICP4D\n",
    "\n",
    "TODO - collapse these three cells one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICP4D  - injected by 'Connected to instance' menu item\n",
    "try:\n",
    "    from icpd_core import icpd_util\n",
    "    cfg = icpd_util.get_service_instance_details(name='sample-icp1')\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    instance = rest.Instance.of_service(cfg)\n",
    "    print(\"Within ICP4D\")\n",
    "except ImportError:\n",
    "    cfg = None\n",
    "    print(\"Outside ICP4D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable 'InsecureRequestWarning'  - must be put after startup\n",
    "if cfg is not None:\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT ICP4D : cloud access - \n",
    "if cfg is None:\n",
    "    import credential   # remove @ ICP4D\n",
    "    import common\n",
    "    # TODO * check if instance is up\n",
    "    # - link up to first cell (can you do a test and execute)\n",
    "    sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', vcap_services={'streaming-analytics':[{'name':'Streaming3Turbine','credentials':credential.streaming3Turbine}]})\n",
    "    instance = sc.get_instances()[0]\n",
    "    # Render the views....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jobs(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Jump Table](#jumpTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='accessFoundation'></a>\n",
    "# Server Foundation : \n",
    "This page is a is build upon WikieRecentPhase{0,1,2,3} the contents are included, refer to the originating code for\n",
    "details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Streams processing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"fetch recent changes from wikievents site using SSE\"\"\"\n",
    "    for change in EventSource('https://stream.wikimedia.org/v2/stream/recentchange'):\n",
    "        if len(change.data):\n",
    "            try:\n",
    "                obj = json.loads(change.data)\n",
    "            except json.JSONDecodeError as err:\n",
    "                print(\"JSON l1 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            except json.decoder.JSONDecodeError as err:\n",
    "                print(\"JSON l2 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            else:\n",
    "                yield(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sum_aggregate():\n",
    "    def __init__(self, sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }):\n",
    "        \"\"\"\n",
    "        Summation of column(s) over a window's list of objects. \n",
    "        On a windows list objects, perform summation on the summary_field, output to result_field\n",
    "        \"\"\"\n",
    "        self.sum_map = sum_map\n",
    "    def __call__(self, tuples):    \n",
    "        summaries = dict()\n",
    "        for summary_field,result_field in self.sum_map.items():\n",
    "            summation = sum([ele[summary_field] for ele in tuples])\n",
    "            summaries.update({result_field : summation})\n",
    "        return(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "class tally_fields(object):\n",
    "    def __init__(self, top_count=3, fields=['user', 'wiki', 'title']):\n",
    "        \"\"\"\n",
    "        Tally fields of a list of tuples, output the name and . \n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.top_count = top_count\n",
    "    def __call__(self, tuples)->list:\n",
    "        tallies = dict()\n",
    "        for field in self.fields:\n",
    "            stage = [tuple[field] for tuple in tuples if tuple[field] is not None]\n",
    "            tallies[field] = collections.Counter(stage).most_common(self.top_count)\n",
    "        return tallies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "class wiki_lang():\n",
    "    \"\"\"\n",
    "    Augment the tuple to include language a description of the wiki the \n",
    "    event is destined for. The entry is augmented with language, native and code. \n",
    "    \n",
    "    Mapping is loaded at build time and utilized at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fname=\"wikimap.csv\"):\n",
    "        self.wiki_map = dict()\n",
    "        with open(fname, mode='r') as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            for row in csv_reader:\n",
    "                self.wiki_map[row['dbname']] = row\n",
    "\n",
    "    def __call__(self, tuple):\n",
    "        if tuple['wiki'] in self.wiki_map:\n",
    "            key = tuple['wiki']\n",
    "            tuple['code'] = self.wiki_map[key]['code']\n",
    "            tuple['language'] = self.wiki_map[key]['in_english']\n",
    "            tuple['native'] = self.wiki_map[key]['name_language']\n",
    "        else:\n",
    "            tuple['code'] = tuple['language'] = tuple['native'] = None\n",
    "        return tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='soupFunctionality'></a>\n",
    "## Soup functionality...\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "- find possible link to image\n",
    "- build and shred the link to find the image. \n",
    "- shredding can go down mulitple levels.\n",
    "- turn on lru_cache() after you determine why it fails\n",
    "\n",
    "If the title has a URL it's with an image extenstion (.jpg, .gif) it's good bet that and image is being added to an article. The url maybe a a page with the image url embeded within it. Look at using the 'beatifulsoup' package to locate the image within the linked page. With the image you can do image analysis use beautifulsoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shred_item_image(url):\n",
    "    \"\"\"Shed the item page, seeking image. \n",
    "    \n",
    "    Discover if referencing image by shredding referening url. If it is, dig deeper \n",
    "    and extract the 'src' link. \n",
    "    \n",
    "    Locate the image within the page, locate <a class='image' src=**url** ,..>\n",
    "    \n",
    "    This traverses two files, pulls the thumbnail ref and follows to fullsize.\n",
    "    \n",
    "    Args:\n",
    "        url: item page to analyse\n",
    "    \n",
    "    Returns: \n",
    "        If image found [{name,title,org_url},...]\n",
    "    \n",
    "    .. warning:: this fetches from wikipedia, requesting too frequenty is bad manners. Uses the lru_cache()\n",
    "    so it minimises the requests.\n",
    "    \n",
    "    This can pick up multiple titles, on a page that is extract, dropping to only one. \n",
    "    \"\"\"\n",
    "    img_urls = list()\n",
    "    try:\n",
    "        rThumb = requests.get(url = url)\n",
    "        #print(r.content)\n",
    "        soupThumb = BeautifulSoup(rThumb.content, \"html.parser\")\n",
    "        divThumb = soupThumb.find(\"div\", class_=\"thumb\")\n",
    "        if divThumb is None:\n",
    "            print(\"No thumb found\", url  )\n",
    "            return img_urls\n",
    "        thumbA = divThumb.find(\"a\", class_=\"image\")\n",
    "        thumbHref = thumbA.attrs['href']\n",
    "\n",
    "        rFullImage = requests.get(url=thumbHref)\n",
    "        soupFull = BeautifulSoup(rFullImage.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(\"Error request.get, url: {} except:{}\".format(url, str(e)))\n",
    "    else:\n",
    "        divFull = soupFull.find(\"div\", class_=\"fullImageLink\", id=\"file\")\n",
    "        if (divFull is not None):\n",
    "            fullA = divFull.find(\"a\")\n",
    "            img_urls.append({\"title\":soupThumb.title.getText(),\"img\": fullA.attrs['href'],\"org_url\":url})\n",
    "    finally:\n",
    "        return img_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shred_jpg_image(url):\n",
    "    \"\"\"    ends with '.jpg'.\n",
    "    \n",
    "    Discover if referencing image by shredding referening url. If it is, dig deeper \n",
    "    and extract the 'src' link. \n",
    "    \n",
    "    Locate the image within the page, \n",
    "            locate : <div class='fullImageLinks'..>\n",
    "                         <a href=\"..url to image\" ...>.</a>\n",
    "                         :\n",
    "                     </div>  \n",
    "    Args:\n",
    "        url: item page to analyse\n",
    "    \n",
    "    Returns: \n",
    "        If image found [{name,title,org_url='requesting url'},...]\n",
    "    \n",
    "    .. warning:: this fetches from wikipedia, requesting too frequenty is bad manners. Uses the lru_cache()\n",
    "    so it minimises the requests.\n",
    "    \n",
    "    \"\"\"\n",
    "    img_urls = list()\n",
    "    try:\n",
    "        r = requests.get(url = url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(\"Error request.get, url: {} except:{}\".format(url, str(e)))\n",
    "    else:\n",
    "        div = soup.find(\"div\", class_=\"fullImageLink\")\n",
    "        if (div is not None):\n",
    "            imgA = div.find(\"a\")\n",
    "            img_urls.append({\"title\":soup.title.getText(),\"img\":\"https:\" + imgA.attrs['href'],\"org_url\":url})\n",
    "        else: \n",
    "            print(\"failed to find div for\",url)\n",
    "    finally:\n",
    "        return img_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support of streams processing\n",
    "class cache_url_process():\n",
    "    def __init__(self, process_url, cache_max=200):\n",
    "        \"\"\"I would use @lru_cache() but I ran into two problems. \n",
    "           - when I got to the server it could not find the function. \n",
    "           - get a stack overflow when building the topology. \n",
    "        Args::\n",
    "            process_url: a function that process's the request, when not cached.\n",
    "            Function will accept a URL and retrn  dict.\n",
    "        Return::\n",
    "            result from process_url that may be a cached value.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.urls = OrderedDict()\n",
    "        self.hits = 0\n",
    "        self.attempts = 0\n",
    "        self.process = process_url\n",
    "        self.cache_max = cache_max\n",
    "    def cache_process(self, url):\n",
    "        self.attempts += 1\n",
    "        if url in self.urls:\n",
    "            self.hits += 1\n",
    "            stage = self.urls[url]\n",
    "            del self.urls[url]  # move to begining of que\n",
    "            self.urls[url] = stage\n",
    "            n = len(self.urls) - self.cache_max \n",
    "            [self.urls.popitem(last=False) for idx in range(n if n > 0 else 0)]\n",
    "            return stage\n",
    "        stage = self.process(url)\n",
    "        self.urls[url] = stage\n",
    "        return stage\n",
    "    def stats(self):\n",
    "        return dict({\"attempts\":self.attempts,\"hits\":self.hits,\"len\":len(self.urls)})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class soup_image():\n",
    "    \"\"\"If the the field_name has a potential for a image we\n",
    "    \n",
    "    Return: \n",
    "        None : field did not have potenital for an image.\n",
    "        [] : had potential but no url found. \n",
    "        [{title,img,href}]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"):\n",
    "        self.url_base = url_base\n",
    "        self.field_name = field_name\n",
    "        self.cache_item = None\n",
    "        self.cache_jpg = None\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "        if self.cache_item is None:\n",
    "            self.cache_item = cache_url_process(shred_item_image)\n",
    "            self.cache_jpg = cache_url_process(shred_jpg_image)\n",
    "        title = _tuple[self.field_name]\n",
    "        img_desc = None \n",
    "        if (title[0] == \"Q\"):\n",
    "            lnk = self.url_base + title\n",
    "            img_desc = self.cache_item.cache_process(lnk)\n",
    "            print(\"cache_item\", self.cache_item.stats())\n",
    "\n",
    "        elif title.startswith(\"File:\") and (title.endswith('.JPG') or title.endswith('.jpg')):\n",
    "            lnk = self.url_base + title.replace(' ','_')\n",
    "            img_desc = self.cache_jpg.cache_process(lnk)\n",
    "            print(\"cache_jpg\", self.cache_jpg.stats())\n",
    "\n",
    "            #print(\"cache_jpg\", self.cache_jpg.stats())\n",
    "            \n",
    "        _tuple['img_desc'] = img_desc\n",
    "        return _tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='facialFunctionality'></a>\n",
    "## Facial Image Extraction\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "Using [IBM Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpBufIoOut = None\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def facial_fetch(imgurl):\n",
    "    \"\"\" Our friends: \"https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/\"\n",
    "\n",
    "    ..note: \n",
    "        This utilizing a function put up the by our friends, $$ == 0.\n",
    "        It can stop working at anytime. \n",
    "    \"\"\"\n",
    "    predict_url='http://max-facial-recognizer.max.us-south.containers.appdomain.cloud/model/predict'\n",
    "    parsed = urlparse(imgurl)   \n",
    "    filename = parsed.path.split('/')[-1]\n",
    "    if (filename.lower().endswith('.svg')):\n",
    "        print(\"Cannot process svg:\", imgurl)\n",
    "        return list(), None\n",
    "    if (filename.lower().endswith('.tif')):\n",
    "        print(\"Cannot process tif:\", imgurl)\n",
    "        return list(), None\n",
    "    try:\n",
    "        page = requests.get(imgurl)\n",
    "    except Exception as e:\n",
    "        print(\"Image fetch exception:\", e)\n",
    "        return None, None\n",
    "    bufIoOut = io.BytesIO(page.content)\n",
    "    files = {'image': (filename, bufIoOut, \"image/jpeg\")}\n",
    "    try: \n",
    "        r = requests.post(predict_url, files=files)\n",
    "    except Exception as e:\n",
    "        print(\"Analysis service exception\", e)\n",
    "        return None, None\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Analysis failure:\",r.status_code, r.json())\n",
    "        return None, None\n",
    "    analysis = r.json()\n",
    "    return analysis, bufIoOut\n",
    "\n",
    "def facial_locate(imgurl):\n",
    "    analysis,bufIoOut = facial_fetch(imgurl)\n",
    "    if bufIoOut is None:\n",
    "        return None\n",
    "    if (analysis['predictions']) == 0:\n",
    "        print(\"No predictions found for\", imgurl)\n",
    "        return None\n",
    "    return({'bin_image':bufIoOut, 'faces':analysis})\n",
    "\n",
    "def crop_percent(img_dim, box_extent):\n",
    "    \"\"\"get the % of image the cropped image is\"\"\"\n",
    "    img_size = img_dim[0] * img_dim[1]\n",
    "    box_size = abs((int(box_extent[0]) - int(box_extent[2])) * (int(box_extent[1])- int(box_extent[3])))\n",
    "    percent = ((box_size/img_size) * 100)\n",
    "    return(percent)\n",
    "\n",
    "\n",
    "def image_cropper(bin_image, faces):\n",
    "    \"\"\"Crop out the faces from a URL.\n",
    "    Args:\n",
    "        url : image images\n",
    "        faces : list of {region,predictions} that that should be cropped\n",
    "    Return:\n",
    "        dict with 'annotated_image' and 'crops'\n",
    "        'crops' is list of dicts with \n",
    "            {image:face image, \n",
    "             probability:chances it's a face, \n",
    "             image_percent:found reqion % of of the image, \n",
    "             detection_box:region of the original image that the image was extacted from}\n",
    "         'crops' empty - nothing found, no faces found\n",
    "    \"\"\"\n",
    "    crops = list()\n",
    "    for face in faces['predictions']: \n",
    "        i = Image.open(bin_image)\n",
    "        percent = crop_percent( i.size, face['detection_box'])\n",
    "        img = i.crop(face['detection_box'])\n",
    "        crops.append({'image':img, 'probability':face['probability'],'detection_box':face['detection_box'],'image_percent':percent})\n",
    "    return crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import base64\n",
    "class facial_image():\n",
    "    \"\"\"Extract all the faces from an image, for each found face generate a tuple with a face field. \n",
    "    Args:\n",
    "        - field_name : name of field on input tuple with the image description dict\n",
    "        - img_field : dictionary entry that has url of image\n",
    "        \n",
    "\n",
    "    Return: \n",
    "        None - No 'img_desc' field or no faces found \n",
    "        List of tuples composed of the input tuple with a new 'face' field\n",
    "        face a dictionary: \n",
    "        - probability : probability that it's a face\n",
    "        - percentage : % of the field_img that the detection_box occupies\n",
    "        - detection_box : within the orginal image, coodinates of extracted image\n",
    "        - bytes_PIL_b64 : cropped image in binary Base64 ascii\n",
    "    ..notes: \n",
    "        1. the next operator in line should be the flat_map() that takes the list of tuples and converts \n",
    "        to a stream of tuples.\n",
    "    ..code::\n",
    "        '''\n",
    "        ## Example of displaying encoded cropped image.\n",
    "        from PIL import Image\n",
    "        from io import BytesIO\n",
    "        import copy\n",
    "        \n",
    "        calidUrlImage = \"URL of valid Image to be analysized\"\n",
    "        minimal_tuple = {'img_desc':[{'img':validUrlImage}]}\n",
    "        fi = facial_image()\n",
    "        crops = fi.__call__(minimal_tuple)\n",
    "        for crop in crops:\n",
    "            cropImg = Image.open(io.BytesIO(base64.b64decode(crop['face']['bytes_PIL_b64'])))\n",
    "            print(\"Image Size\",cropImg.size)\n",
    "            display(cropImg)\n",
    "        '''\n",
    "    \"\"\"\n",
    "    def __init__(self, field_name=\"img_desc\", url_base=\"https://www.wikidata.org/wiki/\", image_field='img'):\n",
    "        self.url_base = url_base\n",
    "        self.img_desc = field_name\n",
    "        self.img_field = image_field\n",
    "        self.cache_item = None\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "\n",
    "        if self.img_desc not in _tuple or len(_tuple[self.img_desc]) == 0:\n",
    "            return None\n",
    "        desc = _tuple[self.img_desc][0]\n",
    "        if self.img_field not in desc:\n",
    "            print(\"Missing 'img' field in 'img_desc'\")\n",
    "            return None\n",
    "        processed = facial_locate(desc[self.img_field])\n",
    "        if processed is None:\n",
    "            return None\n",
    "\n",
    "        crops = image_cropper(processed['bin_image'], processed['faces'])\n",
    "        tuples = list()\n",
    "        for crop in crops:\n",
    "            augmented_tuple = copy.copy(_tuple)\n",
    "            with io.BytesIO() as output:\n",
    "                crop['image'].save(output, format=\"JPEG\")\n",
    "                contents = output.getvalue() \n",
    "            crop['bytes_PIL_b64'] = base64.b64encode(contents).decode('ascii')\n",
    "            del crop['image']\n",
    "            augmented_tuple['face'] = crop\n",
    "            tuples.append(augmented_tuple)\n",
    "        return tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Fake up tuple: facial_image\n",
    "validUrlImage = \"https://upload.wikimedia.org/wikipedia/commons/5/52/Bundesarchiv_B_145_Bild-F023358-0012%2C_Empfang_in_der_Landesvertretung_Bayern%2C_Hallstein.jpg\"\n",
    "minimal_tuple = {'img_desc':[{'img':validUrlImage}]}\n",
    "fi = facial_image()\n",
    "crops = fi.__call__(minimal_tuple)\n",
    "for crop in crops:\n",
    "    cropImg = Image.open(io.BytesIO(base64.b64decode(crop['face']['bytes_PIL_b64'])))\n",
    "    #cropImg = Image.open(io.BytesIO(res[0]['face']['bytes_PIL']))\n",
    "    print(\"Image Size\",cropImg.size)\n",
    "    display(cropImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysisFunctionality'></a>\n",
    "## Analysis Functionality\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "Using the  [IBM emotion classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/) to analyise images that are being added to wikiepedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_crop(bufIoOut, imgurl):\n",
    "    \"\"\" Our friends: \"https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/\"\n",
    "    \n",
    "    Analyse an image using the service \"http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/\".\n",
    "    \n",
    "    Send binary image to analysis \n",
    "    \n",
    "    The processing nodes not necessarily return a prediction, could be an indication that it's not a predictable image.\n",
    "    \n",
    "    Args:\n",
    "        imgurl: the original source image that the cropped region came from\n",
    "        buiIoOut : the binary cropped image to be analysized\n",
    "\n",
    "    Returns:\n",
    "        None - error encountered\n",
    "        [] : executed, no prediction.\n",
    "        [{anger,contempt,disgust,happiness,neutral,sadness,surpise}]   \n",
    "    ..note: \n",
    "        This utilizing a function put up the by our friends, $$ == 0.\n",
    "        It can stop working at anytime. \n",
    "    \"\"\"\n",
    "    predict_url='http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/model/predict'\n",
    "    parsed = urlparse(imgurl)   \n",
    "    filename = parsed.path.split('/')[-1]\n",
    "\n",
    "    files = {'image': (filename, bufIoOut, \"image/jpeg\")}\n",
    "    try: \n",
    "        r = requests.post(predict_url, files=files)\n",
    "    except Exception as e:\n",
    "        print(\"Analysis service exception\", e)\n",
    "        return None\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Analysis failure:\",r.status_code, r.json())\n",
    "        return None\n",
    "    analysis = r.json()\n",
    "    if len(analysis['predictions']) == 0:\n",
    "        return list()\n",
    "    emotions = analysis['predictions'][0]['emotion_predictions']\n",
    "    return [{emot['label']:float(\"{0:.2f}\".format(emot['probability'])) for emot in emotions}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotion_image():\n",
    "    \"\"\"If there is an img entry, attempt to analyize\n",
    "    Args:\n",
    "        field_name : name of field on input tuple with the image description dict\n",
    "        img_field: dictionary entry that has url of image\n",
    "    \n",
    "    Return: \n",
    "        None - No 'img_desc' field or no entries in the field\n",
    "        Add a emotion to the tuple. \n",
    "            Empty [] if nothing in img_desc or no emotion could be derived\n",
    "            None : field did not have potenital for an image.\n",
    "        [] : had potential but no url found. \n",
    "        [{title,img,href}]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "        bufIoOut_decode_image = io.BytesIO(base64.b64decode(_tuple['face']['bytes_PIL_b64']))\n",
    "        url = _tuple['img_desc'][0]['img']\n",
    "        emotion = emotion_crop(bufIoOut_decode_image, url)\n",
    "        _tuple['emotion'] = emotion\n",
    "        return(_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fake up tuple: facial_image + emotion operators. \n",
    "validUrlImage = \"https://upload.wikimedia.org/wikipedia/commons/5/52/Bundesarchiv_B_145_Bild-F023358-0012%2C_Empfang_in_der_Landesvertretung_Bayern%2C_Hallstein.jpg\"\n",
    "minimal_tuple = {'img_desc':[{'img':validUrlImage}]}\n",
    "fi = facial_image()\n",
    "crops = fi.__call__(minimal_tuple)\n",
    "ei = emotion_image()\n",
    "for crop in crops:\n",
    "    cropImg = Image.open(io.BytesIO(base64.b64decode(crop['face']['bytes_PIL_b64'])))\n",
    "    #cropImg = Image.open(io.BytesIO(res[0]['face']['bytes_PIL']))\n",
    "    print(\"Image Size\",cropImg.size)\n",
    "    display(cropImg)\n",
    "    print(ei.__call__(crop)['emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='composeSubmit'></a>\n",
    "# Compose and submit\n",
    "[Jump Table](#jumpTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO If WikiPhase5 is running, cancel before submitting.\n",
    "\n",
    "\n",
    "def WikiPhase5(jobName=None, wiki_lang_fname=None):\n",
    "    \"\"\"\n",
    "    Compose topology. \n",
    "    -- wiki_lang : csv file mapping database name to langauge\n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    ### make sure we sseclient in Streams environment.\n",
    "    topo.add_pip_package('sseclient')\n",
    "    topo.add_pip_package('bs4')\n",
    "    \n",
    "    ## wiki events\n",
    "    source = topo.source(get_events, name=\"WikiFeed\")\n",
    "    ## select events generated by humans\n",
    "    edits = source.filter(lambda x: x['type']=='edit' and x['bot'] is False, name='humanEdits')\n",
    "    # pare down the humans set of columns\n",
    "    paredEdits = edits.map(lambda x : {'timestamp':x['timestamp'],'new_len':x['length']['new'],'old_len':x['length']['old'], 'delta_len':x['length']['new'] - x['length']['old'],'wiki':x['wiki'],'user':x['user'],'title':x['title']}, \n",
    "                        name=\"selectMain\")\n",
    "    paredHumanEdits = paredEdits.view(buffer_time=1.0, sample_size=200, name=\"paredHumanEdits\", description=\"Edits done by humans\")\n",
    "\n",
    "    ## Define window(count)& aggregate\n",
    "    winAgg = paredEdits.last(100).trigger(20)\n",
    "    sumAggregate_ = winAgg.aggregate(sum_aggregate(sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }), name=\"sumAggregate\")\n",
    "    aggEdits = sumAggregate_.view(buffer_time=1.0, sample_size=200, name=\"aggEdits\", description=\"Aggregations of human edits\")\n",
    "\n",
    "    ## Define window(count) & tally edits\n",
    "    tallyWin = paredEdits.last(100).trigger(10)\n",
    "    tallyTop = tallyWin.aggregate(tally_fields(fields=['user', 'title'], top_count=10), name=\"talliesTop\")\n",
    "    talliesCount = tallyTop.view(buffer_time=1.0, sample_size=200, name=\"talliesCount\", description=\"Top count tallies: user,titles\")\n",
    "\n",
    "    ## augment filterd/pared edits with language\n",
    "    langAugment = paredEdits.map(wiki_lang(fname=wiki_lang_fname), name=\"langAugment\")\n",
    "    langAugment.view(buffer_time=1.0, sample_size=200, name=\"langAugment\", description=\"Language derived from wiki\")\n",
    "    \n",
    "    ## attempt to extract image using beautifulsoup add img_desc[{}] field\n",
    "    soupImage = langAugment.map(soup_image(field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"),name=\"soup4Img\")\n",
    "    imgActive = soupImage.filter(lambda x: x['img_desc'] is not None and len(x['img_desc']) > 0, name=\"imgActive\")\n",
    "    imgActive.view(buffer_time=1.0, sample_size=200, name=\"imgActive\", description=\"Images soup located\")\n",
    "    \n",
    "    ## factial extraction  - \n",
    "    facialImages = imgActive.map(facial_image(field_name='img_desc'),name=\"facialImg\")\n",
    "    facialImage = facialImages.flat_map(name=\"flatFacial\")\n",
    "    facialImage.view(buffer_time=10.0, sample_size=20, name=\"facialImg\", description=\"Facial analysis\")\n",
    "\n",
    "    ## emotion anaylsis on image - \n",
    "    emotionImage = facialImage.map(emotion_image(), name=\"emotionImg\")\n",
    "    emotionImage.view(buffer_time=10.0, sample_size=20, name=\"emotionImg\", description=\"Emotion analysis\")\n",
    "    \n",
    "    \n",
    "    ## Define window(time) & tally language\n",
    "    timeLangWin = langAugment.last(datetime.timedelta(minutes=2)).trigger(5)\n",
    "    timeLanguage = timeLangWin.aggregate(tally_fields(fields=['language'], top_count=10), name=\"timeLang\")\n",
    "    talliesTime = timeLanguage.view(buffer_time=1.0, sample_size=200, name=\"talliesTime\", description=\"Top timed tallies: language\")\n",
    "   \n",
    "    return ({\"topo\":topo,\"view\":{ }})\n",
    "\n",
    "\n",
    "## ICP4D submission\n",
    "from streamsx.topology import context\n",
    "from streamsx.topology.topology import Topology\n",
    "\n",
    "#cfg=icpd_util.get_service_instance_details(name='sample-icp1')\n",
    "if cfg is not None:\n",
    "    resp = WikiPhase5(jobName=\"WikiPhase5\", wiki_lang_fname=os.environ['DSX_PROJECT_DIR']+'/datasets/wikimap.csv')\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "\n",
    "    submission_result = context.submit(\"DISTRIBUTED\",\n",
    "                                   resp['topo'], \n",
    "                                   config=cfg)\n",
    "    # The submission_result object contains information about the running application, or job\n",
    "    if submission_result.job:\n",
    "        print(\"JobId: \", submission_result['id'] , \"Name: \", submission_result['name'])\n",
    "## Cloud submission\n",
    "if cfg is None:\n",
    "    resp = WikiPhase5(jobName=\"WikiPhase5\", wiki_lang_fname=\"wikimap.csv\")\n",
    "    submitStatus = common.submitProcess(topology=resp['topo'],\n",
    "                                        streamsService=\"Streaming3Turbine\",\n",
    "                                        buildType=\"DISTRIBUTED\",\n",
    "                                        serviceType=\"STREAMING_ANALYTIC\",\n",
    "                                        jobName=\"WikiPhase5\",\n",
    "                                        cancel=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='runningActive'></a>\n",
    "### Running / Active \n",
    "[Jump Table](#jumpTable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![phase5.jpg](images/phase5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data that is flowing.....\n",
    "display_views(instance, \"WikiPhase5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='jumpTable'></a>\n",
    "### Jump Table..\n",
    "- **[Running / Active](#runningActive)**\n",
    "- [Access Foundation](#accessFoundation)@server\n",
    "- [Compose Submit](#composeSubmit)@server\n",
    "- [Language Distribution](#languageDistribution)\n",
    "- [Soup Functionality](#soupFunctionality)@server\n",
    "- [Image Extraction](#imageExtraction)  \n",
    "- [Facial Image Extraction](#facialFunctionality)@server\n",
    "- [Image Facial Location](#imageFacialAnalysis)\n",
    "- [Analysis Functionaltiy](#analysisFunctionality)@server\n",
    "- [Image Emotion Analysis](#imageEmotionAnalysis) : \n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![phase4_1.gif](attachment:phase5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='languageDistribution'></a>\n",
    "# Render Language Distribution\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "List the top users, titles and languages over the last 100 trigger ever 20/10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two tables and a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_out(*args, out=None, tally_field='title'):\n",
    "    \"\"\"\n",
    "    Render tally data: json format of order list of lists. \n",
    "```    \n",
    "{'language': [['English', 1178],\n",
    "   ['French', 39],\n",
    "   ['German', 38],\n",
    "   ['Spanish', 38],\n",
    "   ['Italian', 32],\n",
    "   ['Portuguese', 28],\n",
    "   ['Russian', 27],\n",
    "   ['Swedish', 13],\n",
    "   ['Dutch', 11],\n",
    "   ['Hebrew', 10]]}]\n",
    "```\n",
    "    \"\"\"\n",
    "    tallies = args[0]\n",
    "    assert tally_field in tallies[0], \"Field {} does not exist in input tuples, keys found : {}\".format(tally_field, tallies[0].keys()) \n",
    "    title_tallies = tallies[0][tally_field]\n",
    "    title = [ele[0] for ele in title_tallies]\n",
    "    count = [ele[1] for ele in title_tallies]\n",
    "    tbl = [(tally_field, title),('count', count)]\n",
    "    out.append_display_data(pd.DataFrame.from_dict(dict(tbl)))\n",
    "    out.clear_output(wait=True)\n",
    "\n",
    "def default_out(*args, out=None):\n",
    "    tuples = args[0]\n",
    "    out.append_display_data(pd.DataFrame(tuples))\n",
    "    out.clear_output(wait=True)\n",
    "\n",
    "def pie_out(*args, out=None, tally_field=\"language\", drop_top=True):\n",
    "    \"\"\"Render piechart into Output\n",
    "    drop_top: do not display the highest value element. \n",
    "    \n",
    "    \"\"\"\n",
    "    tuples = args[0]\n",
    "    if drop_top:\n",
    "        start = -1\n",
    "    else:\n",
    "        start = 0\n",
    "    assert tally_field in tuples[0], \"Field {} does not exist in input tuples, keys found : {}\".format(tally_field, tuples[0].keys()) \n",
    "    language= [lst[0] for lst in tuples[start][tally_field]]\n",
    "    counts = [lst[-1] for lst in tuples[start][tally_field]]\n",
    "    percent = counts[0]/sum(counts) * 100\n",
    "    #print(\"[{2}]{0:4.2f}% of the events are in {1},\\n {1} will be dropped from the piechart.\".format(percent,language[0],\"+*\"[idx%2]))\n",
    "    df = pd.DataFrame({'counts': counts[1:]}, index=language[1:])\n",
    "    with out:\n",
    "        plt.show(df.plot.pie(y='counts'))\n",
    "        clear_output(wait=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_display_out(self, duration, period, active, label, lock, transform_func):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import IPython\n",
    "    tqueue = self.start_data_fetch()\n",
    "    end = time.time() + float(duration) if duration is not None else None\n",
    "    max_rows = pd.options.display.max_rows\n",
    "    max_rows = 10\n",
    "    last = 0\n",
    "    idx = 0\n",
    "    try:\n",
    "        while self._data_fetcher and (duration is None or time.time() < end):\n",
    "            idx += 1\n",
    "            # Slow down pace when view is busy\n",
    "            gap = time.time() - last\n",
    "            label.value = \"{0} wait:{1:4.2}\".format(\"-|\"[idx%2],period - gap)\n",
    "            if gap < period:\n",
    "                time.sleep(period - gap)\n",
    "            # Display latest tuples by removing earlier ones\n",
    "            # Avoids display falling behind live data with\n",
    "            # large view buffer\n",
    "            tqs = tqueue.qsize()\n",
    "            if tqs > max_rows:\n",
    "                tqs -= max_rows\n",
    "                for _ in range(tqs):\n",
    "                    try:\n",
    "                        tqueue.get(block=False)\n",
    "                    except queue.Empty:\n",
    "                        break\n",
    "            tuples = self.fetch_tuples(max_rows, None)\n",
    "            if not tuples:\n",
    "                if not self._data_fetcher:\n",
    "                    break\n",
    "                #out.append_stdout('No tuples')\n",
    "            else:\n",
    "                lock.acquire()\n",
    "                transform_func(tuples) ### make call to modfield tally_test\n",
    "                #out.clear_output(wait=True)\n",
    "                lock.release()\n",
    "                #out.append_display_data(pd.DataFrame(tuples))\n",
    "            #out.clear_output(wait=True)\n",
    "            last = time.time()\n",
    "    except Exception as e:\n",
    "        self.stop_data_fetch()\n",
    "        label.value = str(e)\n",
    "        active.value=False\n",
    "        raise e\n",
    "    label.value = \"-\"\n",
    "    self.stop_data_fetch()\n",
    "    active.value=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dir().count('view_talliesUser'):\n",
    "    print(\"shutting down\")\n",
    "    print(view_talliesUser.stop_data_fetch())\n",
    "    print(view_talliesTitle.stop_data_fetch())\n",
    "    print(view_talliesLanguage.stop_data_fetch())\n",
    "view_talliesUser = view_count =  get_view(instance=instance,job_name=\"WikiPhase5\", view_name=\"talliesCount\")[0]\n",
    "print(view_talliesUser.start_data_fetch())\n",
    "view_talliesTitle = view_time = get_view(instance=instance,job_name=\"WikiPhase5\", view_name=\"talliesCount\")[0]\n",
    "print(view_talliesTitle.start_data_fetch())\n",
    "view_talliesLanguage  = get_view(instance=instance,job_name=\"WikiPhase5\", view_name=\"talliesTime\")[0]\n",
    "print(view_talliesLanguage.start_data_fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from  functools import partial\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "self1 = view_talliesUser\n",
    "self2 = view_talliesTitle\n",
    "self3 = view_talliesLanguage\n",
    "\n",
    "duration = 60.0\n",
    "period = 2.0\n",
    "\n",
    "topline = widgets.Text(value=self1.description, description=\"Testing\", disabled=True)\n",
    "active1 = widgets.Valid(value=True, description='users', readout='Stopped')\n",
    "label1 = widgets.Label(value=\"starting\", description=\"status\")\n",
    "active2 = widgets.Valid(value=True, description='titles', readout='Stopped')\n",
    "label2 = widgets.Label(value=\"starting\")\n",
    "active3 = widgets.Valid(value=True, description='languages', readout='Stopped')\n",
    "label3 = widgets.Label(value=\"starting\")\n",
    "user_region = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'270pt'})\n",
    "title_region = widgets.Output(layout={'border': '1px solid black','width':'30%','height':'270pt'})\n",
    "chart_region = widgets.Output(layout={'border': '3px solid orange','width':'60%', \"height\":\"270pt\"})\n",
    "user_region.clear_output(wait=True)\n",
    "title_region.clear_output(wait=True)\n",
    "chart_region.clear_output(wait=True)\n",
    "status = widgets.HBox([topline])\n",
    "activity = widgets.HBox([active1, label1, active2, label2, active3, label3])\n",
    "tables = widgets.HBox([user_region,title_region])\n",
    "piechart = widgets.HBox([chart_region])\n",
    "dashboard = widgets.VBox([status, activity, tables, piechart])\n",
    "display(dashboard)\n",
    "\n",
    "self1._display_thread = threading.Thread(target=lambda: graph_display_out(self1, duration, period, active1, label1, lock, partial(tally_out, tally_field=\"user\", out=user_region) ))\n",
    "self2._display_thread = threading.Thread(target=lambda: graph_display_out(self2, duration, period, active2, label2, lock, partial(tally_out, tally_field=\"title\", out=title_region)))\n",
    "self3._display_thread = threading.Thread(target=lambda: graph_display_out(self3, duration, period, active3, label3, lock, partial(pie_out, out=chart_region)))\n",
    "\n",
    "self1._display_thread.start()\n",
    "self2._display_thread.start()\n",
    "self3._display_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imageExtraction'></a>\n",
    "# Render Image Extraction :\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "\n",
    "The images that can be sent to wikipedia include tiff. The analysis, described below, can be done on tiff images. The Safari browser can render tiff images. But, Firefox and Chrome browsers fail to render tiff - which I find amazing. If you want to see the \n",
    "full completment of images flowing from Streams, use Safari. \n",
    "\n",
    "\n",
    "## Render the feed of images .\n",
    "\n",
    "Issues : \n",
    "- Redudant names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook support / common\n",
    "ana_stage = list()\n",
    "def display_image(tup, image_region=None, title_region=None, url_region=None):\n",
    "    if tup['img_desc'] is not None and len(tup['img_desc']) > 0:\n",
    "        display_desc = tup['img_desc'][0]\n",
    "        ana_stage.append(display_desc)\n",
    "        title_region.value = \"Img Title:{}\".format(display_desc['title'] )\n",
    "        url_region.value = \"{}\".format(display_desc['img'])\n",
    "        render_image(image_url=display_desc['img'], output_region=image_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the images - beatifulsoup @ browser.\n",
    "\n",
    "Tapping the view before the beatifulsoup is invoked : langAugment\n",
    "\n",
    "Why is this here? This code was created/debugged locally using the WikieRecentPhase3,\n",
    "when I was done I added the lines to the application to pull them up into the server \n",
    "use them. \n",
    "\n",
    "```python\n",
    "    ## attempt to extract image using beautifulsoup add img_desc[{}] field\n",
    "    soupImage = langAugment.map(soup_image(field_name=\"title\", \n",
    "                                           url_base=\"https://www.wikidata.org/wiki/\"),name=\"soup4Img\")\n",
    "    imgActive = soupImage.filter(lambda x: x['img_desc'] is not None and len(x['img_desc']) > 0, \n",
    "                                 name=\"imgActive\")\n",
    "    imgActive.view(buffer_time=1.0, sample_size=200, name=\"imgActive\", description=\"Images soup located\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Mention....hmm?\n",
    "\n",
    "\n",
    "In the view from Streams\n",
    "- extract the item url\n",
    "- get item page, extract image tag\n",
    "- render image\n",
    "\n",
    "The output region is below the cell,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jobs(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the located images\n",
    "\n",
    "\n",
    "The server side is traversing web pages to locate the images. The search is based \n",
    "upon the SSE message arriving from Wiki. The page(s) are sheded using the beautifulsoup\n",
    "package .\n",
    "\n",
    "Tapping the after the beatifulsoup is invoked : soupUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_views(instance, job_name=\"WikiPhase5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_widget = widgets.Label(value=\"Status\", layout={'border': '1px solid green','width':'30%'})\n",
    "url_widget = widgets.Label(value=\"Img URL\", layout={'border': '1px solid green','width':'100%'})\n",
    "image_widget = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'270pt'})\n",
    "title_widget = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'30%'})\n",
    "\n",
    "dashboard = widgets.VBox([status_widget, image_widget, title_widget, url_widget])\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_view = instance.get_views(name=\"imgActive\")[0]\n",
    "_view.start_data_fetch()\n",
    "@catchInterrupt\n",
    "def server_soup(iterations=30):\n",
    "    \"\"\"Wiki's accepts images, locating the source image requires \n",
    "    shredding the referencing pages. This fetches the located image links \n",
    "    from Streams and display them.\n",
    "    \n",
    "    Args::\n",
    "        iterations: How many times to interation, <1  goes to infinity. \n",
    "    \"\"\"\n",
    "    while iterations:\n",
    "        iterations -= 1\n",
    "        view_tuples = _view.fetch_tuples(max_tuples=100, timeout=2)\n",
    "        for soup_tuple in view_tuples:\n",
    "            status_widget.value = \"{} : {}\".format(iterations if iterations > 0 else \"infinity\", soup_tuple['title'])\n",
    "            display_image(soup_tuple, image_region=image_widget, title_region=title_widget, url_region=url_widget)\n",
    "    status_widget.value = \"Status:...DONE retrieving extracted images\"\n",
    "server_soup(iterations=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imageFacialAnalysis'></a>\n",
    "## Image Facial Location with [MAX](https://developer.ibm.com/exchanges/models/)\n",
    "\n",
    "Using [IBM Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/)\n",
    "\n",
    "[Jump Table](#jumpTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server side\n",
    "\n",
    "From the server this is getting the cropped images. Streams is passing the image through the \n",
    "IBM Facial Recognizer that extract the coordinates of potential faces. A new tuple is generated\n",
    "for each potential face consisting of the \n",
    "- input tuple, this include a url image being analyzed\n",
    "- face dict() consisting of ...\n",
    "- - probability : probabilty that it's an face\n",
    "- - image_percentage : % of image original image the found face occupies\n",
    "- - bytes_PIL_b64 : binary image version of found image\n",
    "- - detection_box : region within the original image the face was detected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_view = instance.get_views(name=\"facialImg\")[0]\n",
    "_view.start_data_fetch()\n",
    "view_tuples = _view.fetch_tuples(max_tuples=100, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tuple in view_tuples:\n",
    "    bin64 = tuple['face']['bytes_PIL_b64']\n",
    "    cropImg = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "#cropImg = Image.open(io.BytesIO(res[0]['face']['bytes_PIL']))\n",
    "    print(\"crop size:{0} cropped image % of original:{3:.2f} probability image is a face:{1:.2f}\\n img:{2}\".format(cropImg.size, \n",
    "                                                                       tuple['face']['probability'], \n",
    "                                                                       tuple['img_desc'][0]['img'],\n",
    "                                                                       tuple['face']['image_percent']))\n",
    "    cropImg.thumbnail([128,128])\n",
    "    display(cropImg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -X POST \"http://max-facial-recognizer.max.us-south.containers.appdomain.cloud/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: multipart/form-data\" -F \"image=@childSoldier.jpg;type=image/jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client side -\n",
    "## If time permits use the server side version. \n",
    "This is a nice presentation since, easier todo since I do not needs to put synchronize the main image an the crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_emotion_(imgRegion):\n",
    "    \"\"\"get emotion from a region, \n",
    "    Args:\n",
    "        imgRegion: PIL.Image.Image: cropped region that with an face image for analysis\n",
    "\n",
    "    ..note:: The region extracted from a image using analysis to locate faces. \n",
    "        \n",
    "    \"\"\"\n",
    "    predict_url='http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/model/predict'\n",
    "    \n",
    "    imgByteArr = io.BytesIO()\n",
    "    imgRegion.save(imgByteArr, format='jpeg')\n",
    "    imgByteArr = imgByteArr.getvalue()\n",
    "\n",
    "    files = {'image': (\"fred.jpg\", imgByteArr, \"image/jpeg\")}\n",
    "    try: \n",
    "        r = requests.post(predict_url, files=files)\n",
    "    except Exception as e:\n",
    "        print(\"Analysis service exception\", e)\n",
    "        return None\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Analysis failure:\",r.status_code, r.json())\n",
    "        return None\n",
    "    analysis = r.json()\n",
    "    if len(analysis['predictions']) == 0:\n",
    "        return list()\n",
    "    emotions = analysis['predictions'][0]['emotion_predictions']\n",
    "    return [{emot['label']:float(\"{0:.2f}\".format(emot['probability'])) for emot in emotions}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,  ImageDraw  # https://pillow.readthedocs.io/en/4.3.x/\n",
    "import requests  # http://docs.python-requests.org/en/master/\n",
    "# color link https://pillow.readthedocs.io/en/3.0.0/_modules/PIL/ImageColor.html\n",
    "def line_box(ele):\n",
    "    \"\"\"build a box with lines.\"\"\"\n",
    "    return (ele[0],ele[1],ele[0],ele[3],ele[2],ele[3],ele[2],ele[1],ele[0],ele[1])\n",
    "\n",
    "def image_percent(img_dim, box_extent):\n",
    "    \"\"\"get the % of image the box is \"\"\"\n",
    "    img_size = img_dim[0] * img_dim[1]\n",
    "    box_size = abs((int(box_extent[0]) - int(box_extent[2])) * (int(box_extent[1])- int(box_extent[3])))\n",
    "    percent = ((box_size/img_size) * 100)\n",
    "    print(\"Img size :{} Box size:{}  {}%\".format(img_size, box_size, percent))\n",
    "    return(percent)\n",
    "\n",
    "def resize_image(bin_image, basewidth=None, baseheight=None):\n",
    "    \"\"\"Resize image proportional to the base, make it fit in cell\"\"\"\n",
    "    if basewidth is not None:\n",
    "        wpercent = (basewidth/float(bin_image.size[0]))\n",
    "        hsize = int((float(bin_image.size[1])*float(wpercent)))\n",
    "        return bin_image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    wpercent = (baseheight/float(bin_image.size[1]))\n",
    "    wsize = int((float(bin_image.size[0])*float(wpercent)))\n",
    "    return bin_image.resize((wsize,baseheight), Image.ANTIALIAS)\n",
    "\n",
    "# example image url: https://m.media-amazon.com/images/S/aplus-media/vc/6a9569ab-cb8e-46d9-8aea-a7022e58c74a.jpg\n",
    "def face_cropper(bin_image, faces):\n",
    "    \"\"\"Crop out the faces from a URL using detection_box and send to analysis.\n",
    "    Args:\n",
    "        url : image images\n",
    "        faces : list of {region,predictions} that that should be cropped\n",
    "    Return:\n",
    "        dict with 'annotated_image' and 'crops'\n",
    "        'crops' is list of dicts with \n",
    "            {image:face image, \n",
    "             probability:chances it's a face, \n",
    "             image_percent:found reqion % of of the image, \n",
    "             detection_box:region of the original image that the image was extacted from}\n",
    "         'crops' empty - nothing found, no faces found\n",
    "    \"\"\"\n",
    "    crops = list()\n",
    "    draw = ImageDraw.Draw(bin_image) \n",
    "    for face in faces['predictions']:         \n",
    "        #print(face['probability'],face['detection_box'])\n",
    "        percent = image_percent( bin_image.size, face['detection_box'])\n",
    "        img = bin_image.crop(face['detection_box'])\n",
    "        draw.line(line_box(face['detection_box']), fill=\"orange\", width=5)\n",
    "        #draw.rectangle(face['detection_box'], fill=128)\n",
    "        crops.append({'image':img, 'probability':face['probability'],'detection_box':face['detection_box'],'image_percent':percent})\n",
    "    return {'annotated_image':resize_image(bin_image, baseheight=400), 'crops':crops}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull some data from view\n",
    "_view = instance.get_views(name=\"imgActive\")[0]\n",
    "_view.start_data_fetch()\n",
    "view_tuples = _view.fetch_tuples(max_tuples=100, timeout=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_bar = list()  # setup in layout section.\n",
    "def bar_cell(idx, crop):\n",
    "    \"\"\"Display cropped data in cell below main photo.\"\"\"\n",
    "    assert idx < len(crops_bar), \"crops_bar exceeded idx:{} max:{}\".format(idx,len(crops_bar))\n",
    "    \n",
    "    with crops_bar[idx]['image']:\n",
    "        display(resize_image(crop['image'],basewidth=100))\n",
    "        clear_output(wait=True)\n",
    "    crops_bar[idx]['probability'].value = \"prob/confidence : {0:.2f}\".format(crop['probability'])\n",
    "    crops_bar[idx]['image_percent'].value = \"img {0:.2f}%\".format(crop['image_percent'])\n",
    "\n",
    "## Layout the dashboard cells \n",
    "url_widget = widgets.Label(value=\"Img URL\", layout={'border': '1px solid red','width':'100%'})\n",
    "full_widget = widgets.Output(layout={'border': '1px solid red','width':'100%','height':'300pt'})\n",
    "title_widget = widgets.Label(value=\"Title\", layout={'border': '1px solid red','width':'30%'})\n",
    "\n",
    "vbox_bar = list()\n",
    "for idx in range(7):\n",
    "    vbox = {\n",
    "        'probability' : widgets.Label(value=\"prop:{}\".format(idx), layout={'border': '1px solid blue','width':'100pt'}),\n",
    "        'image_percent' : widgets.Label(value=\"image %\", layout={'border': '1px solid blue','width':'100pt'}),\n",
    "        'image' : widgets.Output(layout={'border': '1px solid blue','width':'100pt','height':'120pt'})\n",
    "    }\n",
    "    crops_bar.append(vbox)\n",
    "    vbox_bar.append(widgets.VBox([vbox['probability'], vbox['image_percent'], vbox['image']]))\n",
    "    \n",
    "display(widgets.VBox([full_widget,widgets.HBox(vbox_bar)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose the dashboard - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_tuples = _view.fetch_tuples(max_tuples=100, timeout=4)\n",
    "img_urls = [tup['img_desc'][0]['img'] for tup in view_tuples if tup['img_desc'] is not None]\n",
    "print(\"Images URLs to process:{}\".format(len(img_urls)))\n",
    "for img_url in img_urls:\n",
    "    print(\"processing image\", img_url )\n",
    "    faces = facial_locate(img_url)\n",
    "    if faces is None or len(faces) == 0:\n",
    "        continue\n",
    "    print(\"facial fetch status:\", faces['status'])\n",
    "    r = requests.get(img_url, timeout=4.0)\n",
    "    if r.status_code != requests.codes.ok:\n",
    "        assert False, 'Status code error: {}.'.format(r.status_code)\n",
    "    with Image.open(io.BytesIO(r.content)) as bin_image:\n",
    "        face_crops = face_cropper(bin_image, faces)\n",
    "        with full_widget:\n",
    "            display(face_crops['annotated_image'])\n",
    "            clear_output(wait=True)            \n",
    "        idx = 0\n",
    "        probs = [crop['probability'] for crop in face_crops['crops']]\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        sorted_face_crops = [face_crops['crops'][idx] for idx in sorted_indices[:7]]\n",
    "        for crop in sorted_face_crops:\n",
    "            bar_cell(idx, crop)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imageEmotionAnalysis'></a>\n",
    "## Image Emotion Classification with [MAX](https://developer.ibm.com/exchanges/models/)\n",
    "\n",
    "Using [IBM Facial Emotion Classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/)\n",
    "\n",
    "[Jump Table](#jumpTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropped image emotion classification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_view = instance.get_views(name=\"emotionImg\")[0]\n",
    "_view.start_data_fetch()\n",
    "view_tuples = _view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "len(view_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tuple in view_tuples:\n",
    "    bin64 = tuple['face']['bytes_PIL_b64']\n",
    "    cropImg = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "#cropImg = Image.open(io.BytesIO(res[0]['face']['bytes_PIL']))\n",
    "    print(\"crop size:{0}, cropped image % of original:{3:.2f}, probability image is a face:{1:.2f}\\n img:{2}\".format(cropImg.size, \n",
    "                                                                       tuple['face']['probability'], \n",
    "                                                                       tuple['img_desc'][0]['img'],\n",
    "                                                                       tuple['face']['image_percent']))\n",
    "    print(\"emotion\", tuple['emotion'])\n",
    "    cropImg.thumbnail([128,128])\n",
    "    display(cropImg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO update so use image sent from server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_stage = list()\n",
    "\n",
    "\n",
    "order_index = ['surprise', 'happiness', 'contempt', 'neutral', 'sadness', 'anger', 'disgust', 'fear']\n",
    "@catchInterrupt\n",
    "def display_analysis(tup, emotion, image_region=None, title_region=None, graph_region=None, score_region=None, url_region=None):\n",
    "    display_desc = tup['img_desc'][0]\n",
    "    analysis_stage.append(tup)\n",
    "    title_region.value = \"Img Title:{}\".format(display_desc['title'] )\n",
    "    if len(emotion) == 0:\n",
    "        score_region.value = \"no score title : {}\".format(tup['title'])\n",
    "        return\n",
    "    url_region.value = tup['img_desc'][0]['img']\n",
    "    score_region.value = \"scored: {}\".format(tup['title'])\n",
    "    render_image(tup['img_desc'][0]['img'], output_region=image_region)\n",
    "    emot = [emotion[0][key] for key in order_index]\n",
    "    with graph_widget:\n",
    "        #df = pandas.DataFrame(emotion[0], index=order_index)\n",
    "        df = pandas.DataFrame(emot, index=order_index)\n",
    "        df.plot.bar(rot=90)\n",
    "        plt.ylim(0.0, 1)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@catchInterrupt\n",
    "def server_analysis(tuples):\n",
    "    for idx in range(len(tuples)):\n",
    "        if tuples[idx]['emotion'] is not None:\n",
    "            emotion = tuples[idx]['emotion']\n",
    "            if len(emotion) != 0:\n",
    "                print(emotion)\n",
    "            display_analysis(tuples[idx], emotion, image_region=anaimg_widget, \n",
    "                             title_region=title_widget, \n",
    "                             graph_region=graph_widget,\n",
    "                             score_region=score_widget,\n",
    "                             url_region=url_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "status_widget = widgets.Label(value=\"Status\", layout={'border': '1px solid green','width':'40%'})\n",
    "anaimg_widget = widgets.Output(layout={'border': '1px solid red','width':'40%','height':'270pt'})\n",
    "score_widget = widgets.Label(value=\"Analysis\", layout={'border': '1px solid green','width':'40%'})\n",
    "graph_widget = widgets.Output(layout={'border': '1px solid blue','width':'40%','height':'240pt'})\n",
    "url_widget = widgets.Label(value=\"Img URL\", layout={'border': '1px solid green','width':'100%'})\n",
    "title_widget = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'30%'})\n",
    "\n",
    "\n",
    "dashboard = VBox([status_widget, anaimg_widget,score_widget, graph_widget, url_widget])\n",
    "display(dashboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Continious processing\n",
    "_view = instance.get_views(name=\"emotionImg\")[0]\n",
    "_view.start_data_fetch()\n",
    "\n",
    "@catchInterrupt\n",
    "def streaming_analysis(iterations=30):\n",
    "    \"\"\"Analyse image are being submitted to wikiepedia, the analysis In this case is the \n",
    "    'Facial Emotion Calssifier'\n",
    "    \n",
    "    This renders image and emotional rating of the image. All the images found \n",
    "    are analysized, only those images that result in a emotion characterization\n",
    "    are flow down this branch of the flow. \n",
    "    \n",
    "    In theory only images of people, with the face exposed, warrent an emotional \n",
    "    characterization - this is not current state.\n",
    "    \n",
    "    note::\n",
    "        - This leaves up the last successful image and analysis. \n",
    "        - A interation does not necessarily get an image.\n",
    "        - In theory only images of people, with the face exposed, warrent an emotional characterization - this is not current state.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Args::\n",
    "        iterations: obvious, <1 is forever \n",
    "    \"\"\"\n",
    "\n",
    "    while iterations:\n",
    "        iterations -= 1\n",
    "        status_widget.value = \"Status: iterations remaining: {}\".format(iterations if iterations > 0 else \"infinity\")\n",
    "        view_tuples = _view.fetch_tuples(max_tuples=100, timeout=4)\n",
    "        server_analysis(view_tuples)\n",
    "    status_widget.value = \"Status:...DONE retriving analyzed image and results.\"\n",
    "streaming_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel the Streams job, not so much for the wikipedia hit but for the analysis hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
