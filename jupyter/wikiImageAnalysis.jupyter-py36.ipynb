{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    print(\"JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"])\n",
    "except KeyError as err:\n",
    "    os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home\"\n",
    "    print(\"Had to set JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"],\"\\n   --- what is going on here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Streaming of Wiki and Video \n",
    "**The big picture**\n",
    "\n",
    "[WikiRecentPhase0](imgAna_0.jupyter-py36.ipynb) illustrated accessing continious streams events from a notebook in Python as a way to explore the nature of the data and to understand how to develop it into useful insights. In that exploration data is collected, averages are calculated only as long as the notebook open and specific cells are running. That model makes it challenging to make business use of insights derived from the stream.\n",
    "\n",
    "\n",
    "In this notebook we will transition the work we did with the data in step 0 to a continuously running Streams job, develop the insights further and show how the Notebook can be used to access the outputs of the job for user visualization and action. This notebook is composed of the number applications each of which refines the \n",
    "the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='DIRECTORY'>Directory</a>\n",
    "- [Overview ](#OVERVIEW)\n",
    "- [Setup - Helper functions connect to service](#SETUP)\n",
    "    - [Setup Imports](#setupImports)\n",
    "    - [Setup Credentials](#setupCredentials)\n",
    "- [Collecting Wiki Events (WikiPhase1)](#PHASE1)\n",
    "     - [View data collected by the first Streams application](#PHASE1_RENDER)\n",
    "- [Aggregating and Filtering (WikiPhase2)](#PHASE2)\n",
    "    -  [Phase2 Render- WikiPhase2](#PHASE2_RENDER)\n",
    "- [Shred Page with Beautiful Soup - (WikiPhase3)](#PHASE3)\n",
    "    -  [Render Shredding](#PHASE3_RENDER)\n",
    "- [Image Analysis - (ObjectAnalysis & FaceAnalysis)](#PHASE4)\n",
    "    -  [Render Image Analysis](#PHASE4_RENDER)\n",
    "- [Recieve Video Frames via Kafka - VideoRcvKafka](#PHASE5)\n",
    "- [Image Processing only - ImageProcessingOnly](#PHASE6)\n",
    "\n",
    "\n",
    "-  [Back to Directroy](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"Overview  - Continious processing with Streams\">OVERVIEW</a>\n",
    "![graph of application](images/allApplications.png)\n",
    "\n",
    "## Wiki\n",
    "**About the sample** \n",
    "\n",
    "The appliction recieves Wikipedia updates as events via SSE. An event\n",
    "has a 'bots' field indicating that it was generated by a robot. Many robots exist Wikipedia to perform\n",
    "mandane checking tasks: checking, copyright infringment,inappropriate content...\n",
    "This application will focus on messages generated by humans filtering out those from 'bots'. \n",
    "The event has a number of superflous fields that are removed before it is presented to 'view'. \n",
    "\n",
    "A 'view'enables a developer to observe data that is flowing through the live stream, data presented to the \n",
    "view is available for display over the web. Due to the realtime nature of Streams the view may and the \n",
    "limitations of web communcations a view will drop observations as resources (CPU, memory, bandwidy) become limited. \n",
    "This has not been an issue with this suite of notebooks.\n",
    "\n",
    "The application is composed and submitted from the notebook, it runs on a server continiously until it is stopped. \n",
    "This notebook accesses and renders view data in order to see how the Streams is processing the events \n",
    "recieved from Wikipedia. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "- [Topology](https://streamsxtopology.readthedocs.io/en/latest/index.html) Streams Topology documentation\n",
    "- [Widgets](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Basics.html) Notebook Widgets documentation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Submit the appplication that has a filter. \n",
    "\n",
    "- rough cut look at the data\n",
    "- plot the 'type' data\n",
    "- look at the filtered data\n",
    "\n",
    "##  Collect in buffer / Aggregate\n",
    "- Last 1000 in local buffer & aggregate\n",
    "- Render\n",
    "- Push code to server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='SETUP'>Setup</a>\n",
    "\n",
    "## <a id='setupImports'>Setup Imports</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Install components\n",
    "!pip --user install SSEClient===0.0.22 --upgrade\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports are in support of accessing Streams with Wikipedia data, \n",
    "subsequent are in support of rendering the results as they flow back from \n",
    "Streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "\n",
    "from  functools import lru_cache\n",
    "from statistics import mean\n",
    "import collections\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sseclient import SSEClient as EventSource\n",
    "import requests\n",
    "\n",
    "import streamsx\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "    \n",
    "import ipynb\n",
    "#from ipynb.fs.full.sharedCode import get_instance\n",
    "#from ipynb.fs.full.sharedCode import list_jobs\n",
    "#from ipynb.fs.full.sharedCode import common_submit\n",
    "#from ipynb.fs.full.sharedCode import display_views\n",
    "#from ipynb.fs.full.sharedCode import catchInterrupt\n",
    "\n",
    "#import wiki_streams\n",
    "\n",
    "import streams_aid as aid\n",
    "import streams_render as render\n",
    "import credential\n",
    "import cvsupport\n",
    "print(\"streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions used in interfacing  to Streams  \n",
    "\n",
    "Their are number of helper functions to make it aid in the development of Streams applicatons, refer to scripts/streams_aid.py or utilize.\n",
    "\n",
    "For details on the avaliable aid functions, use the help command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='setupCredentials'>Setup Credentials</a>\n",
    "\n",
    "The service that you use defines what you need to setup.\n",
    "\n",
    "- Describe CP4D\n",
    "- Describe Cloud \n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICP4D setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from icpd_core import icpd_util\n",
    "cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud setup\n",
    "To use Streams instance running in the cloud setup a [credential.py](setup_credential.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# If your using the cloud uncomment the next line.\n",
    "# Credential data \n",
    "SERVICE_NAME='Streaming3Turbine'\n",
    "#SERVICE_NAME='StreamingHourly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# For details uncomment the next line. \n",
    "# help(aid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Connect to the server :  ICP4D or Cloud instance -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# The code is setup to run in the Cloud and CP4D. \n",
    "# If in the Cloud, you'll see a message.\n",
    "# If in CP4D, you'll need the service credential, which what the above link explains. \n",
    "#    Delete this cell and use the above instructions if you only using CP4D.\n",
    "\n",
    "try:\n",
    "    from icpd_core import icpd_util\n",
    "except ModuleNotFoundError as e:  # get all exceptions\n",
    "    instance,cfg = aid.get_instance(service_name=SERVICE_NAME)\n",
    "\n",
    "else:   # runs when no exception occurs\n",
    "    cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Connected\n",
    "When you're up to this point you have established connection to the Streams instance. The cell below shows, if any, the applications that are running. For the applications that are running thier corresponding rendering codes could be executed without going through the process of submitting the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next compose and submit the first appication or back to the [Directory](#DIRECTORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thier may not be any jobs currently running. \n",
    "\n",
    "Continue on to compose an application or [back to Directory](#DIRECTORY) \n",
    "\n",
    "<tr style=\"border-bottom: 1px solid #000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='PHASE1'>Collecting Wiki events (WikiPhase1)</a>\n",
    "- get data from wiki usng SSE\n",
    "- filter data, seperate out the humans and RObots\n",
    "- setup views : allEvents, allHumans, paredHumans, paredAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive messages updates from Wikipedia\n",
    "As updates are made to Wikipidia pages the changs are sent over and SSE feed. The get_events() function recieves the events and acting as a [source](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Topology.source) pushes them onto the Streams streams.\n",
    "\n",
    "This is functional the same code as the 'imgAna0' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"fetch recent changes from wikievents site using SSE\"\"\"\n",
    "    for change in EventSource('https://stream.wikimedia.org/v2/stream/recentchange'):\n",
    "        if len(change.data):\n",
    "            try:\n",
    "                obj = json.loads(change.data)\n",
    "            except json.JSONDecodeError as err:\n",
    "                print(\"JSON l1 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            except json.decoder.JSONDecodeError as err:\n",
    "                print(\"JSON l2 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            else:\n",
    "                yield(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter messages\n",
    "The [filter](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.filter) is used to break out messages not generated by robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View into the live stream\n",
    "The [view](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.view) enables access to live stream at runtime. We spread them liberaly throughout the application to observe how the processing is procedeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose, build and submit the Streams application.¶\n",
    "\n",
    "The following Code cell composes the Streams application depicted here:\n",
    "\n",
    "![graph of application](images/phase1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a single event source (wikiEvents)' we are creating a topology that delivers two streams of derived information.\n",
    "To make each step in this process accessible to examination we are declaring a view at each stage to look at the raw data and also to attach graphical visualizations. \n",
    "\n",
    "A Steams flow is composed by linking operator outputs to inputs.\n",
    "\n",
    "Two streams are being composed and three views are exposed\n",
    "\n",
    "1) All source events are sent to 'allEvents' view where they are exposed for viewing.\n",
    "\n",
    "2) All the source events sent to 'paredAll', where they are pared down to 6 fields and exposed for viewing. \n",
    "- the Wikipeida events are output from the 'source' method named 'wikiEvents on 'wiki_events' \n",
    "- the input to the 'map' method named 'paredAll'is wiki_events where fields  ('timestamp',type','wiki','bot','user','title') are extracted and output to 'pared_all'. \n",
    "- the input to 'view' method is 'named allEvents is 'pared_all' where the are exposed for viewing. \n",
    "\n",
    "3) All the source events are sent to 'humansOnly' that drops 'bot' fields of False, they are then are pared down to 5 fields and exposed for viewing.\n",
    "- the Wikipeida events are output from the 'source' method named 'wikiEvents on 'wiki_events' \n",
    "- the input to the 'filter' method named 'humanFilter' only events in whih the 'bot' field set to False are output on all_human. \n",
    "- the input to the 'map' method named 'paredHuman' is all_human where fields  ('timestamp',type','wiki','user','title') are extracted and output to 'pared_human'. \n",
    "- the input to 'view' method is named 'paredHuman' is 'pared_human' where the are exposed for viewing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def WikiPhase1(jobName=None):\n",
    "    \"\"\"\n",
    "    Compose topology \n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    topo.add_pip_package('SSEClient===0.0.22')\n",
    "\n",
    "    ## Receive wiki data \n",
    "    wiki_events = topo.source(get_events, name=\"wikiEvents\")\n",
    "    wiki_events.view(buffer_time=1.0, sample_size=1, name=\"allEvents\", description=\"All wiki events\")\n",
    "    \n",
    "    ## drop fields \n",
    "    pared_all = wiki_events.map(lambda x : {'timestamp':x['timestamp'],'type':x['type'],'wiki':x['wiki'],'bot':x['bot'],'user':x['user'],'title':x['title']}, name=\"paredAll\")\n",
    "    pared_all.view(buffer_time=1.0, sample_size=200, name=\"paredAll\", description=\"All events pared\")\n",
    "    \n",
    "    ## Filter out bots and non edits\n",
    "    human_filter = wiki_events.filter(lambda x: x['type']=='edit' and x['bot'] is False, name='humanFilter')\n",
    "    # pare down the humans set of columns\n",
    "    pared_human= human_filter.map(lambda x : {'timestamp':x['timestamp'],\n",
    "                                              'new_len':x['length']['new'],\n",
    "                                              'old_len':x['length']['old'], \n",
    "                                              'delta_len':x['length']['new'] - x['length']['old'],\n",
    "                                              'wiki':x['wiki'],'user':x['user'],\n",
    "                                              'title':x['title']}, \n",
    "                        name=\"paredHuman\")\n",
    "\n",
    "\n",
    "    ## drop fields\n",
    "    pared_human.view(buffer_time=1.0, sample_size=200, name=\"paredHuman\", description=\"Human events pared\")\n",
    "    pared_human.publish(topic=\"pared_human\", name=\"pubParedHuman\")\n",
    "    \n",
    "    return topo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit topology : build/submit (ICP4D or Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aid.cloudSubmit(instance, SERVICE_NAME, WikiPhase1(jobName=\"WikiPhase1\"), credential=credential)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next view the data being collected by Streams or [back to Directory](#DIRECTORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='PHASE1_RENDER'>View data collected by the first Streams application</a>\n",
    "\n",
    "The running application has number of views to see what what data is moving through the stream. The following \n",
    "cell will fetch the views' queue and dipslay it's data when selected. \n",
    "\n",
    "|view name | description of data is the view |\n",
    "|---------|-------------|\n",
    "|allEvents  | all fields of all events  |\n",
    "|paredAll | subset of fields for all events |\n",
    "|paredHumans | subset of fields of all events where field 'bot is **False**|\n",
    "\n",
    "Running the cell below will bring up the list of active views from which data can be seen. \n",
    "\n",
    "When done 'Stop Updating', continious Widget updates are browser resource expensive. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Render the views.....\n",
    "aid.display_views(instance, job_name=\"WikiPhase1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing live data:\n",
    "- constructing the view object : https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html?highlight=view#streamsx.topology.topology.Stream.view\n",
    "- methods on the view object : https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html?highlight=view#streamsx.topology.topology.View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph frequency of 'type' events and 'bots'\n",
    "\n",
    "The following is a example of using widgets to view the live data, accessing the 'paredAll' view data, we're able to\n",
    "see the counts of edit types to bots/non-bot (human) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# tally the the bots/types  - SHOW GRAPH\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "\n",
    "@aid.catchInterrupt\n",
    "def tally_bot(view, ele_window=20, count=20):\n",
    "    \"\"\"bots vs human updates \n",
    "    Args:\n",
    "        view: Streams view that data will be fetched from\n",
    "        ele_window : max number of elements in window\n",
    "        count: number of times to fetch data, < 0 until interrupt \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    nam_list = ['new', 'edit', 'log','categorize']\n",
    "    cntbot = {key:0 for key in nam_list}\n",
    "    cntnobot = {key:0 for key in nam_list}\n",
    "    view.start_data_fetch()\n",
    "\n",
    "    while count != 0:\n",
    "        count -= 1\n",
    "        listTuples= view.fetch_tuples(max_tuples=20, timeout=4)\n",
    "        cntbot = Counter({key:0 for key in nam_list})\n",
    "        cntnobot = Counter({key:0 for key in nam_list})\n",
    "        for evt in listTuples:\n",
    "            if evt['bot']:\n",
    "                cntbot[evt['type']] += 1\n",
    "            else:\n",
    "                cntnobot[evt['type']] += 1\n",
    "        bot_list = [cntbot[key] for key in nam_list]\n",
    "        nobot_list = [cntnobot[key] for key in nam_list]\n",
    "        \n",
    "        df = pd.DataFrame({'bot': bot_list, ' nobot': nobot_list}, index=nam_list)\n",
    "        df.plot.bar(rot=0, stacked=True)\n",
    "        plt.ylim(0.0, ele_window)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "view = aid.get_view(instance, job_name=\"WikiPhase1\", view_name=\"paredAll\")\n",
    "tally_bot(view=view[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel job when done. \n",
    "If you not going onth next phase, cancel the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aid.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiPhase1 application wrapup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook composed and deployed a Streams application that processes live Wikipedia events on a server. After \n",
    "filtering the events they're pared down and made avaiable via a view. Using standard notebook widgets\n",
    "we rendered the view data. \n",
    "\n",
    "This application is running on server, once deployed it is not necessary to have notebook open and executing in order to process the data. Future notebooks will collect and average data over windows of events and time. \n",
    "\n",
    "[Back to Directroy](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='PHASE2'>Aggregating and Filtering - WikiPhase2</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of application](images/phase2.png)\n",
    "\n",
    "#Aggegating the Wiki data collected by the WikiPhase1 application\n",
    "\n",
    "The following functions that will be executing within the deployed Streams application. \n",
    "The functions a [composed](#composeBuildSubmit) into an application that is compiled and submitted to the Streams instance.\n",
    "\n",
    "## Aggregating tuples\n",
    "The [aggregation](http://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#streamsx.topology.topology.Window.aggregate) which is performed\n",
    "accross a list of tuples. The list is defined by a [window](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Window) which can be count or timed based with \n",
    "various options. \n",
    "in general the function is invoked when the window's\n",
    "critera are met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class sum_aggregation():\n",
    "    def __init__(self, sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }):\n",
    "        \"\"\"\n",
    "        Summation of column(s) over a window's tuples. \n",
    "        Args::\n",
    "            sum_map :  specfify tuple columns to be summed and the result field. \n",
    "            tuples : at run time, list of tuples will flow in. Sum each fields\n",
    "        \"\"\"\n",
    "        self.sum_map = sum_map\n",
    "    def __call__(self, tuples)->dict: \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tuples : list of tuples constituting a window, over all the tuples sum using the sum_map key/value \n",
    "                     to specify the input and result field.\n",
    "        Returns:\n",
    "            dictionary of fields summations over tuples\n",
    "            \n",
    "        \"\"\"\n",
    "        summaries = dict()\n",
    "        for summary_field,result_field in self.sum_map.items():\n",
    "            summation = sum([ele[summary_field] for ele in tuples])\n",
    "            summaries.update({result_field : summation})\n",
    "        return(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tallying fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "class tally_fields(object):\n",
    "    def __init__(self, top_count=3, fields=['user', 'wiki', 'title']):\n",
    "        \"\"\"\n",
    "        Tally fields of a list of tuples.\n",
    "        Args::\n",
    "            fields :  fields of tuples that are to be tallied\n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.top_count = top_count\n",
    "    def __call__(self, tuples)->dict:\n",
    "        \"\"\"\n",
    "        Args::\n",
    "            tuples : list of tuples tallying to perform. \n",
    "        return::\n",
    "            dict of tallies\n",
    "        \"\"\"\n",
    "        tallies = dict()\n",
    "        for field in self.fields:\n",
    "            stage = [tuple[field] for tuple in tuples if tuple[field] is not None]\n",
    "            tallies[field] = collections.Counter(stage).most_common(self.top_count)\n",
    "        return tallies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping tuples\n",
    "\n",
    "The event's 'wiki' field is the database that events pertains to. The database has a primary language which muliple databases share a common language. We'll use aht wikimap.csv to reconcile the events message using the augment_lang\n",
    "function using [map](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.map) to drive the processing.\n",
    "\n",
    "When defining a class be aware that the __init__() is executed locally at compile invocation when the self value is pickeld. Before call is __call__() invoked at runtime the self is depickeled. In the augmentation_lang.__init__() code \n",
    "below, csv file is read into dict that maps from 'wiki' to 'language' and saved to *self*. At runtime the *self* with it's wiki/language mapping reconsile the language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "class wiki_lang():\n",
    "    \"\"\"\n",
    "    Augment the tuple to include language wiki event.\n",
    "    \n",
    "    Mapping is loaded at build time and utilized at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fname=\"wikimap.csv\"):\n",
    "        self.wiki_map = dict()\n",
    "        with open(fname, mode='r') as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            for row in csv_reader:\n",
    "                self.wiki_map[row['dbname']] = row\n",
    "\n",
    "    def __call__(self, tuple):\n",
    "        \"\"\"using 'wiki' field to look pages code, langauge and native\n",
    "        Args:\n",
    "            tuple: tuple (dict) with a 'wiki' fields\n",
    "        Returns:'\n",
    "            input tuple with  'code', 'language, 'native' fields added to the input tuple.\n",
    "        \"\"\"\n",
    "        if tuple['wiki'] in self.wiki_map:\n",
    "            key = tuple['wiki']\n",
    "            tuple['code'] = self.wiki_map[key]['code']\n",
    "            tuple['language'] = self.wiki_map[key]['in_english']\n",
    "            tuple['native'] = self.wiki_map[key]['name_language']\n",
    "        else:\n",
    "            tuple['code'] = tuple['language'] = tuple['native'] = None\n",
    "        return tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose, build and submit the Streams application.\n",
    "The following Code cell composed the Streams application depicted here:\n",
    "\n",
    "This is application build on the previous, I'll only discuss operators beyond 'paredHuman' details of this operator and prior can be found in the previous [notebook](./imgAna_1.ipynb).\n",
    "\n",
    "The events output by the map named 'paredHuman' are limited to those with of 'type' i'edit' and bot is 'False', \n",
    "and a reduced set fields. \n",
    "\n",
    "Three streams and thier views are composed from here. \n",
    "\n",
    "1) The aggregate method named 'talliesTop' is preceded by a windowing operation that stages 100 tuples in increments \n",
    "of 10. 'talliesTop' tuples are processed by the 'tallies_field()' object that tallies the 'user' and 'titles' fields and returns the results in dict. The dict is forwarded onto the view named 'talliesTop'.\n",
    "\n",
    "2) The aggregate method named 'sumAggregation' is preceded by windowing that stages 100 with increment of 20. 'sumAggregation' tuples are processed by 'sum_aggregation()' that returns a dict the view named 'sumAggregate' publishes.\n",
    "\n",
    "3) The map method named 'langAugment' invokes wiki_lang() with each tuple. The wiki_lang() method expands the tuple \n",
    "by three fields 'native', 'code', 'language' keying off the 'wiki' field. The resulting tuple is sent onto aggregate\n",
    "named 'timeLang' is preceded by a windoing operation that stages 2 minutes of tuples in increments of 5. \n",
    "'langAugment' tuples are processed by the 'tallies_fields()' object that tallies the 'langugage' fields and returns\n",
    "the results in a dict. The dict is forwarded onto a view named 'langAugment'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def WikiPhase2(jobName=None):\n",
    "    \"\"\"\n",
    "    UPDATED \n",
    "    -- wiki_lang : csv file mapping database name to langauge\n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    ### make sure we sseclient in Streams environment.\n",
    "\n",
    "    pared_human = topo.subscribe(topic=\"pared_human\", name=\"subParedHuman\")\n",
    "\n",
    "    ## Define window(count)& aggregate\n",
    "    sum_win = pared_human.last(100).trigger(20)\n",
    "    sum_aggregate = sum_win.aggregate(sum_aggregation(sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }), name=\"sumAggregate\")\n",
    "    sum_aggregate.view(buffer_time=1.0, sample_size=200, name=\"aggEdits\", description=\"Aggregations of human edits\")\n",
    "\n",
    "    ## Define window(count) & tally edits\n",
    "    tally_win = pared_human.last(100).trigger(10)\n",
    "    tally_top = tally_win.aggregate(tally_fields(fields=['user', 'title'], top_count=10), name=\"talliesTop\")\n",
    "    tally_top.view(buffer_time=1.0, sample_size=200, name=\"talliesCount\", description=\"Top count tallies: user,titles\")\n",
    "\n",
    "    ## augment filterd/pared edits with language\n",
    "    if cfg is None:        \n",
    "        lang_augment = pared_human.map(wiki_lang(fname='../datasets/wikimap.csv'), name=\"langAugment\")\n",
    "    else:\n",
    "        lang_augment = pared_human.map(wiki_lang(fname=os.environ['DSX_PROJECT_DIR']+'/datasets/wikimap.csv'), name=\"langAugment\")\n",
    "    lang_augment.view(buffer_time=1.0, sample_size=200, name=\"langAugment\", description=\"Language derived from wiki\")\n",
    "\n",
    "    ## Define window(time) & tally language\n",
    "    time_lang_win = lang_augment.last(datetime.timedelta(minutes=2)).trigger(5)\n",
    "    time_lang = time_lang_win.aggregate(tally_fields(fields=['language'], top_count=10), name=\"timeLang\")\n",
    "    time_lang.view(buffer_time=1.0, sample_size=200, name=\"talliesTime\", description=\"Top timed tallies: language\")\n",
    "   \n",
    "    ## publish lang_augment\n",
    "    lang_augment.publish(topic=\"lang_augment\", name=\"pubLangAugment\")\n",
    "    return topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aid.cloudSubmit(instance, SERVICE_NAME, WikiPhase2(jobName=\"WikiPhase2\"), credential) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Directroy](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='PHASE2_RENDER'>Phase2 Render- WikiPhase2</a>\n",
    "The Streams application is sending out 5 views which you can seen using the wiget above. The following show the data in more favorable light. \n",
    "\n",
    "### Tallied languages\n",
    "\n",
    "We extreact the languge data and tallied it in Streams. This is showing the submitted events pertaining to article written in various languages in the last 2 minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO Cart  - SHOW PIE\n",
    "\n",
    "@aid.catchInterrupt\n",
    "def tally_pie(view=None, tally_field='language', count=10):\n",
    "    \"\"\"Pie chart of language that edits by humans being done in. \n",
    "    Args:\n",
    "        view: Streams view that data will be fetched from\n",
    "        tally_field: fields within view to get data.\n",
    "        count: number of times to fetch data, < 0 until interrupt \n",
    "    \n",
    "    \"\"\"\n",
    "    while (count != 0):\n",
    "        count -= 1\n",
    "        tuples = view.fetch_tuples(max_tuples=10, timeout=10 )\n",
    "        if len(tuples) is 0:\n",
    "            print(\"No new tuples @{}\".format(count))\n",
    "            time.sleep(3)\n",
    "            next\n",
    "        language= [lst[0] for lst in tuples[-1][tally_field]]\n",
    "        counts = [lst[-1] for lst in tuples[-1][tally_field]]\n",
    "        percent = counts[0]/sum(counts) * 100\n",
    "        print(\"[{2}]{0:4.2f}% of the events are in {1}, {1} is  dropped from the piechart.\".format(percent,language[0],\"+*\"[count%2]))\n",
    "        df = pd.DataFrame({'counts': counts[1:]}, index=language[1:])\n",
    "        df.plot.pie(y='counts')    \n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "view = instance.get_views(name=\"talliesTime\")[0]\n",
    "view.start_data_fetch()\n",
    "tally_pie(view, tally_field='language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tally users\n",
    "Show the users who have submitted the most events in the last 200 events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "@aid.catchInterrupt\n",
    "def tally_table(view=None, tally_field=\"title\", count=10):\n",
    "    \"\"\"Display a title data in a table\n",
    "    Args:\n",
    "        view: Streams view that data will be fetched from\n",
    "        tally_field: fields within view to get data.\n",
    "        count: number of times to fetch data, < 0 until interrupt \n",
    " \n",
    "    \"\"\"\n",
    "    while (count != 0):\n",
    "        count = count - 1\n",
    "        tallies = view.fetch_tuples(max_tuples=10, timeout=6)\n",
    "        if tallies is not None and len(tallies) != 0:\n",
    "            title_tallies = tallies[0][tally_field]\n",
    "            title = [ele[0] for ele in title_tallies]\n",
    "            cnt = [ele[1] for ele in title_tallies]\n",
    "            tbl = [(tally_field, title),('count', cnt)]\n",
    "            df = pd.DataFrame.from_items(tbl)\n",
    "            display(df)\n",
    "        else:\n",
    "            display(\"Fetch Fail count down value ... {}\".format(count))\n",
    "            time.sleep(3)\n",
    "        clear_output(wait=True)\n",
    "view = instance.get_views(name=\"talliesCount\")[0]\n",
    "view.start_data_fetch()\n",
    "tally_table(view, tally_field=\"user\", count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tally titles\n",
    "\n",
    "Show the most updated titles within the last 200 events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "view = instance.get_views(name=\"talliesCount\")[0]\n",
    "view.start_data_fetch()\n",
    "tally_table(view, tally_field=\"title\", count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapup WikiPhase2\n",
    "\n",
    "\n",
    "Continue onto the next phase or back up to directory....\n",
    "\n",
    "\n",
    "[Back to Directroy](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='PHASE3'>Shred Page with Beautiful Soup - WikiPhase3</a>\n",
    "![graph of application](images/phase3.png)\n",
    "The next phase of the Stream will be to check if the event is associated with an image, if it is extract the \n",
    "image URL. \n",
    "\n",
    "- find possible link to image\n",
    "- build url and use to fetch page, shred,  searching for an image link\n",
    "- shredding can go down mulitple levels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='composeBuildSubmit'></a>\n",
    "### Compose, build and submit the Streams application.\n",
    "\n",
    "\n",
    "This section subscribes to the previous, in this we be using the 'lang_augmement' subscription. \n",
    "\n",
    "The events output by the map named 'langAugment' are limited to those with of type 'edit' and bot is 'False'. The fields are: code, delta_len, language, native, new_len, old_len, timestamp, title, user and wiki. This phase uses the 'title' field to build a url of a webpage, the webpage is feched and processed looking for a image URL.\n",
    "\n",
    "The map method named 'imageSoup' invokes soup_image_extract() where it uses the 'title' field attempting to locate an image. If no image is found, None is returned and nothing flows out of the operator. If an image is found then the output includes a 'img_desc' field. A filter is applied to the 'img_desc' for content, if it does have content the tuple procedes to the view 'soupActive' where it can be viewed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# %load -s tally_fields ../scripts/wiki_streams.py\n",
    "class tally_fields(object):\n",
    "    def __init__(self, top_count=3, fields=['user', 'wiki', 'title']):\n",
    "        \"\"\"\n",
    "        Tally fields of a list of tuples.\n",
    "\n",
    "        Args::\n",
    "            fields :  fields of tuples that are to be tallied\n",
    "\n",
    "        Note::\n",
    "           Refer to the %load command's file for further details.\n",
    "\n",
    "        \"\"\"\n",
    "        import collections\n",
    "        self.fields = fields\n",
    "        self.top_count = top_count\n",
    "    def __call__(self, tuples)->dict:\n",
    "        \"\"\"\n",
    "        Args::\n",
    "            tuples : list of tuples tallying to perform.\n",
    "        return::\n",
    "            dict of tallies\n",
    "        \"\"\"\n",
    "        tallies = dict()\n",
    "        for field in self.fields:\n",
    "            stage = [tuple[field] for tuple in tuples if tuple[field] is not None]\n",
    "            tallies[field] = collections.Counter(stage).most_common(self.top_count)\n",
    "        return tallies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from streams_operations import soup_image_extract\n",
    "def WikiPhase3(jobName=None, wiki_lang_fname=None):\n",
    "    \"\"\"\n",
    "    Compose topology. \n",
    "    -- wiki_lang : csv file mapping database name to langauge\n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    ### make sure we sseclient in Streams environment.\n",
    "    topo.add_pip_package('SSEClient===0.0.22')\n",
    "    topo.add_pip_package('bs4')\n",
    "    topo.add_pip_package('ipynb')\n",
    "\n",
    "    lang_augment =topo.subscribe(topic=\"lang_augment\", name=\"subLangAugment\")\n",
    "    \n",
    "    ## Define window(time) & tally language\n",
    "    time_lang_win = lang_augment.last(datetime.timedelta(minutes=2)).trigger(5)\n",
    "    time_lang = time_lang_win.aggregate(tally_fields(fields=['language'], top_count=10), name=\"timeLang\")\n",
    "    time_lang.view(buffer_time=1.0, sample_size=200, name=\"talliesTime\", description=\"Top timed tallies: language\")\n",
    "\n",
    "    ## attempt to extract image using beautifulsoup add img_desc[{}] field\n",
    "    soup_image = lang_augment.map(soup_image_extract(field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"),name=\"imgSoup\")\n",
    "    soup_active = soup_image.filter(lambda x: x['img_desc'] is not None and len(x['img_desc']) > 0, name=\"soupActive\")\n",
    "    soup_active.view(buffer_time=1.0, sample_size=200, name=\"soupActive\", description=\"Image extracted via Bsoup\")\n",
    "    soup_active.publish(topic=\"soup_active\", name=\"pubSoupActive\")\n",
    "\n",
    "    return topo\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, WikiPhase3(jobName=\"WikiPhase3\"), credential) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The running application has number of views to see what what data is moving through the stream. The following \n",
    "cell will fetch the views' queue and display it's data when selected. \n",
    "\n",
    "|view name | description of data is the view | bot |\n",
    "|---------|-------------|------|\n",
    "|talliesTimes | 2 minute windowed | False |\n",
    "|soupActive | extracted images links| False | \n",
    "\n",
    "\n",
    "You want to stop the the fetching the view data when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aid.display_views(instance, job_name=\"WikiPhase3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Directroy](#DIRECTORY)\n",
    "\n",
    "## <a id='PHASE3_RENDER'>Render Shredding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acces Views / Render Views UI\n",
    "\n",
    "## Render image submitted to wiki feed \n",
    "Build dashboard to display images are being submitted to Wikipedia. \n",
    "\n",
    "It's not uncommon to see the  same image multiple times. An image (any content) may need to be vetted for \n",
    "quailty, copyright, pornograpy etc... Each vet stage generating another event on the Stream\n",
    "\n",
    "A variety of images are submitted, unfortunaly not all images are rendered in all browsers. I found that the Safari \n",
    "browser and render .tif files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Notebook support\n",
    "\n",
    "def render_image(image_url=None, output_region=None):\n",
    "    \"\"\"Write the image into a output region.\n",
    "    \n",
    "    Args::\n",
    "        url: image\n",
    "        output_region: output region\n",
    "        \n",
    "    .. note:: The creation of the output 'stage', if this is not done the image is rendered in the page and\n",
    "        the output region. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        stage = widgets.Output(layout={'border': '1px solid green'})\n",
    "    except:\n",
    "        print(\"Error on request : \", image_url)\n",
    "    else:\n",
    "        if response.status_code == 200:\n",
    "            with output_region:\n",
    "                stage.append_display_data(widgets.Image(\n",
    "                    value=response.content,\n",
    "                    #format='jpg',\n",
    "                    width=300,\n",
    "                    height=400,\n",
    "                ))\n",
    "            output_region.clear_output(wait=True) \n",
    "\n",
    "ana_stage = list()\n",
    "def display_image(tup, image_region=None, title_region=None, url_region=None):\n",
    "    if tup['img_desc'] is not None and len(tup['img_desc']) > 0:\n",
    "        display_desc = tup['img_desc'][0]\n",
    "        ana_stage.append(display_desc)\n",
    "        title_region.value = \"Img Title:{}\".format(display_desc['title'] )\n",
    "        url_region.value = \"{}\".format(display_desc['img'])\n",
    "        render_image(image_url=display_desc['img'], output_region=image_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Setup the Dashboard - display images sent to Wikipedia \n",
    "##                         Next cell populates the 'Dashboard'.....\n",
    "status_widget = widgets.Label(value=\"Status\", layout={'border': '1px solid green','width':'30%'})\n",
    "url_widget = widgets.Label(value=\"Img URL\", layout={'border': '1px solid green','width':'100%'})\n",
    "image_widget = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'270pt'})\n",
    "title_widget = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'30%'})\n",
    "\n",
    "dashboard = widgets.VBox([status_widget, image_widget, title_widget, url_widget])\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Notebook support - SHOW IMAGES\n",
    "# setup \n",
    "_view = instance.get_views(name=\"soupActive\")[0]\n",
    "_view.start_data_fetch()\n",
    "\n",
    "\n",
    "@aid.catchInterrupt\n",
    "def server_soup(count=25):\n",
    "    \"\"\"Fetch and display images from view.\n",
    "    Args::\n",
    "        count: number of iterations to fetch images, count<0\n",
    "        is infinite\n",
    "    \"\"\"\n",
    "    while count != 0:\n",
    "        count -= 1\n",
    "        view_tuples = _view.fetch_tuples(max_tuples=100, timeout=2)\n",
    "        for soup_tuple in view_tuples:\n",
    "            status_widget.value = soup_tuple['title']\n",
    "            display_image(soup_tuple, image_region=image_widget, title_region=title_widget, url_region=url_widget)\n",
    "\n",
    "server_soup(count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### List the active Streams applications\n",
    "aid.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section wrap up.¶\n",
    "In this section composed and deployed a Streams application that processes live Wikipedia events on a server. It \n",
    "extended the previous application to extract images assocated with the event. In the case that the event\n",
    "does have an associated image, it pushed out to a view where it was rendered. \n",
    "\n",
    "\n",
    "In the next section we will continue the build out, using the extraced image we'll  apply AI image processing to extract out faces an score them. \n",
    "\n",
    "[Back to Directory](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='PHASE4'>Image Analysis - (ObjectAnalysis & FaceAnalysis)</a>\n",
    "\n",
    "![graph of application](images/imageAnalysis.png)\n",
    "\n",
    "We've located urls of images that have been sb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the shreded link image\n",
    "![graph of application](images/webImageFetch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import cvsupport\n",
    "\n",
    "def ImageWebFetch():\n",
    "    \"\"\"Fetch image using the url, place encode image in Stream.\n",
    "    The encoded image is ascii so it can pass through json.\n",
    "    \n",
    "    - fetch the image into 'image_string'\n",
    "    \n",
    "    \"\"\"\n",
    "    topo = Topology(\"ImageWebFetch\")\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "    soup_active = topo.subscribe(topic=\"soup_active\", name=\"subSoupActive\")\n",
    "    active_image = soup_active.map(cvsupport.ImageFetch(), name=\"image_fetch\")\n",
    "    active_image.view(name=\"image_fetch\", description=\"encoded binary image\")\n",
    "    active_image.publish(topic=\"image_active\", name=\"pubImageActive\")\n",
    "    return topo\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, ImageWebFetch(), credential) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'FaceAnalysis' application\n",
    "![graph of application](images/faceAnalysis.png)\n",
    "- Two analysis operators thatdo analysis: findFace , findObject.\n",
    "- Two applications feeding via pub/sub : VideoRcvKafka, ImageWebFetch\n",
    "- This takes a longgg time to subumit...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## compose the findImageElements application\n",
    "import cvsupport\n",
    "\n",
    "def FaceAnalysis():\n",
    "    \"\"\"Analyse the images with various tools. \n",
    "       - Subscribe to 'image_active' with encoded images in 'image_string'\n",
    "       - process 'image_string' with FaceRegions into 'face_regions' \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"FaceAnalysis\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    #topo.add_file_dependency('../datasets/yolov3.weights', 'etc')\n",
    "    #topo.add_file_dependency('../datasets/yolov3.cfg', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')   \n",
    "    \n",
    "    image_active = topo.subscribe(topic=\"image_active\", name=\"subImageActive\")\n",
    "    \n",
    "    # Find faces analysis ....\n",
    "    face_regions = image_active.map(cvsupport.FaceRegions(), name=\"face_regions\")\n",
    "    face_trimmed = face_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'face_regions':t['face_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"faces_trimmed\")\n",
    "    face_trimmed.view(name=\"faces_view\", description=\"faces regions\")\n",
    "    return topo\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, FaceAnalysis(), credential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'ObjectAnalysis' application\n",
    "![graph of application](images/objectAnalysis.png)\n",
    "- Analysis operator that does analysis: findObject.\n",
    "- Two applications feeding via pub/sub : VideoRcvKafka, ImageWebFetch\n",
    "- This takes a longgg time to subumit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def ObjectAnalysis():\n",
    "    \"\"\"Analyse the images with various tools. \n",
    "       - Subscribe to 'image_active' with encoded images in 'image_string'\n",
    "       - process 'image_string' with ObjectRegions into 'object_regions'\n",
    "       - \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"ObjectAnalysis\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.weights', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.cfg', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')    \n",
    "    image_active = topo.subscribe(topic=\"image_active\", name=\"subImageActive\")\n",
    "    \n",
    "    # Find objects analysis ...\n",
    "    object_regions = image_active.map(cvsupport.ObjectRegions(classes=\"../datasets/yolov3.txt\"), name=\"object_fetch\")\n",
    "    object_trimmed = object_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'object_regions':t['object_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"objects_trimmed\")\n",
    "    object_trimmed.view(name=\"objects_view\", description=\"object regions\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "## Compose and submit the findElements Application \n",
    "\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, ObjectAnalysis(), credential) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Directory](#DIRECTORY)\n",
    "## <a id='PHASE4_RENDER'>Render Image Analysis - </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  View the ImageAnalysis application's FacesRegions results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# SHOW Found Faces\n",
    "face_dash = render.faceDashboard(instance, sleep=2)\n",
    "display(face_dash.dashboard)\n",
    "face_dash.ignition(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Fetch FacesRegions start/get/stop\n",
    "faces_view = instance.get_views(name=\"faces_view\")[0]\n",
    "faces_view.start_data_fetch()\n",
    "faces_tuples = faces_view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "print(\"Number of faces_tuples:\", len(faces_tuples))\n",
    "faces_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the ImageAnalysis application's ObjectRegions results\n",
    "Display the images and the objects they found. Run in a thread to make is easier to stop.\n",
    "\n",
    "On the Streams side it's processing the images to find objects, if not no objects are \n",
    "found they are not pushed to the view and will not be rendered here.\n",
    "\n",
    "The object detection maybe CPU intensive (you don't say) you may want to distribute this across hardware, which\n",
    "is left as an exercise to the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "object_dash = render.objectDashboard(instance, sleep=2.0)\n",
    "display(object_dash.dashboard)\n",
    "object_dash.ignition(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Directroy](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='PHASE5'>Recieve Video Frames via Kafka - VideoRcvKafka</a>\n",
    "### Fetch the shreded link image\n",
    "![graph of application](images/videoRcvKafka.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import streamsx.eventstreams as eventstreams\n",
    "from streamsx.topology.schema import CommonSchema\n",
    "\n",
    "def VideoRcvKafka():\n",
    "    \"\"\"Recieve video frame on topic and publish to 'image_string'. \n",
    "    \n",
    "    Notes:\n",
    "        - The script VideoSndKafka.py pushed video frame onto the topic.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    topo = Topology(\"VideoRcvKafka\")\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "\n",
    "    video_chunk = eventstreams.subscribe(topo, schema=CommonSchema.Json, \n",
    "                                         credentials=json.loads(credential.magsEventStream),\n",
    "                                         topic='VideoFrame' )\n",
    "    kafka_frame = video_chunk.map(cvsupport.BuildVideoFrame(), name=\"kafka_frame\")\n",
    "    kafka_frame.view(name=\"frame_kafka\", description=\"frame from kafka image\")\n",
    "    kafka_frame.publish(topic=\"image_active\", name=\"pubImageActive\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, VideoRcvKafka(), credential) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Directory](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='PHASE6'>Image Processing only - ImageProcessingOnly</a>\n",
    "\n",
    "\n",
    "![graph of application](images/imageOnlyApplications.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "object_dash = render.objectDashboard(instance, sleep=0.1)\n",
    "display(object_dash.dashboard)\n",
    "object_dash.ignition(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aid.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
