{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiRecentPhase4 \n",
    "[WikiRecentPhase3](./imgAna_3.ipynb) illustrated processing Wikipedia events continuously with Streams using [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to find and extract images releated to the event. Building on previous notebooks, this notebook extracts faces from the submitted image and analyze's them with machine learning facilities. \n",
    "\n",
    "## Overview - Image Analysis utilizing deep learning\n",
    "\n",
    "The previous notebook extracted images from the Wikipedia events. This continues the processing using two deep learning models provided by IBM's  [Model Asset Exchange](https://developer.ibm.com/exchanges/models/). \n",
    "- [Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/) to locate images faces within an image. \n",
    "- [Facial Emotion Classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/) to classify the emotions of face(s) located within an image. \n",
    "\n",
    "The facial recognizer locates the images, Streams extracts them and forwards onto for classificaion, the results are eventually presented on a view where this notebook renders \n",
    "the orinal image, recognized faces and classification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# Setup\n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICPD setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud setup\n",
    "\n",
    "To use Streams instance running in the cloud setup a [credential.py](setup_credential.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Show me\n",
    "After doing the 'Setup' above you can use Menu 'Cell' | 'Run All' to compose, build, submit and start the rendering of the live Wikidata, go to [Show me now](#showMeNow) for the rendering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install components\n",
    "!pip install sseclient\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "import pandas as pd\n",
    "import pandas\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import io\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from  functools import lru_cache\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import copy\n",
    "import base64\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    \"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt\n",
    "\n",
    "#\n",
    "# Support for locating/rendering views.\n",
    "def display_view_stop(eventView, period=2):\n",
    "    \"\"\"Wrapper for streamsx.rest_primitives.View.display() to have button. \"\"\"\n",
    "    button =  widgets.Button(description=\"Stop Updating\")\n",
    "    display(button)\n",
    "    eventView.display(period=period) \n",
    "    def on_button_clicked(b):\n",
    "        eventView.stop_data_fetch()\n",
    "        b.description = \"Stopped\"\n",
    "    button.on_click(on_button_clicked)\n",
    "\n",
    "def view_events(views):\n",
    "    \"\"\"\n",
    "    Build interface to display a list of views and \n",
    "    display view when selected from list.\n",
    "     \n",
    "    \"\"\"\n",
    "    view_names = [view.name for view in views]\n",
    "    nameView = dict(zip(view_names, views))    \n",
    "    select = widgets.RadioButtons(\n",
    "        options = view_names,\n",
    "        value = None,\n",
    "        description = 'Select view to display',\n",
    "        disabled = False\n",
    "    )\n",
    "    def on_change(b):\n",
    "        if (b['name'] == 'label'):\n",
    "            clear_output(wait=True)\n",
    "            [view.stop_data_fetch() for view in views ]\n",
    "            display(select)\n",
    "            display_view_stop(nameView[b['new']], period=2)\n",
    "    select.observe(on_change)\n",
    "    display(select)\n",
    "\n",
    "def find_job(instance, job_name=None):\n",
    "    \"\"\"locate job within instance\"\"\"\n",
    "    for job in instance.get_jobs():    \n",
    "        if job.applicationName.split(\"::\")[-1] == job_name:\n",
    "            return job\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def display_views(instance, job_name):\n",
    "    \"Locate/promote and display all views of a job\"\n",
    "    job = find_job(instance, job_name=job_name)\n",
    "    if job is None:\n",
    "        print(\"Failed to locate job\")\n",
    "    else:\n",
    "        views = job.get_views()\n",
    "        view_events(views)\n",
    "\n",
    "def list_jobs(_instance=None, cancel=False):\n",
    "    \"\"\"\n",
    "    Interactive selection of jobs to cancel.\n",
    "    \n",
    "    Prompts with SelectMultiple widget, if thier are no jobs, your presente with a blank list.\n",
    "    \n",
    "    \"\"\"\n",
    "    active_jobs = { \"{}:{}\".format(job.name, job.health):job for job in _instance.get_jobs()}\n",
    "\n",
    "    selectMultiple_jobs = widgets.SelectMultiple(\n",
    "        options=active_jobs.keys(),\n",
    "        value=[],\n",
    "        rows=len(active_jobs),\n",
    "        description = \"Cancel jobs(s)\" if cancel else \"Active job(s):\",\n",
    "        layout=Layout(width='60%')\n",
    "    )\n",
    "    cancel_jobs = widgets.ToggleButton(\n",
    "        value=False,\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Delete selected jobs',\n",
    "        icon=\"stop\"\n",
    "    )\n",
    "    def on_value_change(change):\n",
    "        for job in selectMultiple_jobs.value:\n",
    "            print(\"canceling job:\", job, active_jobs[job].cancel())\n",
    "        cancel_jobs.disabled = True\n",
    "        selectMultiple_jobs.disabled = True\n",
    "\n",
    "    cancel_jobs.observe(on_value_change, names='value')\n",
    "    if cancel:\n",
    "        return HBox([selectMultiple_jobs, cancel_jobs])\n",
    "    else:\n",
    "        return HBox([selectMultiple_jobs])\n",
    "    \n",
    "def render_image(image_url=None, output_region=None):\n",
    "    \"\"\"Write the image into a output region.\n",
    "    \n",
    "    Args::\n",
    "        url: image\n",
    "        output_region: output region\n",
    "        \n",
    "    .. note:: The creation of the output 'stage', if this is not done the image is rendered in the page and\n",
    "        the output region. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        stage = widgets.Output(layout={'border': '1px solid green'})\n",
    "    except:\n",
    "        print(\"Error on request : \", image_url)\n",
    "    else:\n",
    "        if response.status_code == 200:\n",
    "            with output_region:\n",
    "                stage.append_display_data(widgets.Image(\n",
    "                    value=response.content,\n",
    "                    #format='jpg',\n",
    "                    width=300,\n",
    "                    height=400,\n",
    "                ))\n",
    "            output_region.clear_output(wait=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the server : ICP4D or Cloud instance.Â¶\n",
    "\n",
    "Attempt to import if fails the cfg will not be defined we know were using Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside ICP4D\n"
     ]
    }
   ],
   "source": [
    "def get_instance():\n",
    "    \"\"\"Setup to access your Streams instance.\n",
    "\n",
    "    ..note::The notebook is work within Cloud and ICP4D. \n",
    "            Refer to the 'Setup' cells above.              \n",
    "    Returns:\n",
    "        instance : Access to Streams instance, used for submitting and rendering views.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from icpd_core import icpd_util\n",
    "        import urllib3\n",
    "        global cfg\n",
    "        cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        instance = rest.Instance.of_service(cfg)\n",
    "        print(\"Within ICP4D\")\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    except ImportError:\n",
    "        cfg = None\n",
    "        print(\"Outside ICP4D\")\n",
    "        import credential  \n",
    "        sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', \n",
    "                                               vcap_services=credential.vcap_conf)\n",
    "        instance = sc.get_instances()[0]\n",
    "    return instance,cfg\n",
    "\n",
    "instance,cfg = get_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List jobs and cancel....\n",
    "\n",
    "This page will submit a job named 'WikiPhase4'. If it's running you'll want to cancel it before submitting a new version. If it is running, no need to cancel/submit you can just procede to the [Viewing data section](#viewingData).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de37b49cbe5b44fca9fd75f809d1cf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(SelectMultiple(description='Cancel jobs(s)', layout=Layout(width='60%'), options=('ipythoninput113a4b30d5de96::WikiPhase3_10:healthy',), rows=1, value=()), ToggleButton(value=False, button_style='warning', description='Cancel', icon='stop', tooltip='Delete selected jobs')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='accessFoundation'></a>\n",
    "## Support functions that are executed within Streams\n",
    "\n",
    "Details of these functions can be found in previous notesbooks of this suite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"fetch recent changes from wikievents site using SSE\"\"\"\n",
    "    for change in EventSource('https://stream.wikimedia.org/v2/stream/recentchange'):\n",
    "        if len(change.data):\n",
    "            try:\n",
    "                obj = json.loads(change.data)\n",
    "            except json.JSONDecodeError as err:\n",
    "                print(\"JSON l1 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            except json.decoder.JSONDecodeError as err:\n",
    "                print(\"JSON l2 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            else:\n",
    "                yield(obj)\n",
    "\n",
    "\n",
    "class sum_aggregation():\n",
    "    def __init__(self, sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }):\n",
    "        \"\"\"\n",
    "        Summation of column(s) over a window's tuples. \n",
    "        Args::\n",
    "            sum_map :  specfify tuple columns to be summed and the result field. \n",
    "            tuples : at run time, list of tuples will flow in. Sum each fields\n",
    "        \"\"\"\n",
    "        self.sum_map = sum_map\n",
    "    def __call__(self, tuples)->dict: \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tuples : list of tuples constituting a window, over all the tuples sum using the sum_map key/value \n",
    "                     to specify the input and result field.\n",
    "        Returns:\n",
    "            dictionary of fields summations over tuples\n",
    "            \n",
    "        \"\"\"\n",
    "        summaries = dict()\n",
    "        for summary_field,result_field in self.sum_map.items():\n",
    "            summation = sum([ele[summary_field] for ele in tuples])\n",
    "            summaries.update({result_field : summation})\n",
    "        return(summaries)\n",
    "\n",
    "import collections\n",
    "class tally_fields(object):\n",
    "    def __init__(self, top_count=3, fields=['user', 'wiki', 'title']):\n",
    "        \"\"\"\n",
    "        Tally fields of a list of tuples.\n",
    "        Args::\n",
    "            fields :  fields of tuples that are to be tallied\n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.top_count = top_count\n",
    "    def __call__(self, tuples)->dict:\n",
    "        \"\"\"\n",
    "        Args::\n",
    "            tuples : list of tuples tallying to perform. \n",
    "        return::\n",
    "            dict of tallies\n",
    "        \"\"\"\n",
    "        tallies = dict()\n",
    "        for field in self.fields:\n",
    "            stage = [tuple[field] for tuple in tuples if tuple[field] is not None]\n",
    "            tallies[field] = collections.Counter(stage).most_common(self.top_count)\n",
    "        return tallies\n",
    "\n",
    "import csv\n",
    "class wiki_lang():\n",
    "    \"\"\"\n",
    "    Augment the tuple to include language wiki event.\n",
    "    \n",
    "    Mapping is loaded at build time and utilized at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fname=\"wikimap.csv\"):\n",
    "        self.wiki_map = dict()\n",
    "        with open(fname, mode='r') as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            for row in csv_reader:\n",
    "                self.wiki_map[row['dbname']] = row\n",
    "\n",
    "    def __call__(self, tuple):\n",
    "        \"\"\"using 'wiki' field to look pages code, langauge and native\n",
    "        Args:\n",
    "            tuple: tuple (dict) with a 'wiki' fields\n",
    "        Returns:'\n",
    "            input tuple with  'code', 'language, 'native' fields added to the input tuple.\n",
    "        \"\"\"\n",
    "        if tuple['wiki'] in self.wiki_map:\n",
    "            key = tuple['wiki']\n",
    "            tuple['code'] = self.wiki_map[key]['code']\n",
    "            tuple['language'] = self.wiki_map[key]['in_english']\n",
    "            tuple['native'] = self.wiki_map[key]['name_language']\n",
    "        else:\n",
    "            tuple['code'] = tuple['language'] = tuple['native'] = None\n",
    "        return tuple\n",
    "\n",
    "#@lru_cache(maxsize=None)\n",
    "def shred_item_image(url):\n",
    "    \"\"\"Shred the item page, seeking image. \n",
    "    \n",
    "    Discover if referencing image by shredding referening url. If it is, dig deeper \n",
    "    and extract the 'src' link. \n",
    "    \n",
    "    Locate the image within the page, locate <a class='image' src=**url** ,..>\n",
    "    \n",
    "    This traverses two files, pulls the thumbnail ref and follows to fullsize.\n",
    "    \n",
    "    Args:\n",
    "        url: item page to analyse\n",
    "    \n",
    "    Returns: \n",
    "        If image found [{name,title,org_url},...]\n",
    "    \n",
    "    .. warning:: this fetches from wikipedia, requesting too frequenty is bad manners. Uses the lru_cache()\n",
    "    so it minimises the requests.\n",
    "    \n",
    "    This can pick up multiple titles, on a page that is extract, dropping to only one. \n",
    "    \"\"\"\n",
    "    img_urls = list()\n",
    "    try:\n",
    "        rThumb = requests.get(url = url)\n",
    "        #print(r.content)\n",
    "        soupThumb = BeautifulSoup(rThumb.content, \"html.parser\")\n",
    "        divThumb = soupThumb.find(\"div\", class_=\"thumb\")\n",
    "        if divThumb is None:\n",
    "            print(\"No thumb found\", url  )\n",
    "            return img_urls\n",
    "        thumbA = divThumb.find(\"a\", class_=\"image\")\n",
    "        thumbHref = thumbA.attrs['href']\n",
    "\n",
    "        rFullImage = requests.get(url=thumbHref)\n",
    "        soupFull = BeautifulSoup(rFullImage.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(\"Error request.get, url: {} except:{}\".format(url, str(e)))\n",
    "    else:\n",
    "        divFull = soupFull.find(\"div\", class_=\"fullImageLink\", id=\"file\")\n",
    "        if (divFull is not None):\n",
    "            fullA = divFull.find(\"a\")\n",
    "            img_urls.append({\"title\":soupThumb.title.getText(),\"img\": fullA.attrs['href'],\"org_url\":url})\n",
    "    finally:\n",
    "        return img_urls\n",
    "\n",
    "#@lru_cache(maxsize=None)\n",
    "def shred_jpg_image(url):\n",
    "    \"\"\"Shed the jpg page, seeking image, the reference begins with 'Fred:' and \n",
    "    ends with '.jpg'.\n",
    "    \n",
    "    Discover if referencing image by shredding referening url. If it is, dig deeper \n",
    "    and extract the 'src' link. \n",
    "    \n",
    "    Locate the image within the page, \n",
    "            locate : <div class='fullImageLinks'..>\n",
    "                         <a href=\"..url to image\" ...>.</a>\n",
    "                         :\n",
    "                     </div>  \n",
    "    Args:\n",
    "        url: item page to analyse\n",
    "    \n",
    "    Returns: \n",
    "        If image found [{name,title,org_url='requesting url'},...]\n",
    "    \n",
    "    .. warning:: this fetches from wikipedia, requesting too frequenty is bad manners. Uses the lru_cache()\n",
    "    so it minimises the requests.\n",
    "    \n",
    "    \"\"\"\n",
    "    img_urls = list()\n",
    "    try:\n",
    "        r = requests.get(url = url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(\"Error request.get, url: {} except:{}\".format(url, str(e)))\n",
    "    else:\n",
    "        div = soup.find(\"div\", class_=\"fullImageLink\")\n",
    "        if (div is not None):\n",
    "            imgA = div.find(\"a\")\n",
    "            img_urls.append({\"title\":soup.title.getText(),\"img\":\"https:\" + imgA.attrs['href'],\"org_url\":url})\n",
    "        else: \n",
    "            print(\"failed to find div for\",url)\n",
    "    finally:\n",
    "        return img_urls\n",
    "\n",
    "class soup_image_extract():\n",
    "    \"\"\"If the the field_name has a potential a image we\n",
    "    \n",
    "    Return: \n",
    "        None : field did not have potenital for an image.\n",
    "        [] : had potential but no url found. \n",
    "        [{title,img,href}]\n",
    "    \"\"\"\n",
    "    def __init__(self, field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"):\n",
    "        self.url_base = url_base\n",
    "        self.field_name = field_name\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "        title = _tuple[self.field_name]\n",
    "        img_desc = None \n",
    "        if (title[0] == \"Q\"):\n",
    "            lnk = self.url_base + title\n",
    "            img_desc = shred_item_image(lnk)\n",
    "        elif title.startswith(\"File:\") and (title.endswith('.JPG') or title.endswith('.jpg')):\n",
    "            lnk = self.url_base + title.replace(' ','_')\n",
    "            img_desc = shred_jpg_image(lnk)\n",
    "        _tuple['img_desc'] = img_desc\n",
    "        return _tuple\n",
    "\n",
    "class soup_image():\n",
    "    \"\"\"If the the field_name has a potential for a image we\n",
    "    \n",
    "    Return: \n",
    "        None : field did not have potenital for an image.\n",
    "        [] : had potential but no url found. \n",
    "        [{title,img,href}]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"):\n",
    "        self.url_base = url_base\n",
    "        self.field_name = field_name\n",
    "        self.cache_item = None\n",
    "        self.cache_jpg = None\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "        if self.cache_item is None:\n",
    "            self.cache_item = cache_url_process(shred_item_image)\n",
    "            self.cache_jpg = cache_url_process(shred_jpg_image)\n",
    "        title = _tuple[self.field_name]\n",
    "        img_desc = None \n",
    "        if (title[0] == \"Q\"):\n",
    "            lnk = self.url_base + title\n",
    "            img_desc = self.cache_item.cache_process(lnk)\n",
    "            print(\"cache_item\", self.cache_item.stats())\n",
    "\n",
    "        elif title.startswith(\"File:\") and (title.endswith('.JPG') or title.endswith('.jpg')):\n",
    "            lnk = self.url_base + title.replace(' ','_')\n",
    "            img_desc = self.cache_jpg.cache_process(lnk)\n",
    "            print(\"cache_jpg\", self.cache_jpg.stats())\n",
    "\n",
    "            #print(\"cache_jpg\", self.cache_jpg.stats())\n",
    "            \n",
    "        _tuple['img_desc'] = img_desc\n",
    "        return _tuple\n",
    "\n",
    "## Support of streams processing\n",
    "class cache_url_process():\n",
    "    def __init__(self, process_url, cache_max=200):\n",
    "        \"\"\"I would use @lru_cache() but I ran into two problems. \n",
    "           - when I got to the server it could not find the function. \n",
    "           - get a stack overflow when building the topology. \n",
    "        Args::\n",
    "            process_url: a function that process's the request, when not cached.\n",
    "            Function will accept a URL and retrn  dict.\n",
    "        Return::\n",
    "            result from process_url that may be a cached value.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.urls = OrderedDict()\n",
    "        self.hits = 0\n",
    "        self.attempts = 0\n",
    "        self.process = process_url\n",
    "        self.cache_max = cache_max\n",
    "    def cache_process(self, url):\n",
    "        self.attempts += 1\n",
    "        if url in self.urls:\n",
    "            self.hits += 1\n",
    "            stage = self.urls[url]\n",
    "            del self.urls[url]  # move to begining of que\n",
    "            self.urls[url] = stage\n",
    "            n = len(self.urls) - self.cache_max \n",
    "            [self.urls.popitem(last=False) for idx in range(n if n > 0 else 0)]\n",
    "            return stage\n",
    "        stage = self.process(url)\n",
    "        self.urls[url] = stage\n",
    "        return stage\n",
    "    def stats(self):\n",
    "        return dict({\"attempts\":self.attempts,\"hits\":self.hits,\"len\":len(self.urls)})\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='facialFunctionality'></a>\n",
    "## Facial Image Extraction + Emotion Analysis\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "Using [IBM Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpBufIoOut = None\n",
    "\n",
    "def facial_fetch(imgurl):\n",
    "    \n",
    "    \"\"\"Using the facial recognizer get the location of all the faces on the image. \n",
    "    \n",
    "    Args:\n",
    "        imgurl : image the recognizer is done on.\n",
    "    Return:\n",
    "        location of found faces \n",
    "    ..note: \n",
    "        - In light of the fact that were using a free service, it can stop working at anytime.\n",
    "        - Pulls the binary image from wikipedia forwards the binary onto the service.\n",
    "    \"\"\"\n",
    "    predict_url='http://max-facial-recognizer.max.us-south.containers.appdomain.cloud/model/predict'\n",
    "    parsed = urlparse(imgurl)   \n",
    "    filename = parsed.path.split('/')[-1]\n",
    "    if (filename.lower().endswith('.svg')):\n",
    "        print(\"Cannot process svg:\", imgurl)\n",
    "        return list(), None\n",
    "    if (filename.lower().endswith('.tif')):\n",
    "        print(\"Cannot process tif:\", imgurl)\n",
    "        return list(), None\n",
    "    try:\n",
    "        page = requests.get(imgurl)\n",
    "    except Exception as e:\n",
    "        print(\"Image fetch exception:\", e)\n",
    "        return None, None\n",
    "    bufIoOut = io.BytesIO(page.content)\n",
    "    files = {'image': (filename, bufIoOut, \"image/jpeg\")}\n",
    "    try: \n",
    "        r = requests.post(predict_url, files=files)\n",
    "    except Exception as e:\n",
    "        print(\"Analysis service exception\", e)\n",
    "        return None, None\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Analysis failure:\",r.status_code, r.json())\n",
    "        return None, None\n",
    "    analysis = r.json()\n",
    "    return analysis, bufIoOut\n",
    "\n",
    "def facial_locate(imgurl):\n",
    "    analysis,bufIoOut = facial_fetch(imgurl)\n",
    "    if bufIoOut is None:\n",
    "        return None\n",
    "    if (analysis['predictions']) == 0:\n",
    "        print(\"No predictions found for\", imgurl)\n",
    "        return None\n",
    "    return({'bin_image':bufIoOut, 'faces':analysis})\n",
    "\n",
    "def crop_percent(img_dim, box_extent):\n",
    "    \"\"\"get the % of image the cropped image is\"\"\"\n",
    "    img_size = img_dim[0] * img_dim[1]\n",
    "    box_size = abs((int(box_extent[0]) - int(box_extent[2])) * (int(box_extent[1])- int(box_extent[3])))\n",
    "    percent = ((box_size/img_size) * 100)\n",
    "    return(percent)\n",
    "\n",
    "\n",
    "def image_cropper(bin_image, faces):\n",
    "    \"\"\"Crop out the faces from a URL.\n",
    "    Args:\n",
    "        url : image images\n",
    "        faces : list of {region,predictions} that that should be cropped\n",
    "    Return:\n",
    "        dict with 'annotated_image' and 'crops'\n",
    "        'crops' is list of dicts with \n",
    "            {image:face image, \n",
    "             probability:chances it's a face, \n",
    "             image_percent:found reqion % of of the image, \n",
    "             detection_box:region of the original image that the image was extacted from}\n",
    "         'crops' empty - nothing found, no faces found\n",
    "    \"\"\"\n",
    "    crops = list()\n",
    "    for face in faces['predictions']: \n",
    "        i = Image.open(bin_image)\n",
    "        percent = crop_percent( i.size, face['detection_box'])\n",
    "        img = i.crop(face['detection_box'])\n",
    "        crops.append({'image':img, 'probability':face['probability'],'detection_box':face['detection_box'],'image_percent':percent})\n",
    "    return crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class facial_image():\n",
    "    \"\"\"Extract all the faces from an image, for each found face generate a tuple with a face field. \n",
    "    Args:\n",
    "        - field_name : name of field on input tuple with the image description dict\n",
    "        - img_field : dictionary entry that has url of image\n",
    "        \n",
    "\n",
    "    Return: \n",
    "        None - No 'img_desc' field or no faces found \n",
    "        List of tuples composed of the input tuple with a new 'face' field\n",
    "        face a dictionary: \n",
    "        - probability : probability that it's a face\n",
    "        - percentage : % of the field_img that the detection_box occupies\n",
    "        - detection_box : within the orginal image, coodinates of extracted image\n",
    "        - bytes_PIL_b64 : cropped image in binary Base64 ascii\n",
    "    ..notes: \n",
    "        1. the next operator in line should be the flat_map() that takes the list of tuples and converts \n",
    "        to a stream of tuples.\n",
    "    ..code::\n",
    "        '''\n",
    "        ## Example of displaying encoded cropped image.\n",
    "        from PIL import Image\n",
    "        from io import BytesIO\n",
    "        import copy\n",
    "        \n",
    "        calidUrlImage = \"URL of valid Image to be analysized\"\n",
    "        minimal_tuple = {'img_desc':[{'img':validUrlImage}]}\n",
    "        fi = facial_image()\n",
    "        crops = fi.__call__(minimal_tuple)\n",
    "        for crop in crops:\n",
    "            cropImg = Image.open(io.BytesIO(base64.b64decode(crop['face']['bytes_PIL_b64'])))\n",
    "            print(\"Image Size\",cropImg.size)\n",
    "            display(cropImg)\n",
    "        '''\n",
    "    \"\"\"\n",
    "    def __init__(self, field_name=\"img_desc\", url_base=\"https://www.wikidata.org/wiki/\", image_field='img'):\n",
    "        self.url_base = url_base\n",
    "        self.img_desc = field_name\n",
    "        self.img_field = image_field\n",
    "        self.cache_item = None\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "\n",
    "        if self.img_desc not in _tuple or len(_tuple[self.img_desc]) == 0:\n",
    "            return None\n",
    "        desc = _tuple[self.img_desc][0]\n",
    "        if self.img_field not in desc:\n",
    "            print(\"Missing 'img' field in 'img_desc'\")\n",
    "            return None\n",
    "        processed = facial_locate(desc[self.img_field])\n",
    "        if processed is None:\n",
    "            return None\n",
    "\n",
    "        crops = image_cropper(processed['bin_image'], processed['faces'])\n",
    "        tuples = list()\n",
    "        for crop in crops:\n",
    "            augmented_tuple = copy.copy(_tuple)\n",
    "            with io.BytesIO() as output:\n",
    "                crop['image'].save(output, format=\"JPEG\")\n",
    "                contents = output.getvalue() \n",
    "            crop['bytes_PIL_b64'] = base64.b64encode(contents).decode('ascii')\n",
    "            del crop['image']\n",
    "            augmented_tuple['face'] = crop\n",
    "            tuples.append(augmented_tuple)\n",
    "        return tuples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codeVerify debug Simulate a test tuple: facial_image\n",
    "```python\n",
    "validUrlImage = \"https://upload.wikimedia.org/wikipedia/commons/5/52/Bundesarchiv_B_145_Bild-F023358-0012%2C_Empfang_in_der_Landesvertretung_Bayern%2C_Hallstein.jpg\"\n",
    "minimal_tuple = {'img_desc':[{'img':validUrlImage}]}\n",
    "fi = facial_image()\n",
    "crops = fi.__call__(minimal_tuple)\n",
    "for crop in crops:\n",
    "    cropImg = Image.open(io.BytesIO(base64.b64decode(crop['face']['bytes_PIL_b64'])))\n",
    "    #cropImg = Image.open(io.BytesIO(res[0]['face']['bytes_PIL']))\n",
    "    print(\"Image Size\",cropImg.size)\n",
    "    display(cropImg)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysisFunctionality'></a>\n",
    "## Facial Analysis Functionality\n",
    "[Jump Table](#jumpTable)\n",
    "\n",
    "Using the  [IBM emotion classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/) to analyise images that are being added to wikiepedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_crop(bufIoOut, imgurl):\n",
    "    \"\"\" Our friends: \"https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/\"\n",
    "    \n",
    "    Analyse an image using the service \"http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/\".\n",
    "    \n",
    "    Send binary image to analysis \n",
    "    \n",
    "    The processing nodes not necessarily return a prediction, could be an indication that it's not a predictable image.\n",
    "    \n",
    "    Args:\n",
    "        imgurl: the original source image that the cropped region came from\n",
    "        bufIoOut : the binary cropped image to be analysized\n",
    "\n",
    "    Returns:\n",
    "        None - error encountered\n",
    "        [] : executed, no prediction.\n",
    "        [{anger,contempt,disgust,happiness,neutral,sadness,surpise}]   \n",
    "    ..note: \n",
    "        This utilizing a function put up the by our friends, $$ == 0.\n",
    "        It can stop working at anytime. \n",
    "    \"\"\"\n",
    "    predict_url='http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/model/predict'\n",
    "    parsed = urlparse(imgurl)   \n",
    "    filename = parsed.path.split('/')[-1]\n",
    "\n",
    "    files = {'image': (filename, bufIoOut, \"image/jpeg\")}\n",
    "    try: \n",
    "        r = requests.post(predict_url, files=files)\n",
    "    except Exception as e:\n",
    "        print(\"Analysis service exception\", e)\n",
    "        return None\n",
    "    if (r.status_code != 200):\n",
    "        print(\"Analysis failure:\",r.status_code, r.json())\n",
    "        return None\n",
    "    analysis = r.json()\n",
    "    if len(analysis['predictions']) == 0:\n",
    "        return list()\n",
    "    emotions = analysis['predictions'][0]['emotion_predictions']\n",
    "    return [{emot['label']:float(\"{0:.2f}\".format(emot['probability'])) for emot in emotions}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emotion_image():\n",
    "    \"\"\"If there is an img entry, attempt to analyize\n",
    "    Args:\n",
    "        field_name : name of field on input tuple with the image description dict\n",
    "        img_field: dictionary entry that has url of image\n",
    "    \n",
    "    Return: \n",
    "        None - No 'img_desc' field or no entries in the field\n",
    "        Add a emotion to the tuple. \n",
    "            Empty [] if nothing in img_desc or no emotion could be derived\n",
    "            None : field did not have potenital for an image.\n",
    "        [] : had potential but no url found. \n",
    "        [{title,img,href}]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, _tuple):\n",
    "        bufIoOut_decode_image = io.BytesIO(base64.b64decode(_tuple['face']['bytes_PIL_b64']))\n",
    "        url = _tuple['img_desc'][0]['img']\n",
    "        emotion = emotion_crop(bufIoOut_decode_image, url)\n",
    "        _tuple['emotion'] = emotion\n",
    "        return(_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codeVerify  / Simuate a test tuple: facial_image + emotion operators. \n",
    "```python\n",
    "validUrlImage = \"https://upload.wikimedia.org/wikipedia/commons/5/52/Bundesarchiv_B_145_Bild-F023358-0012%2C_Empfang_in_der_Landesvertretung_Bayern%2C_Hallstein.jpg\"\n",
    "minimal_tuple = {'img_desc':[{'img':validUrlImage}]}\n",
    "fi = facial_image()\n",
    "crops = fi.__call__(minimal_tuple)\n",
    "ei = emotion_image()\n",
    "for crop in crops:\n",
    "    cropImg = Image.open(io.BytesIO(base64.b64decode(crop['face']['bytes_PIL_b64'])))\n",
    "    #cropImg = Image.open(io.BytesIO(res[0]['face']['bytes_PIL']))\n",
    "    print(\"Image Size\",cropImg.size)\n",
    "    display(cropImg)\n",
    "    print(ei.__call__(crop)['emotion'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='composeSubmit'></a>\n",
    "## Compose, build and submit the Streams application.\n",
    "The following Code cell composed the Streams application depicted here:\n",
    "![stillPhase4.jpg](images/stillPhase4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook is an extention of the previous, I'll only discuss processing beyond 'langAugment' for details regarding prior processing refer to previous [notebook](./imgAna_3.ipynb)s.\n",
    "\n",
    "The events output by the map named 'soupActive' have an associated image determined by using the [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) libary. At 'facialImgs' the \n",
    "'img_desc is used by facial_image() to extract a list of faces from the image using the [Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/). \n",
    "\n",
    "The list of faces is decomposed into tuples at 'faceImg' by flat_map(), resulting in an \n",
    "tuple for every face found. The tuple includes: a binary version of the cropped face,\n",
    "original image url, location within the image of the face, probaility that it is a face, \n",
    "the percentage of the image the face occupies.\n",
    "\n",
    "The image binary version of the cropped face is processed by 'faceEmotion' using the \n",
    "[Facial Emotion Classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/) service. If the cropped face is deemed worthy of analysis an emotion\n",
    "score is is added to the tuple where it can be inspected as 'faceEmotion' view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a6a69bab15433ab278131cc562d09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(SelectMultiple(description='Cancel jobs(s)', layout=Layout(width='60%'), options=('ipythoninput113a4b30d5de96::WikiPhase3_10:healthy',), rows=1, value=()), ToggleButton(value=False, button_style='warning', description='Cancel', icon='stop', tooltip='Delete selected jobs')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO If WikiPhase4 is running, cancel before submitting.\n",
    "\n",
    "\n",
    "def WikiPhase4(jobName=None, wiki_lang_fname=None):\n",
    "    \"\"\"\n",
    "    Compose topology. \n",
    "    -- wiki_lang : csv file mapping database name to langauge\n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    ### make sure we sseclient in Streams environment.\n",
    "    topo.add_pip_package('sseclient')\n",
    "    topo.add_pip_package('bs4')\n",
    "\n",
    "    ## wiki events\n",
    "    wiki_events = topo.source(get_events, name=\"wikiEvents\")\n",
    "    ## select events generated by humans\n",
    "    human_filter = wiki_events.filter(lambda x: x['type']=='edit' and x['bot'] is False, name='humanFilter')\n",
    "    # pare down the humans set of columns\n",
    "    pared_human= human_filter.map(lambda x : {'timestamp':x['timestamp'],\n",
    "                                              'new_len':x['length']['new'],\n",
    "                                              'old_len':x['length']['old'], \n",
    "                                              'delta_len':x['length']['new'] - x['length']['old'],\n",
    "                                              'wiki':x['wiki'],'user':x['user'],\n",
    "                                              'title':x['title']}, \n",
    "                        name=\"paredHuman\")\n",
    "    pared_human.view(buffer_time=1.0, sample_size=200, name=\"paredEdits\", description=\"Edits done by humans\")\n",
    "\n",
    "    ## Define window(count)& aggregate\n",
    "    sum_win = pared_human.last(100).trigger(20)\n",
    "    sum_aggregate = sum_win.aggregate(sum_aggregation(sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }), name=\"sumAggregate\")\n",
    "    sum_aggregate.view(buffer_time=1.0, sample_size=200, name=\"aggEdits\", description=\"Aggregations of human edits\")\n",
    "\n",
    "    ## Define window(count) & tally edits\n",
    "    tally_win = pared_human.last(100).trigger(10)\n",
    "    tally_top = tally_win.aggregate(tally_fields(fields=['user', 'title'], top_count=10), name=\"talliesTop\")\n",
    "    tally_top.view(buffer_time=1.0, sample_size=200, name=\"talliesCount\", description=\"Top count tallies: user,titles\")\n",
    "\n",
    "    ## augment filterd/pared edits with language\n",
    "    if cfg is None:        \n",
    "        lang_augment = pared_human.map(wiki_lang(fname='../datasets/wikimap.csv'), name=\"langAugment\")\n",
    "    else:\n",
    "        lang_augment = pared_human.map(wiki_lang(fname=os.environ['DSX_PROJECT_DIR']+'/datasets/wikimap.csv'), name=\"langAugment\")\n",
    "\n",
    "    lang_augment.view(buffer_time=1.0, sample_size=200, name=\"langAugment\", description=\"Language derived from wiki\")\n",
    "\n",
    "    ## Define window(time) & tally language\n",
    "    time_lang_win = lang_augment.last(datetime.timedelta(minutes=2)).trigger(5)\n",
    "    time_lang = time_lang_win.aggregate(tally_fields(fields=['language'], top_count=10), name=\"timeLang\")\n",
    "    time_lang.view(buffer_time=1.0, sample_size=200, name=\"talliesTime\", description=\"Top timed tallies: language\")\n",
    "\n",
    "    ## attempt to extract image using beautifulsoup add img_desc[{}] field\n",
    "    soup_image = lang_augment.map(soup_image_extract(field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"),name=\"imgSoup\")\n",
    "    soup_active = soup_image.filter(lambda x: x['img_desc'] is not None and len(x['img_desc']) > 0, name=\"soupActive\")\n",
    "    soup_active.view(buffer_time=1.0, sample_size=200, name=\"soupActive\", description=\"Image extracted via Bsoup\")\n",
    "    \n",
    "    ## facial extraction  - \n",
    "    facial_images = soup_active.map(facial_image(field_name='img_desc'),name=\"facialImgs\")\n",
    "    face_image = facial_images.flat_map(name=\"faceImg\")\n",
    "    face_image.view(buffer_time=10.0, sample_size=20, name=\"faceImg\", description=\"Face image analysis/extraction\")\n",
    "\n",
    "    ## emotion anaylsis on image - \n",
    "    face_emotion = face_image.map(emotion_image(), name=\"faceEmotion\")\n",
    "    face_emotion.view(buffer_time=10.0, sample_size=20, name=\"faceEmotion\", description=\"Factial emotion analysis\")\n",
    "       \n",
    "    return ({\"topo\":topo,\"view\":{ }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting job : ICP or Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe665fef0064ff79a30ce106e412731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId:  11 Name:  ipythoninput12ed1fcb35c85d::WikiPhase4_11\n"
     ]
    }
   ],
   "source": [
    "resp = WikiPhase4(jobName=\"WikiPhase4\")\n",
    "if cfg is not None:\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    submission_result = context.submit(\"DISTRIBUTED\",resp['topo'], config=cfg)\n",
    "\n",
    "if cfg is None:\n",
    "    import credential\n",
    "    cloud = {\n",
    "        context.ConfigParams.VCAP_SERVICES: credential.vcap_conf,\n",
    "        context.ConfigParams.SERVICE_NAME: \"Streaming3Turbine\",\n",
    "        context.ContextTypes.STREAMING_ANALYTICS_SERVICE:\"STREAMING_ANALYTIC\",\n",
    "        context.ConfigParams.FORCE_REMOTE_BUILD: True,\n",
    "    }\n",
    "    submission_result = context.submit(\"STREAMING_ANALYTICS_SERVICE\",resp['topo'],config=cloud)\n",
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    print(\"JobId: \", submission_result['id'] , \"Name: \", submission_result['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viewingData'></a>\n",
    "## Viewing data \n",
    "\n",
    "The running application has number of views to see what what data is moving through the stream. The following \n",
    "cell will fetch the views' queue and display it's data when selected. \n",
    "\n",
    "| view name | description of data is the view | bot |\n",
    "|---------|-------------|--------------|\n",
    "|aggEdits  | summarised fields | False |\n",
    "|langAugment | mapped augmented fields | False |\n",
    "|paredEdits | seleted fields | False |\n",
    "|talliesCount | last 100 messages tallied | False | \n",
    "|talliesTimes | 2 minute windowed | False |\n",
    "|soupActive | extracted images links| False |\n",
    "|faceImg | analyse image for faces and extract | False |\n",
    "|faceEmotion | emotional analysis of facial images | False | \n",
    "\n",
    "\n",
    "\n",
    "You want to stop the the fetching the view data when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acf5059016a488a96c3464b05145129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>RadioButtons</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "RadioButtons(description='Select view to display', options=('aggEdits', 'faceEmotion', 'faceImg', 'langAugment', 'paredEdits', 'soupActive', 'talliesCount', 'talliesTime'), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the data that is flowing.....\n",
    "display_views(instance, \"WikiPhase4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='jumpTable'></a>\n",
    "### Jump Table..\n",
    "- **[Running / Active](#runningActive)**\n",
    "- [Access Foundation](#accessFoundation)@server\n",
    "- [Compose Submit](#composeSubmit)@server\n",
    "- [Language Distribution](#languageDistribution)\n",
    "- [Soup Functionality](#soupFunctionality)@server\n",
    "- [Image Extraction](#imageExtraction)  \n",
    "- [Facial Image Extraction](#facialFunctionality)@server\n",
    "- [Image Facial Location](#imageFacialAnalysis)\n",
    "- [Analysis Functionaltiy](#analysisFunctionality)@server\n",
    "- [Image Emotion Analysis](#imageEmotionAnalysis) : \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![phase4_1.gif](attachment:phase5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imageFacialAnalysis'></a>\n",
    "## Image Facial Location with [MAX](https://developer.ibm.com/exchanges/models/)\n",
    "\n",
    "Using [IBM Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/)\n",
    "\n",
    "[Jump Table](#jumpTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Views / Render Views UI\n",
    "\n",
    "From the server this is getting the cropped images. Streams is passing the image through the \n",
    "IBM Facial Recognizer that extracts the coordinates of potential faces. A new tuple is generated\n",
    "for each potential face consisting of the \n",
    "- input tuple, this include a url image being analyzed\n",
    "- face dict() consisting of ...\n",
    "- - probability : probabilty that it's an face\n",
    "- - image_percentage : % of image original image the found face occupies\n",
    "- - bytes_PIL_b64 : binary image version of found image\n",
    "- - detection_box : region within the original image the face was detected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,  ImageDraw  # https://pillow.readthedocs.io/en/4.3.x/\n",
    "import requests  # http://docs.python-requests.org/en/master/\n",
    "def line_box(ele):\n",
    "    \"\"\"build a box with lines.\"\"\"\n",
    "    return (ele[0],ele[1],ele[0],ele[3],ele[2],ele[3],ele[2],ele[1],ele[0],ele[1])\n",
    "\n",
    "def resize_image(bin_image, basewidth=None, baseheight=None):\n",
    "    \"\"\"Resize image proportional to the base, make it fit in cell\"\"\"\n",
    "    if basewidth is not None:\n",
    "        wpercent = (basewidth/float(bin_image.size[0]))\n",
    "        hsize = int((float(bin_image.size[1])*float(wpercent)))\n",
    "        return bin_image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    wpercent = (baseheight/float(bin_image.size[1]))\n",
    "    wsize = int((float(bin_image.size[0])*float(wpercent)))\n",
    "    return bin_image.resize((wsize,baseheight), Image.ANTIALIAS)\n",
    "\n",
    "# example image url: https://m.media-amazon.com/images/S/aplus-media/vc/6a9569ab-cb8e-46d9-8aea-a7022e58c74a.jpg\n",
    "def face_crop(bin_image, detection_box, percent, probability):\n",
    "    \"\"\"Crop out the faces from a URL using detection_box and send to analysis.\n",
    "    Args:\n",
    "        url : image images\n",
    "        faces : list of {region,predictions} that that should be cropped\n",
    "    Return:\n",
    "        dict with 'annotated_image' and 'crops'\n",
    "        'crops' is list of dicts with \n",
    "            {image:face image, \n",
    "             probability:chances it's a face, \n",
    "             image_percent:found reqion % of of the image, \n",
    "             detection_box:region of the original image that the image was extacted from}\n",
    "         'crops' empty - nothing found, no faces found\n",
    "    \"\"\"\n",
    "    crops = list()\n",
    "    draw = ImageDraw.Draw(bin_image) \n",
    "    box_width = 5 if percent > .01 else 20\n",
    "    box_fill = \"orange\" if probability > .90 else \"red\"\n",
    "    draw.line(line_box(detection_box), fill=box_fill, width=box_width)\n",
    "    #draw.rectangle(detection_box, fill=128)\n",
    "    return {'annotated_image':bin_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codeVerify debug Test the scale box\n",
    "\n",
    "```python\n",
    "graph_widget = widgets.Output(layout={'border': '1px solid blue','width':'200pt','height':'200pt'})\n",
    "graphboard = VBox([ graph_widget])\n",
    "display(graphboard)\n",
    "scale(graph_widget)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_index = ['surprise', 'happiness', 'contempt', 'neutral', 'sadness', 'anger', 'disgust','fear']\n",
    "colors =      ['hotpink', 'gold', 'lightcoral', 'beige', 'brown', 'red', 'green', 'purple']\n",
    "\n",
    "def scale(region):\n",
    "    \"\"\"Display the scale used on the scoring.\n",
    "    \n",
    "    Args:\n",
    "        region to write the scale into\n",
    "\n",
    "    ..note: this invoked when the emotion classifier does not return any results. Put\n",
    "    up the scale to understand the score. \n",
    "    \n",
    "    \"\"\"\n",
    "    with region:\n",
    "        fz = 150\n",
    "        fd = -1.30\n",
    "        plt.text(0.0,  1.0, \n",
    "                \"{:^35s}\".format(\"Emotion Anlysis Inconclusive\"), size=fz,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            bbox=dict(boxstyle=\"square\",\n",
    "            fc=\"white\",\n",
    "            fill=True)\n",
    "         )\n",
    "        \n",
    "        plt.rcParams['font.family'] = 'monospace'\n",
    "        for idx in range(len(colors)):\n",
    "            plt.text(0.0,  (fd * idx) + -2, \n",
    "                     \"{:^35s}\".format(order_index[idx]), size=fz,\n",
    "                ha=\"left\", va=\"top\",\n",
    "                bbox=dict(boxstyle=\"square\",\n",
    "                   fc=colors[idx],\n",
    "                   fill=True\n",
    "                )\n",
    "         )\n",
    "    \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_idx = 0\n",
    "img_dict = dict()\n",
    "\n",
    "def bar_cell(percentage, probability, emotion, crop_img):\n",
    "    \"\"\"In cells below main photo the results of the two\n",
    "    deep learning models are displayed by this function. \n",
    "    \"\"\"\n",
    "    global bar_idx\n",
    "    with crops_bar[bar_idx % bar_cells]['image']:\n",
    "        display(resize_image(crop_img,basewidth=100))\n",
    "        clear_output(wait=True)\n",
    "    if len(emotion) > 0:\n",
    "        print(emotion)\n",
    "        with crops_bar[bar_idx % bar_cells]['pie']:\n",
    "            fig1, ax1 = plt.subplots()\n",
    "            emot = [emotion[0][key] for key in order_index]\n",
    "            #df = pandas.DataFrame(emotion[0], index=order_index)\n",
    "            ax1.pie(emot ,\n",
    "                shadow=True, startangle=90, colors=colors)\n",
    "            ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "    else:\n",
    "        scale(crops_bar[bar_idx % bar_cells]['pie'])\n",
    "    crops_bar[bar_idx % bar_cells]['probability'].value = \"conf : {0:.2f}%\".format(probability)\n",
    "    crops_bar[bar_idx % bar_cells]['image_percent'].value = \"img {0:.2f}%\".format(percentage)\n",
    "    bar_idx += 1\n",
    "    \n",
    "def encode_img(img):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        contents = output.getvalue() \n",
    "    return base64.b64encode(contents).decode('ascii')\n",
    "\n",
    "def decode_img(bin64):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_emotions(emotion_tuples):\n",
    "    \"\"\"Using view data display the emotion results. \n",
    "    \n",
    "    ..note: We have cropped face image, the location and the url of the original image. \n",
    "    Display original image with the outline of the image location, I lay multiple onlines\n",
    "    on the image by holding them in a map, this also reduces the number of times I \n",
    "    pull from wikipedia.\n",
    "    \"\"\"\n",
    "    for emotion in emotion_tuples:\n",
    "        img_url = emotion['img_desc'][0]['img']\n",
    "        percent = emotion['face']['image_percent']\n",
    "        probability = emotion['face']['probability']\n",
    "    \n",
    "        if (img_url in img_dict):\n",
    "            print(\"cache\", img_url)\n",
    "            bimg = decode_img(img_dict[img_url])\n",
    "            face_crops = face_crop(bimg,emotion['face']['detection_box'], percent, probability)\n",
    "            img_dict[img_url] = encode_img(face_crops['annotated_image'])\n",
    "            with full_widget:\n",
    "                fullImg = face_crops['annotated_image']\n",
    "                dspImg = resize_image(fullImg, baseheight=400)\n",
    "                display(dspImg)\n",
    "                clear_output(wait=True)\n",
    "        else:\n",
    "            print(\"web\", img_url)\n",
    "            r = requests.get(img_url, timeout=4.0)\n",
    "            if r.status_code != requests.codes.ok:\n",
    "                assert False, 'Status code error: {}.'.format(r.status_code)\n",
    "            with Image.open(io.BytesIO(r.content)) as bin_image:\n",
    "                bimg = bin_image\n",
    "                #display(bimg)\n",
    "                face_crops = face_crop(bimg,emotion['face']['detection_box'], percent, probability)\n",
    "                img_dict[img_url] = encode_img(face_crops['annotated_image'])\n",
    "                with full_widget:\n",
    "                    fullImg = face_crops['annotated_image']\n",
    "                    dspImg = resize_image(fullImg, baseheight=400)\n",
    "                    display(dspImg)\n",
    "                    clear_output(wait=True)\n",
    "        binImg = emotion['face']['bytes_PIL_b64']\n",
    "        bar_cell(percent, \n",
    "                probability,\n",
    "                emotion['emotion'],\n",
    "                Image.open(io.BytesIO(base64.b64decode(binImg))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show me now\n",
    "<a id='showMeNow'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9251fca9514dcd884ded73609176e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border='1px solid red', height='300pt', width='100%')), HBox(children=(VBox(children=(Label(value='prop:0', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:1', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:2', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:3', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:4', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:5', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:6', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt'))))))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Setup the 'Dashboard' - Display the images sent to Wikipedia, result of facial extraction followed by emotion (pie chart) analysis\n",
    "##                         Next cell populates the 'Dashboard'.....\n",
    "crops_bar = list()  # setup in layout section.\n",
    "bar_cells = 7\n",
    "## Layout the dashboard cells \n",
    "url_widget = widgets.Label(value=\"Img URL\", layout={'border': '1px solid red','width':'100%'})\n",
    "full_widget = widgets.Output(layout={'border': '1px solid red','width':'100%','height':'300pt'})\n",
    "title_widget = widgets.Label(value=\"Title\", layout={'border': '1px solid red','width':'30%'})\n",
    "\n",
    "vbox_bar = list()\n",
    "for idx in range(bar_cells):\n",
    "    vbox = {\n",
    "        'probability' : widgets.Label(value=\"prop:{}\".format(idx), layout={'border': '1px solid blue','width':'100pt'}),\n",
    "        'image_percent' : widgets.Label(value=\"image %\", layout={'border': '1px solid blue','width':'100pt'}),\n",
    "        'image' : widgets.Output(layout={'border': '1px solid blue','width':'100pt','height':'120pt'}),\n",
    "        'pie' : widgets.Output(layout={'border': '1px solid black','width':'100pt','height':'100pt'})\n",
    "    }\n",
    "    crops_bar.append(vbox)\n",
    "    vbox_bar.append(widgets.VBox([vbox['probability'], vbox['image_percent'], vbox['image'], vbox['pie']]))\n",
    "    \n",
    "display(widgets.VBox([full_widget,widgets.HBox(vbox_bar)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Populate the dashboard - If you want this to run longer set cnt higher\n",
    "cnt = 40\n",
    "_view = instance.get_views(name=\"faceEmotion\")[0]\n",
    "_view.start_data_fetch()\n",
    "for idx in range(10):\n",
    "    emotion_tuples = _view.fetch_tuples(max_tuples=10, timeout=20)\n",
    "    print(\"Count of tuples\", len(emotion_tuples))\n",
    "    render_emotions(emotion_tuples)\n",
    "_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel jobs when your done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4850bd238ef24adc91633847f3a57413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(SelectMultiple(description='Cancel jobs(s)', layout=Layout(width='60%'), options=(), rows=0, value=()), ToggleButton(value=False, button_style='warning', description='Cancel', icon='stop', tooltip='Delete selected jobs')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook wrap upÂ¶\n",
    "\n",
    "In this notebook we composed and deployed a Streams application that processes live Wikipedia events on a server. It \n",
    "extended the previous application to using the extracted image that we applied \n",
    "deep learning image processing models to derive insight into the images submitted.\n",
    "\n",
    "## Extentions\n",
    "The processing on the stream can continue on , gaining more insights into the\n",
    "events occuring on the Wikipedia servers.\n",
    "\n",
    "Possible explorations\n",
    "- Continue the face analysis stream by applying the [Facial Age Estimator](https://developer.ibm.com/exchanges/models/all/max-facial-age-estimator/)\n",
    "- For for smaller cropped facial apply the [Image Resolution Enahance](https://developer.ibm.com/exchanges/models/all/max-image-resolution-enhancer/) before proceding to the emotion analysis.\n",
    "- Use the image [Image Caption Generator](https://developer.ibm.com/exchanges/models/all/max-image-caption-generator/) to generate captions for the images\n",
    "- Use the result of the 'Image Caption Generator' to verify captions provided by the submitted, check the translation of captions submitted outside english speaking countries. \n",
    "- Check the sentiment of submitted updated.\n",
    "- On images that faces do not appear use the [Object Detector](https://developer.ibm.com/exchanges/models/all/m[beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) beatuful soup to extract the text being sumbitted and apply \n",
    "[Sentiment Classifier](https://developer.ibm.com/exchanges/models/all/max-text-sentiment-classifier/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
