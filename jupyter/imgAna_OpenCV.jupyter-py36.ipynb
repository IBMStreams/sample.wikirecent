{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgAna_OpenCV \n",
    "\n",
    "In the past a service was invoked to locate faces, this is a replacement for that outboard processing locating\n",
    "the faces using an OpenCV components.\n",
    "This works, it is still in process.\n",
    "\n",
    "## What is going on here\n",
    "\n",
    "The first part of the file is getting access to OpenCV, verify that it works in notebook and possibly in Streams. \n",
    "This work is done working with face detection.  A good part of this work is dealing with renderng the images \n",
    "and adding boxes to regions, if your going todo something with images you gotta show them. \n",
    "\n",
    "\n",
    "After we work out that it can be used I work out how to fetch images from the web, using the url in the\n",
    "tuple. The image must be converted into somthing that JSON will not be upset with. \n",
    "\n",
    "Two types of analysis are being done on the images. \n",
    "- FaceRegion() : finds faces, this what I used to learn on, thus all the work a the begining of the notebook.\n",
    "- ObjectRegions() : finds objects, much better example since it give the location of the object and an key to what the object is. This I got from Jerome. \n",
    "\n",
    "Two applications in here are build here. \n",
    "- imageFetch : pulls images in from the web and encodes them\n",
    "- fineImageElements : invokes FaceRegions() to find faces & ObjectRegions()\n",
    "\n",
    "The 'imageFetch' application gets it's data from [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb), \n",
    "make sure you read about this below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-contrib-python\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note : \n",
    "\n",
    "Getting this build locally and push up to the cloud you need the following...\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "pip install pandas\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "pip install beautifulsoup4\n",
    "pip install Pillow\n",
    "```\n",
    "Here is where I got a jump start on [face-detection-python-opencv](https://www.datacamp.com/community/tutorials/face-detection-python-opencv). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from  functools import lru_cache\n",
    "import requests\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image,  ImageDraw  # https://pillow.readthedocs.io/en/4.3.x/\n",
    "import io\n",
    "import base64\n",
    "import sys\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "import cvsupport\n",
    "import streams_operations\n",
    "import streams_render\n",
    "import streamsx.eventstreams as eventstreams\n",
    "from streamsx.topology.schema import CommonSchema\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "## has credentials for cloud & kafka/event stream\n",
    "import credential\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"imgAna_OpenCV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HACK - his should note be necessary - something is wrong with my environment, \n",
    "##\n",
    "import os\n",
    "try:\n",
    "    print(\"JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"])\n",
    "except KeyError as err:\n",
    "    os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home\"\n",
    "    print(\"Had to set JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"],\"\\n   --- what is going on here...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    \"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a local file\n",
    "Using OpenCv so some  processing on a local file.\n",
    "* do some image manipulation\n",
    "* find the face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "img_raw = cv2.imread('../datasets/baby1.png')\n",
    "print(type(img_raw))\n",
    "print(img_raw.shape)\n",
    "plt.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def convertToRGB(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(convertToRGB(img_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def convertToBW(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(convertToBW(img_raw), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "haar_cascade_face = cv2.CascadeClassifier('../datasets/haarcascade_frontalface_default.xml')\n",
    "haar_cascade_face\n",
    "\n",
    "faces_rects = haar_cascade_face.detectMultiScale(convertToBW(img_raw), scaleFactor = 1.2, minNeighbors = 5);\n",
    "\n",
    "# Let us print the no. of faces found\n",
    "print(\"Faces found: {} faces within regions:{}\".format(len(faces_rects), faces_rects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize \n",
    "* Find faces\n",
    "* Draw a box around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_faces(image, cascade, scaleFactor = 1.1):\n",
    "    # create a copy of the image to prevent any changes to the original one.\n",
    "    image_copy = image.copy()\n",
    "\n",
    "    #convert the test image to gray scale as opencv face detector expects gray images\n",
    "    gray_image = convertToBW(image_copy)\n",
    "\n",
    "    # Applying the haar classifier to detect faces\n",
    "    image_rects = cascade.detectMultiScale(gray_image, scaleFactor=scaleFactor, minNeighbors=5)\n",
    "    return image_rects\n",
    "\n",
    "def rects_render(image, rects):\n",
    "    image_copy = image.copy()\n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(image_copy, (x, y), (x+w, y+h), (0, 255, 0), 15)\n",
    "    return image_copy\n",
    "    \n",
    "## Render the image with the faces_rects\n",
    "if len(faces_rects)> 0:\n",
    "    img_rects = rects_render(convertToRGB(img_raw), faces_rects)\n",
    "    #print(img_rects)\n",
    "    plt.imshow(img_rects)\n",
    "else:\n",
    "    print(\"On image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Another image....\n",
    "img_raw = cv2.imread('../datasets/imgClassify.jpg')\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "img_rects = rects_render(img_rgb, rects)\n",
    "plt.imshow(img_rects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch a image from the web\n",
    "\n",
    "\n",
    "Use the the URLs for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Get and image from the web. \n",
    "urls = [\"https://media.wired.com/photos/5c5354d391d0df22c1dee493/master/w_2560%2Cc_limit/Backchannel-Lena-Final.jpg\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/1/1e/Christopher_de_Paus.JPG\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/a/a5/Anne_Bethel_Spencer_in_her_wedding_dress.jpg\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Barbara_Anderson_1969.JPG\"]\n",
    "response = requests.get(urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV locate face region : face_rects\n",
    "- OpenCV to fetch process image, locate the faces.\n",
    "- OpenCV to render the results.\n",
    "\n",
    "This processing will be pushed to Streams.\n",
    "\n",
    "\n",
    "#### face_rects[] is a list of retanges that the face is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def bts_to_img(bts):\n",
    "    '''\n",
    "    :param bts: results from image_to_bts\n",
    "    '''\n",
    "    buff = np.fromstring(bts, np.uint8)\n",
    "    buff = buff.reshape(1, -1)\n",
    "    img = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "img_raw = bts_to_img(response.content)\n",
    "print(\"Size of image to process : \",img_raw.shape)\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "print(\"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects))\n",
    "img_rects = rects_render(img_rgb, face_rects)\n",
    "plt.imshow(img_rects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render with face_rects[] - reduce dependency on numpy\n",
    "Using image from web add the rects. \n",
    "\n",
    "- Render in a widget, in a browser. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def line_box(ele):\n",
    "    (x,y,w,h)=ele\n",
    "    return (x,y, x+w,y, x+w,y+h, x,y+h, x,y)\n",
    "    \n",
    "\n",
    "def inscribe_rect(bin_image, detection_box, box_line_width=10):\n",
    "    \"\"\"Inscribe box on image\n",
    "    \n",
    "    This is updating the image passed in.\n",
    "    \n",
    "    Args:\n",
    "        bin_image : binary image\n",
    "        detection_box : region to put box around\n",
    "    Return:\n",
    "        return image - \n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(bin_image) \n",
    "    draw.line(line_box(detection_box), fill=\"yellow\", width=box_line_width)\n",
    "    return bin_image\n",
    "   \n",
    "\n",
    "def encode_img(img):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        contents = output.getvalue() \n",
    "    return base64.b64encode(contents).decode('ascii')\n",
    "\n",
    "def decode_img(bin64):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "    return img\n",
    "\n",
    "def resize_image(bin_image, basewidth=None, baseheight=None):\n",
    "    \"\"\"Resize image proportional to the base, make it fit in cell\"\"\"\n",
    "    if basewidth is not None:\n",
    "        wpercent = (basewidth/float(bin_image.size[0]))\n",
    "        hsize = int((float(bin_image.size[1])*float(wpercent)))\n",
    "        return bin_image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    wpercent = (baseheight/float(bin_image.size[1]))\n",
    "    wsize = int((float(bin_image.size[0])*float(wpercent)))\n",
    "    return bin_image.resize((wsize,baseheight), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Widget to display image\n",
    "demo_widget = widgets.Output(layout={'border': '1px solid green'})\n",
    "display(demo_widget)\n",
    "with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "    for rect in face_rects:\n",
    "        inscribe_rect(bin_image, rect)\n",
    "    with demo_widget:\n",
    "        print(type(bin_image))\n",
    "        display(bin_image)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images from Wiki via Streams\n",
    "\n",
    "At this point...\n",
    "- we can fetch images local/web\n",
    "- render the image in the browser\n",
    "- locate faces using the OpenCV facility.\n",
    "- render the image inscribed with 'suspected' face regions. \n",
    "\n",
    "Before we push this processing up to Streams we'll process the types of images the Streams applicagtion will be dealing with locally. \n",
    "If you have not you need to bring up the the Streams application composed in the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb) notebook. \n",
    "This application's view (soupActive) includes the URL of the image being submitted to WikiPedia, the image that we \n",
    "are going preform face OpenCV face detection.\n",
    "\n",
    "We'll do the processing on the Streams, results will be send back. Below were going \n",
    "the processing in the browser to work out the processing to do. \n",
    "\n",
    "This assumes that you have started the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# Setup\n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICPD setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment to submit and view.\n",
    "- Use the data from WikiRecentPhase3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# If you using ICP4D insert your creds here...\n",
    "#from icpd_core import icpd_util\n",
    "#global cfg\n",
    "#cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')\n",
    "\n",
    "# Accessing Streams Instance & views\n",
    "\n",
    "instance,cfg = streams_operations.get_instance(service_name='Streaming3Turbine')\n",
    "\n",
    "\n",
    "## Fetch the view that has the image URLs.\n",
    "_view = instance.get_views(name=\"soupActive\")[0]\n",
    "_view.start_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def commonSubmit(_cfg, streams_cloud_service, _topo):\n",
    "    \"\"\"submit to either the cloud or CP4D\n",
    "    Args:\n",
    "        _cfg: when submitting from CP4D.\n",
    "        streams_cloud_service : when submitting to cloud, the name of the service credential.py must appropriate mapping\n",
    "        _topo : topology to submit\n",
    "    \n",
    "    \"\"\"\n",
    "    if _cfg is not None:\n",
    "        # Disable SSL certificate verification if necessary\n",
    "        _cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        submission_result = context.submit(\"DISTRIBUTED\",_topo, config=_cfg)\n",
    "    if _cfg is None:\n",
    "        cloud = {\n",
    "            context.ConfigParams.VCAP_SERVICES: credential.vcap_conf,\n",
    "            context.ConfigParams.SERVICE_NAME: streams_cloud_service,\n",
    "            context.ContextTypes.STREAMING_ANALYTICS_SERVICE:\"STREAMING_ANALYTIC\",\n",
    "            context.ConfigParams.FORCE_REMOTE_BUILD: True,\n",
    "        }\n",
    "        submission_result = context.submit(\"STREAMING_ANALYTICS_SERVICE\",_topo,config=cloud)\n",
    "\n",
    "    # The submission_result object contains information about the running application, or job\n",
    "    if submission_result.job:\n",
    "        report = \"JobId:{} Name:{} \".format(submission_result['id'], submission_result['name'])\n",
    "    else:\n",
    "        report = \"Somthing did work:{}\".format(submission_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def display_image(view_tuple, image_region=None, rect_region=None, title_region=None, status_region=None, url_region=None):\n",
    "    img_url = view_tuple['img_desc'][0]['img']\n",
    "    response = requests.get(img_url)\n",
    "    \n",
    "    img_raw = bts_to_img(response.content)\n",
    "    if img_raw is None:\n",
    "        print(\"img_url:{} bts_img() failed\".format(img_url))\n",
    "        return\n",
    "    img_rgb = convertToRGB(img_raw)\n",
    "    face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "    status_region.value = \"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects)\n",
    "    # img_rects = rects_render(img_rgb, face_rects)\n",
    "    url_region.value = img_url\n",
    "    title_region.value = view_tuple['title']\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        with image_region:\n",
    "            display(resize_image(bin_image, baseheight=300))\n",
    "            clear_output(wait=True)\n",
    "            for rect in face_rects:\n",
    "                inscribe_rect(bin_image, rect)\n",
    "            with rect_region:\n",
    "                if len(face_rects) is not 0:\n",
    "                    display(resize_image(bin_image, baseheight=300))\n",
    "                    clear_output(wait=True)\n",
    "                else:\n",
    "                    clear_output()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render in 'Dashboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Render in 'Dashboard' images that processed locally using OpenCV. Best to get things working \n",
    "\n",
    "## locally before pushing it up to Streams. \n",
    "## \n",
    "## Images are coming from a Wikipedia via a Streams view.\n",
    "##\n",
    "status_local = widgets.Label(value=\"Status\", layout={'border': '1px solid green','width':'600%'})\n",
    "url_local = widgets.Label(value=\"Img URL\", layout={'border': '1px solid green','width':'60%'})\n",
    "image_local = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'300pt'})\n",
    "rect_local = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'300pt'})\n",
    "hbox_local = widgets.HBox([image_local, rect_local])\n",
    "title_local = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'60%'})\n",
    "dashboard = widgets.VBox([status_local, hbox_local, title_local, url_local])\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# This cell is going to populate the dashboard above.\n",
    "## The left pane has the image that arrived, the right \n",
    "## show the faces demarked, if none found no image.\n",
    "## \n",
    "## - Re-execute the cell to fetch/process/render images.\n",
    "## - Note the debug statements below.\n",
    "\n",
    "@catchInterrupt\n",
    "def server_soup(count=1):\n",
    "    \"\"\"Fetch and display images from view.\n",
    "    Args::\n",
    "        count: number of iterations to fetch images, count<0\n",
    "        is infinite\n",
    "    \"\"\"\n",
    "    while count != 0:\n",
    "        count -= 1\n",
    "        view_tuples = _view.fetch_tuples(max_tuples=12, timeout=2)\n",
    "        print(\"remaining cycles:{} tuples:{}\".format(count, len(view_tuples)))\n",
    "        for soup_tuple in view_tuples:\n",
    "            display_image(soup_tuple, image_region=image_local, rect_region=rect_local, title_region=title_local, status_region=status_local, url_region=url_local)\n",
    "\n",
    "server_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   To Processing Operators. \n",
    "Above we created code to fetch a image from the web and find faces, which is composed a number of functions. \n",
    "To push it up to Streams we've rolled up the processing into two functions.\n",
    "\n",
    "The cvsupport.py in [scripts](../scripts) class that are used below to compose the application.\n",
    "- ImageFetch[] - web request that puts image in tuple. The most time consuming portion of the processing is fetching the images using the URL. Retrieve the image from and push the image data into the Stream where it will be processed by downstream operator.\n",
    "- FaceRegions[] - locates faces in image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the ImageFetch[] & FaceRegions[] before submitting ImageAnalysis application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://media.wired.com/photos/5c5354d391d0df22c1dee493/master/w_2560%2Cc_limit/Backchannel-Lena-Final.jpg\"\n",
    "#url = \"https://upload.wikimedia.org/wikipedia/commons/0/0a/Great_Western_Road%2C_53-100_Kelvin_Court%2C_Glasgow.jpg\"\n",
    "tuple1 = {'img_desc':[{'img':url}]}   # simulate the tuple to process\n",
    "image_fetch = cvsupport.ImageFetch()\n",
    "\n",
    "# simulate the Stream.\n",
    "tuple2 = image_fetch(tuple1)\n",
    "with cvsupport.FaceRegions(haar_file=\"../datasets/haarcascade_frontalface_default.xml\") as fr:\n",
    "    print(type(fr))\n",
    "    tuple3 = fr(tuple2)\n",
    "#face_region = cvsupport.FaceRegions(haar_file=\"../datasets/haarcascade_frontalface_default.xml\")\n",
    "#tuple3 = face_region(tuple2)\n",
    "\n",
    "# render the results.... create a\n",
    "if tuple3 is not None:\n",
    "    print(\" URL:{}\\n had {} regions \\n ... {}\".format(tuple3['img_desc'][0]['img'], len(tuple3['face_regions']), tuple3['face_regions']))\n",
    "    verify_widget = widgets.Output(layout={'border': '1px solid green'})\n",
    "    display(verify_widget)\n",
    "\n",
    "    response = requests.get(tuple3['img_desc'][0]['img'])  # fetch the image\n",
    "\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        for rect in tuple3['face_regions']:\n",
    "            inscribe_rect(bin_image, rect)\n",
    "        with verify_widget:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "else:\n",
    "    print(\"No faces located in image....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ObjectDetect locally before pushing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#url = \"https://upload.wikimedia.org/wikipedia/commons/0/0a/Great_Western_Road%2C_53-100_Kelvin_Court%2C_Glasgow.jpg\"\n",
    "urls = [\"https://upload.wikimedia.org/wikipedia/commons/1/1e/Christopher_de_Paus.JPG\",\n",
    "\"https://upload.wikimedia.org/wikipedia/commons/a/a5/Anne_Bethel_Spencer_in_her_wedding_dress.jpg\",\n",
    "\"https://upload.wikimedia.org/wikipedia/commons/f/f7/Barbara_Anderson_1969.JPG\"]\n",
    "tuple1 = {'img_desc':[{'img':urls[2]}]}   # simulate the tuple to process\n",
    "image_fetch = cvsupport.ImageFetch()\n",
    "\n",
    "# simulate the Stream.\n",
    "tuple2 = image_fetch(tuple1)\n",
    "\n",
    "\n",
    "with cvsupport.ObjectRegions(classes=\"../datasets/yolov3.txt\", \n",
    "                             weights=\"../datasets/yolov3.weights\", \n",
    "                             config=\"../datasets/yolov3.cfg\",\n",
    "                             on_streams=False) as fr:\n",
    "    print(type(fr))\n",
    "    tuple3 = fr(tuple2)\n",
    "#face_region = cvsupport.FaceRegions(haar_file=\"../datasets/haarcascade_frontalface_default.xml\")\n",
    "#tuple3 = face_region(tuple2)\n",
    "\n",
    "# render the results.... create a\n",
    "if tuple3 is not None:\n",
    "    print(\" URL:{}\\n had {} regions \\n ... {}\".format(tuple3['img_desc'][0]['img'], len(tuple3['object_regions']), tuple3['object_regions']))\n",
    "    verify_widget = widgets.Output(layout={'border': '1px solid green'})\n",
    "    display(verify_widget)\n",
    "\n",
    "    response = requests.get(tuple3['img_desc'][0]['img'])  # fetch the image\n",
    "\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        for obj in tuple3['object_regions']:\n",
    "            inscribe_rect(bin_image, obj[\"region\"])\n",
    "        with verify_widget:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "else:\n",
    "    print(\"No objects located in image....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(tuple3.keys())\n",
    "print(tuple3['object_regions'][0])\n",
    "type(tuple3['object_regions'][0]['confidence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose and Submit the applications.\n",
    "\n",
    "The Streams application that will get Tuples by subscribing a feed\n",
    "that is composed in the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what applications are currently running....\n",
    "Cancel those that you are about to submit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What views are available?\n",
    "At the start only WikiPhase3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.display_views(instance, \"WikiPhase3\")\n",
    "streams_render.display_views(instance, \"ImageWebFetch\")\n",
    "streams_render.display_views(instance, \"VideoRcvKafka\")\n",
    "streams_render.display_views(instance, \"ImageAnalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'ImageWebFetch' \n",
    "Fetching from Wiki feed, you will not get anything here unless you have executed the imgAna_3 notebook. \n",
    "Disabling the following cell, not deleting since this all should be moving toward \n",
    "interesting application. \n",
    "\n",
    "Input tuple has URL of the image, fetch the image and publish to 'image_active'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import credential\n",
    "def ImageWebFetch():\n",
    "    \"\"\"Fetch image using the url, place encode image in Stream.\n",
    "    The encoded image is ascii so it can pass through json.\n",
    "    \n",
    "    - fetch the image into 'image_string'\n",
    "    \n",
    "    \"\"\"\n",
    "    topo = Topology(\"ImageWebFetch\")\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "    soup_active = topo.subscribe(topic=\"soup_active\")\n",
    "    active_image = soup_active.map(cvsupport.ImageFetch(), name=\"image_fetch\")\n",
    "    active_image.view(name=\"image_fetch\", description=\"encoded binary image\")\n",
    "    active_image.publish(topic=\"image_active\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "commonSubmit(cfg, \"Streaming3Turbine\", ImageWebFetch())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the images fetched by 'ImageWebFetch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "streams_render.display_views(instance, job_name=\"ImageWebFetch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "image_view = instance.get_views(name=\"image_fetch\")[0]\n",
    "image_view.start_data_fetch()\n",
    "frame_tuples = image_view.fetch_tuples(max_tuples=30, timeout=30)\n",
    "print(\"Number of images:\", len(frame_tuples))\n",
    "image_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "## Display the frame_tuples just fetched\n",
    "image_frame = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "dashboard_frame = widgets.VBox([image_frame])\n",
    "display(dashboard_frame)\n",
    "\n",
    "for frame in frame_tuples:\n",
    "    image_string = frame['image_string']\n",
    "    with Image.open(io.BytesIO(base64.b64decode(image_string))) as bin_image:\n",
    "        with image_frame:\n",
    "            display(bin_image)\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'VideoRcvKafka' \n",
    "VideSndKafka.py opens video files and sends frames via EventStream/Kafka. The received frames\n",
    "are published to 'image_active'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def VideoRcvKafka():\n",
    "    \"\"\"Recieve video frame on topic and publish to 'image_string'. \n",
    "    \n",
    "    Notes:\n",
    "        - The script VideoSndKafka.py pushed video frame onto the topic.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    topo = Topology(\"VideoRcvKafka\")\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "\n",
    "    video_chunk = eventstreams.subscribe(topo, schema=CommonSchema.Json, \n",
    "                                         credentials=json.loads(credential.magsEventStream),\n",
    "                                         topic='VideoFrame' )\n",
    "    kafka_frame = video_chunk.map(cvsupport.BuildVideoFrame(), name=\"kafka_frame\")\n",
    "    kafka_frame.view(name=\"frame_kafka\", description=\"frame from kafka image\")\n",
    "    kafka_frame.publish(topic=\"image_active\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "commonSubmit(cfg, \"Streaming3Turbine\", VideoRcvKafka())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the frames recieved 'VideoRcvKafka'\n",
    "The video feed frames are sent by VideoSndKafka.py, via EventStream/Kafka to this application.\n",
    "The received frames are published.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.display_views(instance, job_name=\"VideoRcvKafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Fetch some frames from the view\n",
    "##   The VideoSndKafka script throttles the speed it send out frames.\n",
    "frame_view = instance.get_views(name=\"frame_kafka\")[0]\n",
    "frame_view.start_data_fetch()\n",
    "frame_tuples = frame_view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "print(\"Number of frame_tuples:\", len(frame_tuples))\n",
    "frame_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Display the frame_tuples just fetched\n",
    "video_frame = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "dashboard_frame = widgets.VBox([video_frame])\n",
    "display(dashboard_frame)\n",
    "\n",
    "for frame in frame_tuples:\n",
    "    image_string = frame['image_string']\n",
    "    with Image.open(io.BytesIO(base64.b64decode(image_string))) as bin_image:\n",
    "        with video_frame:\n",
    "            display(bin_image)\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'ImageAnalysis' application\n",
    "- Two analysis operators thatdo analysis: findFace , findObject.\n",
    "- Two applications feeding via pub/sub : VideoRcvKafka, ImageWebFetch\n",
    "- This takes a longgg time to subumit...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## compose the findImageElements application\n",
    "def ImageAnalysis():\n",
    "    \"\"\"Analyse the images with various tools. \n",
    "       - Subscribe to 'image_active' with encoded images in 'image_string'\n",
    "       - process 'image_string' with FaceRegions into 'face_regions' \n",
    "       - process 'image_string' with ObjectRegions into 'object_regions'\n",
    "       - \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"ImageAnalysis\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.weights', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.cfg', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "    \n",
    "    image_active = topo.subscribe(topic=\"image_active\")\n",
    "    # Find faces analysis ....\n",
    "    face_regions = image_active.map(cvsupport.FaceRegions(), name=\"face_regions\")\n",
    "    face_trimmed = face_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'face_regions':t['face_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"faces_trimmed\")\n",
    "    face_trimmed.view(name=\"faces_view\", description=\"faces regions\")\n",
    "    # Find objects analysis ...\n",
    "    object_regions = image_active.map(cvsupport.ObjectRegions(classes=\"../datasets/yolov3.txt\"), name=\"object_fetch\")\n",
    "    object_trimmed = object_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'object_regions':t['object_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"objects_trimmed\")\n",
    "    object_trimmed.view(name=\"objects_view\", description=\"object regions\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "## TODO turn into function - huge duplication \n",
    "## Compose and submit the findElements Application \n",
    "topo = ImageAnalysis()\n",
    "\n",
    "\n",
    "commonSubmit(cfg, \"Streaming3Turbine\", ImageAnalysis())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the ImageAnalysis processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the processes 'healthy' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the ImageAnalysis views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.display_views(instance, job_name=\"ImageAnalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the ImageAnalysis application's FacesRegions results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Fetch FacesRegions start/get/stop\n",
    "faces_view = instance.get_views(name=\"faces_view\")[0]\n",
    "faces_view.start_data_fetch()\n",
    "faces_tuples = faces_view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "print(\"Number of faces_tuples:\", len(faces_tuples))\n",
    "faces_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# render the results.... only images with regions will arrive via this view\n",
    "source_face = widgets.Label(value=\"Source\", layout={'border': '1px solid green','width':'50%'})\n",
    "rect_face = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "url_face = widgets.Label(value=\"URL\", layout={'border': '1px solid green','width':'50%'})\n",
    "dashboard_face = widgets.VBox([source_face, rect_face, url_face])\n",
    "display(dashboard_face)\n",
    "\n",
    "# Fetch the tuples from the view\n",
    "count = 0\n",
    "for face in faces_tuples:\n",
    "    url_face.value = face['url']\n",
    "    #source_face.value = trimmed['source']  # TODO - move accros the source.\n",
    "    source_face.value = str(count)\n",
    "    count += 1\n",
    "    image_string = face['image_string']\n",
    "    with Image.open(io.BytesIO(base64.b64decode(image_string))) as bin_image:\n",
    "        for rect in face['face_regions']:\n",
    "            inscribe_rect(bin_image, rect)\n",
    "        with rect_face:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "            clear_output(wait=True)\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the ImageAnalysis application's ObjectRegions results\n",
    "\n",
    " Collect some images from the view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Fetch ObjectRegions : start / get / stop\n",
    "objects_view = instance.get_views(name=\"objects_view\")[0]\n",
    "objects_view.start_data_fetch()\n",
    "objects_tuples = objects_view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "print(\"Number of object_tuples:\", len(objects_tuples))\n",
    "objects_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the images and the objects they found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# render the results.... only images with regions will arrive via this view\n",
    "source_object = widgets.Label(value=\"Source\", layout={'border': '1px solid green','width':'50%'})\n",
    "rect_object = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "url_object = widgets.Label(value=\"URL\", layout={'border': '1px solid green','width':'50%'})\n",
    "class_object = widgets.Label(value=\"CLASS\", layout={'border': '1px solid green','width':'50%'})\n",
    "dashboard_face = widgets.VBox([source_object, rect_object, url_object, class_object])\n",
    "display(dashboard_face)\n",
    "\n",
    "# Fetch the tuples from the view\n",
    "count = 0\n",
    "for _object in objects_tuples:\n",
    "    url_object.value = _object['url']\n",
    "    #source_face.value = trimmed['source']  # TODO - move accros the source.\n",
    "    source_object.value = str(count)\n",
    "    count += 1\n",
    "    image_string = _object['image_string']\n",
    "    class_string = \"\"\n",
    "    with Image.open(io.BytesIO(base64.b64decode(image_string))) as bin_image:\n",
    "        for rect in _object['object_regions']:\n",
    "            inscribe_rect(bin_image, rect['region'])\n",
    "            class_string = \",\".join([class_string,rect['class']])\n",
    "            class_object.value = class_string\n",
    "        with rect_object:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "            clear_output(wait=True)\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
