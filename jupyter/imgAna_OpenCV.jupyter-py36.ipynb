{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis imgAna_OpenCV \n",
    "\n",
    "In the past a service was invoked to locate faces, this is a replacement for that outboard processing locating\n",
    "the faces using an OpenCV components.\n",
    "\n",
    "The exploration process of this can is found imageAnaExplore_OpenCV, which maybe like going to a\n",
    "sausage factory. \n",
    "\n",
    "\n",
    "Two types of analysis are being done on the images. \n",
    "- FaceRegion() : finds faces, this what I used to learn on, thus all the work a the begining of the notebook.\n",
    "- ObjectRegions() : finds objects, much better example since it give the location of the object and an key to what the object is. This I got from Jerome. \n",
    "\n",
    "Two applications are composed here.... \n",
    "- imageFetch : pulls images in from the web and encodes them\n",
    "- fineImageElements : invokes FaceRegions() to find faces & ObjectRegions()\n",
    "\n",
    "The images are flowing in via EventStreams(Kafka) from the VideoSndKafka.py a stand alone program that shreds\n",
    "videos from the internet.archive.org pushes the across. Which means you need to have instantiated the EventStreams, I'm using the Cloud instance since I don't have the where-with-all to set it up. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-contrib-python\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note : \n",
    "\n",
    "Getting this build locally and push up to the cloud you need the following...\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "pip install pandas\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "pip install beautifulsoup4\n",
    "pip install Pillow\n",
    "```\n",
    "Here is where I got a jump start on [face-detection-python-opencv](https://www.datacamp.com/community/tutorials/face-detection-python-opencv). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from  functools import lru_cache\n",
    "import requests\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image,  ImageDraw  # https://pillow.readthedocs.io/en/4.3.x/\n",
    "import io\n",
    "import base64\n",
    "import sys\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "import cvsupport\n",
    "import streams_operations\n",
    "import streams_render\n",
    "import streamsx.eventstreams as eventstreams\n",
    "from streamsx.topology.schema import CommonSchema\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "## has credentials for cloud & kafka/event stream\n",
    "import credential\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"imgAna_OpenCV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HACK - his should note be necessary - something is wrong with my environment, \n",
    "##\n",
    "import os\n",
    "try:\n",
    "    print(\"JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"])\n",
    "except KeyError as err:\n",
    "    os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home\"\n",
    "    print(\"Had to set JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"],\"\\n   --- what is going on here...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    \"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render with face_rects[] - reduce dependency on numpy\n",
    "Using image from web add the rects. \n",
    "\n",
    "- Render in a widget, in a browser. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def line_box(ele):\n",
    "    (x,y,w,h)=ele\n",
    "    return (x,y, x+w,y, x+w,y+h, x,y+h, x,y)\n",
    "    \n",
    "\n",
    "def inscribe_rect(bin_image, detection_box, box_line_width=10):\n",
    "    \"\"\"Inscribe box on image\n",
    "    \n",
    "    This is updating the image passed in.\n",
    "    \n",
    "    Args:\n",
    "        bin_image : binary image\n",
    "        detection_box : region to put box around\n",
    "    Return:\n",
    "        return image - \n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(bin_image) \n",
    "    draw.line(line_box(detection_box), fill=\"yellow\", width=box_line_width)\n",
    "    return bin_image\n",
    "   \n",
    "\n",
    "def encode_img(img):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        contents = output.getvalue() \n",
    "    return base64.b64encode(contents).decode('ascii')\n",
    "\n",
    "def decode_img(bin64):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "    return img\n",
    "\n",
    "def resize_image(bin_image, basewidth=None, baseheight=None):\n",
    "    \"\"\"Resize image proportional to the base, make it fit in cell\"\"\"\n",
    "    if basewidth is not None:\n",
    "        wpercent = (basewidth/float(bin_image.size[0]))\n",
    "        hsize = int((float(bin_image.size[1])*float(wpercent)))\n",
    "        return bin_image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    wpercent = (baseheight/float(bin_image.size[1]))\n",
    "    wsize = int((float(bin_image.size[0])*float(wpercent)))\n",
    "    return bin_image.resize((wsize,baseheight), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# Setup\n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICPD setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you using ICP4D insert your creds here...\n",
    "#from icpd_core import icpd_util\n",
    "#global cfg\n",
    "#cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')\n",
    "\n",
    "# Accessing Streams Instance & views\n",
    "\n",
    "instance,cfg = streams_operations.get_instance(service_name='Streaming3Turbine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO move to commonStreams\n",
    "# TODO kill the process if it is running before submitted.\n",
    "def commonSubmit(_cfg, streams_cloud_service, _topo):\n",
    "    \"\"\"submit to either the cloud or CP4D\n",
    "    Args:\n",
    "        _cfg: when submitting from CP4D.\n",
    "        streams_cloud_service : when submitting to cloud, the name of the service credential.py must appropriate mapping\n",
    "        _topo : topology to submit\n",
    "    \n",
    "    \"\"\"\n",
    "    if _cfg is not None:\n",
    "        # Disable SSL certificate verification if necessary\n",
    "        _cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        submission_result = context.submit(\"DISTRIBUTED\",_topo, config=_cfg)\n",
    "    if _cfg is None:\n",
    "        cloud = {\n",
    "            context.ConfigParams.VCAP_SERVICES: credential.vcap_conf,\n",
    "            context.ConfigParams.SERVICE_NAME: streams_cloud_service,\n",
    "            context.ContextTypes.STREAMING_ANALYTICS_SERVICE:\"STREAMING_ANALYTIC\",\n",
    "            context.ConfigParams.FORCE_REMOTE_BUILD: True,\n",
    "        }\n",
    "        submission_result = context.submit(\"STREAMING_ANALYTICS_SERVICE\",_topo,config=cloud)\n",
    "\n",
    "    # The submission_result object contains information about the running application, or job\n",
    "    if submission_result.job:\n",
    "        report = \"JobId:{} Name:{} \".format(submission_result['id'], submission_result['name'])\n",
    "    else:\n",
    "        report = \"Somthing did work:{}\".format(submission_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Move toe commonDisplay.py\n",
    "\n",
    "def display_image(view_tuple, image_region=None, rect_region=None, title_region=None, status_region=None, url_region=None):\n",
    "    img_url = view_tuple['img_desc'][0]['img']\n",
    "    response = requests.get(img_url)\n",
    "    \n",
    "    img_raw = bts_to_img(response.content)\n",
    "    if img_raw is None:\n",
    "        print(\"img_url:{} bts_img() failed\".format(img_url))\n",
    "        return\n",
    "    img_rgb = convertToRGB(img_raw)\n",
    "    face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "    status_region.value = \"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects)\n",
    "    # img_rects = rects_render(img_rgb, face_rects)\n",
    "    url_region.value = img_url\n",
    "    title_region.value = view_tuple['title']\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        with image_region:\n",
    "            display(resize_image(bin_image, baseheight=300))\n",
    "            clear_output(wait=True)\n",
    "            for rect in face_rects:\n",
    "                inscribe_rect(bin_image, rect)\n",
    "            with rect_region:\n",
    "                if len(face_rects) is not 0:\n",
    "                    display(resize_image(bin_image, baseheight=300))\n",
    "                    clear_output(wait=True)\n",
    "                else:\n",
    "                    clear_output()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   To Processing Operators. \n",
    "Above we created code to fetch a image from the web and find faces, which is composed a number of functions. \n",
    "To push it up to Streams we've rolled up the processing into two functions.\n",
    "\n",
    "The cvsupport.py in [scripts](../scripts) class that are used below to compose the application.\n",
    "- ImageFetch[] - web request that puts image in tuple. The most time consuming portion of the processing is fetching the images using the URL. Retrieve the image from and push the image data into the Stream where it will be processed by downstream operator.\n",
    "- FaceRegions[] - locates faces in image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose and Submit the applications.\n",
    "\n",
    "The Streams application that will get Tuples by subscribing a feed\n",
    "that is composed in the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what applications are currently running....\n",
    "Cancel those that you are about to submit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What views are available?\n",
    "At the start only WikiPhase3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.display_views(instance, \"VideoRcvKafka\")\n",
    "streams_render.display_views(instance, \"ImageAnalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'VideoRcvKafka' \n",
    "VideSndKafka.py opens video files and sends frames via EventStream/Kafka. The received frames\n",
    "are published to 'image_active'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def VideoRcvKafka():\n",
    "    \"\"\"Recieve video frame on topic and publish to 'image_string'. \n",
    "    \n",
    "    Notes:\n",
    "        - The script VideoSndKafka.py pushed video frame onto the topic.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    topo = Topology(\"VideoRcvKafka\")\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "\n",
    "    video_chunk = eventstreams.subscribe(topo, schema=CommonSchema.Json, \n",
    "                                         credentials=json.loads(credential.magsEventStream),\n",
    "                                         topic='VideoFrame' )\n",
    "    kafka_frame = video_chunk.map(cvsupport.BuildVideoFrame(), name=\"kafka_frame\")\n",
    "    kafka_frame.view(name=\"frame_kafka\", description=\"frame from kafka image\")\n",
    "    kafka_frame.publish(topic=\"image_active\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "commonSubmit(cfg, \"Streaming3Turbine\", VideoRcvKafka())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is 'VideoRcvKafka' view up.\n",
    "The video feed frames are sent by VideoSndKafka.py, via EventStream/Kafka to this application.\n",
    "The received frames are published.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.display_views(instance, job_name=\"VideoRcvKafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and submit 'ImageAnalysis' application\n",
    "- Two analysis operators thatdo analysis: findFace , findObject.\n",
    "- Two applications feeding via pub/sub : VideoRcvKafka, ImageWebFetch\n",
    "- This takes a longgg time to subumit...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## compose the findImageElements application\n",
    "def ImageAnalysis():\n",
    "    \"\"\"Analyse the images with various tools. \n",
    "       - Subscribe to 'image_active' with encoded images in 'image_string'\n",
    "       - process 'image_string' with FaceRegions into 'face_regions' \n",
    "       - process 'image_string' with ObjectRegions into 'object_regions'\n",
    "       - \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"ImageAnalysis\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.weights', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.cfg', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "    \n",
    "    image_active = topo.subscribe(topic=\"image_active\")\n",
    "    # Find faces analysis ....\n",
    "    face_regions = image_active.map(cvsupport.FaceRegions(), name=\"face_regions\")\n",
    "    face_trimmed = face_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'face_regions':t['face_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"faces_trimmed\")\n",
    "    face_trimmed.view(name=\"faces_view\", description=\"faces regions\")\n",
    "    # Find objects analysis ...\n",
    "    object_regions = image_active.map(cvsupport.ObjectRegions(classes=\"../datasets/yolov3.txt\"), name=\"object_fetch\")\n",
    "    object_trimmed = object_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'object_regions':t['object_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"objects_trimmed\")\n",
    "    object_trimmed.view(name=\"objects_view\", description=\"object regions\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "## TODO turn into function - huge duplication \n",
    "## Compose and submit the findElements Application \n",
    "topo = ImageAnalysis()\n",
    "\n",
    "\n",
    "commonSubmit(cfg, \"Streaming3Turbine\", ImageAnalysis())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the ImageAnalysis processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the processes 'healthy' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the ImageAnalysis views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "streams_render.display_views(instance, job_name=\"ImageAnalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the ImageAnalysis application's FacesRegions results\n",
    "\n",
    "#### TODO - convert to thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Fetch FacesRegions start/get/stop\n",
    "faces_view = instance.get_views(name=\"faces_view\")[0]\n",
    "faces_view.start_data_fetch()\n",
    "faces_tuples = faces_view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "print(\"Number of faces_tuples:\", len(faces_tuples))\n",
    "faces_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# render the results.... only images with regions will arrive via this view\n",
    "source_face = widgets.Label(value=\"Source\", layout={'border': '1px solid green','width':'50%'})\n",
    "rect_face = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "url_face = widgets.Label(value=\"URL\", layout={'border': '1px solid green','width':'50%'})\n",
    "dashboard_face = widgets.VBox([source_face, rect_face, url_face])\n",
    "display(dashboard_face)\n",
    "\n",
    "# Fetch the tuples from the view\n",
    "count = 0\n",
    "for face in faces_tuples:\n",
    "    url_face.value = face['url']\n",
    "    #source_face.value = trimmed['source']  # TODO - move accros the source.\n",
    "    source_face.value = str(count)\n",
    "    count += 1\n",
    "    image_string = face['image_string']\n",
    "    with Image.open(io.BytesIO(base64.b64decode(image_string))) as bin_image:\n",
    "        for rect in face['face_regions']:\n",
    "            inscribe_rect(bin_image, rect)\n",
    "        with rect_face:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "            clear_output(wait=True)\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the ImageAnalysis application's ObjectRegions results\n",
    "Display the images and the objects they found. Run in a thread to make is easier to stop.\n",
    "\n",
    "On the Streams side it's processing the images to find objects, if not no objects are \n",
    "found they are not pushed to the view and will not be rendered here.\n",
    "\n",
    "The object detection maybe CPU intensive (you don't say) you may want to distribute this across hardware, which\n",
    "is left as an exercise to the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "class objectDashboard(object):\n",
    "    def __init__(self, instance, sleep=2):\n",
    "        self.instance = instance\n",
    "        self.source = widgets.Label(value=\"Source\", layout={'border': '1px solid green','width':'50%'})\n",
    "        self.rect = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "        self.url = widgets.Label(value=\"URL\", layout={'border': '1px solid green','width':'50%'})\n",
    "        self._class = widgets.Label(value=\"CLASS\", layout={'border': '1px solid green','width':'50%'})\n",
    "        self.button = widgets.Button(description='', button_style='danger', layout={'width':'25%'}, disabled=True)\n",
    "        self.dashboard = widgets.VBox([self.source, self.rect, self.url, self.button, self._class])\n",
    "        self.time_sleep = sleep\n",
    "        self.execute = threading.Event()\n",
    "        self.thread = None\n",
    "        \n",
    "\n",
    "    def render_tuples(self, tuples):\n",
    "        count = 0\n",
    "        self.source.value = \"view dequed:{} @ {}\".format(len(tuples), time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "        for tuple_object in tuples:\n",
    "            self.url.value = tuple_object['url']\n",
    "            self.source.value = \"{} of {}\".format(count, len(tuples))\n",
    "            count += 1\n",
    "            image_string = tuple_object['image_string']\n",
    "            class_string = \"\"\n",
    "            with Image.open(io.BytesIO(base64.b64decode(image_string))) as bin_image:\n",
    "                for rect in tuple_object['object_regions']:\n",
    "                    inscribe_rect(bin_image, rect['region'])\n",
    "                    class_string = \",\".join([class_string,rect['class']])\n",
    "                    self._class.value = class_string\n",
    "                with self.rect:\n",
    "                    display(resize_image(bin_image, baseheight=600))\n",
    "                    clear_output(wait=True)\n",
    "            if not self.execute.is_set():\n",
    "                break\n",
    "            time.sleep(self.time_sleep)\n",
    "            \n",
    "    def ignition(self, start):\n",
    "        if start:\n",
    "            self.thread = threading.Thread(target=self.fetchRender_thread, name=\"Render_View\")\n",
    "            self.thread.start()\n",
    "        else:\n",
    "            self.execute.clear()\n",
    "            self.button.description = \"Stopping...\"\n",
    "            self.button.disabled = True\n",
    "                \n",
    "\n",
    "    def fetchRender_thread(self):\n",
    "        self.button.disabled = True\n",
    "        self.button.description = 'Stop'\n",
    "        self.button.button_style = 'danger'\n",
    "        button_action = lambda w: self.ignition(False)\n",
    "        self.button.on_click(button_action)\n",
    "        self.button.disabled = False\n",
    "        self.execute.set()\n",
    "\n",
    "        # start up\n",
    "        objects_view = self.instance.get_views(name=\"objects_view\")[0]\n",
    "        objects_view.start_data_fetch()\n",
    "        # fetch til button pushed\n",
    "        while self.execute.is_set():\n",
    "            objects_tuples = objects_view.fetch_tuples(max_tuples=100, timeout=2)\n",
    "            self.render_tuples(objects_tuples)\n",
    "        # shut down\n",
    "        objects_view.stop_data_fetch()\n",
    "        # \n",
    "        self.button.description = 'Stopped'\n",
    "        self.button.button_style = 'warning'\n",
    "\n",
    "        \n",
    "object_dash = objectDashboard(instance, sleep=0.1)\n",
    "display(object_dash.dashboard)\n",
    "object_dash.ignition(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dash.execute.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
