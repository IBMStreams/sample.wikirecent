{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgAna_OpenCV * snap for home....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting this running locally\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "pip install pandas\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "pip install beautifulsoup4\n",
    "pip install Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase I:\n",
    "- **explore**\n",
    "- develop\n",
    "- push \n",
    "- production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from  functools import lru_cache\n",
    "import requests\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "\n",
    "import time\n",
    "from PIL import Image,  ImageDraw  # https://pillow.readthedocs.io/en/4.3.x/\n",
    "import io\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    \"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_raw = cv2.imread('../facialLocation.jpg')\n",
    "img_raw = cv2.imread('../datasets/baby1.png')\n",
    "print(type(img_raw))\n",
    "print(img_raw.shape)\n",
    "plt.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to grayscale\n",
    "img_bw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Displaying the grayscale image\n",
    "plt.imshow(img_bw, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase II:\n",
    "- explore\n",
    "- **develop**\n",
    "- push \n",
    "- production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToRGB(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(convertToRGB(img_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToBW(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(convertToBW(img_raw), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_cascade_face = cv2.CascadeClassifier('../datasets/haarcascade_frontalface_default.xml')\n",
    "haar_cascade_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_rects = haar_cascade_face.detectMultiScale(img_bw, scaleFactor = 1.2, minNeighbors = 5);\n",
    "\n",
    "# Let us print the no. of faces found\n",
    "print(\"Faces found: {} faces within regions:{}\".format(len(faces_rects), faces_rects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image, cascade, scaleFactor = 1.1):\n",
    "    # create a copy of the image to prevent any changes to the original one.\n",
    "    image_copy = image.copy()\n",
    "\n",
    "    #convert the test image to gray scale as opencv face detector expects gray images\n",
    "    gray_image = convertToBW(image_copy)\n",
    "\n",
    "    # Applying the haar classifier to detect faces\n",
    "    image_rects = cascade.detectMultiScale(gray_image, scaleFactor=scaleFactor, minNeighbors=5)\n",
    "    return image_rects\n",
    "\n",
    "def rects_render(image, rects):\n",
    "    image_copy = image.copy()\n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(image_copy, (x, y), (x+w, y+h), (0, 255, 0), 15)\n",
    "    return image_copy\n",
    "    \n",
    "img_raw = cv2.imread('../datasets/baby1.png')\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "if len(rects)> 0:\n",
    "    img_rects = rects_render(img_rgb, rects)\n",
    "    print(rects)\n",
    "    plt.imshow(img_rects)\n",
    "else:\n",
    "    print(\"On image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw = cv2.imread('../datasets/imgClassify.jpg')\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "img_rects = rects_render(img_rgb, rects)\n",
    "plt.imshow(img_rects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch a image from the web\n",
    "Problem - \n",
    "\n",
    "The image that gets processed is not the same image that is sent which makes the face region wrong. \n",
    "Notes:\n",
    "- [x] do not want to send images from streams\n",
    "- [x] Streams will the image, render gets image\n",
    "- [x] region overlays on image that is fetched.\n",
    "[x] get the face detection to work on the exact original image\n",
    "[-]work out the proportion between the computed image region and the original\n",
    "\n",
    "\n",
    "Use the the URLs for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and image from the web. \n",
    "urls = [\"https://upload.wikimedia.org/wikipedia/commons/1/1e/Christopher_de_Paus.JPG\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/a/a5/Anne_Bethel_Spencer_in_her_wedding_dress.jpg\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Barbara_Anderson_1969.JPG\"]\n",
    "response = requests.get(urls[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV locate face region : face_rects\n",
    "- OpenCV to fetch process image, locate the faces.\n",
    "- OpenCV to render the results.\n",
    "\n",
    "This processing will be pushed to Streams.\n",
    "\n",
    "\n",
    "#### face_rects[] is a list of retanges that the face is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bts_to_img(bts):\n",
    "    '''\n",
    "    :param bts: results from image_to_bts\n",
    "    '''\n",
    "    buff = np.fromstring(bts, np.uint8)\n",
    "    buff = buff.reshape(1, -1)\n",
    "    img = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "img_raw = bts_to_img(response.content)\n",
    "print(\"Size of image to process : \",img_raw.shape)\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "print(\"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects))\n",
    "img_rects = rects_render(img_rgb, face_rects)\n",
    "plt.imshow(img_rects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render with face_rects[] - without OpenCV crutch\n",
    "Using image from web add the rects. \n",
    "\n",
    "- Render in a widget, in a browser. \n",
    "- Streams provides the region of the page. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def line_box(ele):\n",
    "    (x,y,w,h)=ele\n",
    "    return (x,y, x+w,y, x+w,y+h, x,y+w, x,y)\n",
    "    \n",
    "\n",
    "\n",
    "def inscribe_rect(bin_image, detection_box):\n",
    "    \"\"\"Inscribe box on image\n",
    "    \n",
    "    This is updating the image passed in.\n",
    "    \n",
    "    Args:\n",
    "        bin_image : binary image\n",
    "        detection_box : region to put box around\n",
    "    Return:\n",
    "        return image - \n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(bin_image) \n",
    "    box_width = 10 \n",
    "    draw.line(line_box(detection_box), fill=\"yellow\", width=box_width)\n",
    "    #draw.rectangle(detection_box, fill=128)\n",
    "    return bin_image\n",
    "   \n",
    "\n",
    "def encode_img(img):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        contents = output.getvalue() \n",
    "    return base64.b64encode(contents).decode('ascii')\n",
    "\n",
    "def decode_img(bin64):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "    return img\n",
    "\n",
    "def resize_image(bin_image, basewidth=None, baseheight=None):\n",
    "    \"\"\"Resize image proportional to the base, make it fit in cell\"\"\"\n",
    "    if basewidth is not None:\n",
    "        wpercent = (basewidth/float(bin_image.size[0]))\n",
    "        hsize = int((float(bin_image.size[1])*float(wpercent)))\n",
    "        return bin_image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    wpercent = (baseheight/float(bin_image.size[1]))\n",
    "    wsize = int((float(bin_image.size[0])*float(wpercent)))\n",
    "    return bin_image.resize((wsize,baseheight), Image.ANTIALIAS)\n",
    "\n",
    "\n",
    "## Widget to display image\n",
    "demo_widget = widgets.Output(layout={'border': '1px solid green'})\n",
    "display(demo_widget)\n",
    "\n",
    "with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "    for rect in face_rects:\n",
    "        inscribe_rect(bin_image, rect)\n",
    "    with demo_widget:\n",
    "        display(bin_image)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images from Wiki via Streams\n",
    "\n",
    "At this point...\n",
    "- we can fetch images local/web\n",
    "- render the image in the browser\n",
    "- locate faces using the OpenCV facility.\n",
    "- render the image inscribed with 'suspected' face regions. \n",
    "\n",
    "Before we push this processing up to Streams we'll process the types of images the Streams applicagtion will be dealing with locally. \n",
    "If you have not you need to bring up the the Streams application composed in the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb) notebook. \n",
    "This application's view (soupActive) includes the URL of the image being submitted to WikiPedia, the image that we \n",
    "are going preform face OpenCV face detection.\n",
    "\n",
    "We'll do the processing on the Streams, results will be send back. Below were going \n",
    "the processing in the browser to work out the processing to do. \n",
    "\n",
    "This assumes that you have started the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb) \n",
    "\n",
    "### Phase III+:\n",
    "- explore\n",
    "- develop+\n",
    "- push \n",
    "- production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icpd_core import icpd_util\n",
    "cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')\n",
    "def get_instance():\n",
    "    \"\"\"Setup to access your Streams instance.\n",
    "\n",
    "    ..note::The notebook is work within Cloud and ICP4D. \n",
    "            Refer to the 'Setup' cells above.              \n",
    "    Returns:\n",
    "        instance : Access to Streams instance, used for submitting and rendering views.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from icpd_core import icpd_util\n",
    "        import urllib3\n",
    "        global cfg\n",
    "        cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        instance = rest.Instance.of_service(cfg)\n",
    "        print(\"Within ICP4D\")\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    except ImportError:\n",
    "        cfg = None\n",
    "        print(\"Outside ICP4D\")\n",
    "        import credential  \n",
    "        sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', \n",
    "                                               vcap_services=credential.vcap_conf)\n",
    "        instance = sc.get_instances()[0]\n",
    "    return instance,cfg\n",
    "\n",
    "instance,cfg = get_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start getting \n",
    "_view = instance.get_views(name=\"soupActive\")[0]\n",
    "_view.start_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_tuples = _view.fetch_tuples(max_tuples=100, timeout=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(view_tuple, image_region=None, rect_region=None, title_region=None, status_region=None, url_region=None):\n",
    "    img_url = view_tuple['img_desc'][0]['img']\n",
    "    response = requests.get(img_url)\n",
    "    \n",
    "    img_raw = bts_to_img(response.content)\n",
    "    if img_raw is None:\n",
    "        print(\"img_url:{} bts_img() failed\".format(img_url))\n",
    "        return\n",
    "    print(\"Size of image to process : \",img_raw.shape)\n",
    "    img_rgb = convertToRGB(img_raw)\n",
    "    face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "    status_region.value = \"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects)\n",
    "    # img_rects = rects_render(img_rgb, face_rects)\n",
    "    url_region.value = img_url\n",
    "    title_region.value = view_tuple['title']\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        with image_region:\n",
    "            display(resize_image(bin_image, baseheight=300))\n",
    "            clear_output(wait=True)\n",
    "            for rect in face_rects:\n",
    "                inscribe_rect(bin_image, rect)\n",
    "            with rect_region:\n",
    "                if len(face_rects) is not 0:\n",
    "                    display(resize_image(bin_image, baseheight=300))\n",
    "                    clear_output(wait=True)\n",
    "                else:\n",
    "                    clear_output()\n",
    "            \n",
    "\n",
    "#    display(bin_image)\n",
    "#    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the Dashboard - display images sent to Wikipedia \n",
    "##                         Next cell populates the 'Dashboard'.....\n",
    "status_widget = widgets.Label(value=\"Status\", layout={'border': '1px solid green','width':'30%'})\n",
    "url_widget = widgets.Label(value=\"Img URL\", layout={'border': '1px solid green','width':'100%'})\n",
    "image_widget = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'300pt'})\n",
    "rect_widget = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'300pt'})\n",
    "title_widget = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'30%'})\n",
    "dashboard = widgets.VBox([status_widget, image_widget, rect_widget, title_widget, url_widget])\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is going to populate the dashboard above.\n",
    "## - Re-execute the cell to fetch/process/render images.\n",
    "## - Note the debug statements below.\n",
    "\n",
    "@catchInterrupt\n",
    "def server_soup(count=1):\n",
    "    \"\"\"Fetch and display images from view.\n",
    "    Args::\n",
    "        count: number of iterations to fetch images, count<0\n",
    "        is infinite\n",
    "    \"\"\"\n",
    "    while count != 0:\n",
    "        count -= 1\n",
    "        view_tuples = _view.fetch_tuples(max_tuples=100, timeout=2)\n",
    "        print(\"remaining cycles:{} tuples:{}\".format(count, len(view_tuples)))\n",
    "        for soup_tuple in view_tuples:\n",
    "            display_image(soup_tuple, image_region=image_widget, rect_region=rect_widget, title_region=title_widget, status_region=status_widget, url_region=url_widget)\n",
    "\n",
    "server_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase :\n",
    "- explore\n",
    "- develop\n",
    "- push \n",
    "- **production**\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
