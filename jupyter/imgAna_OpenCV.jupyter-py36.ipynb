{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgAna_OpenCV \n",
    "\n",
    "In the past a service was invoked to locate faces, this is a replacement for that outboard processing locating\n",
    "the faces using an OpenCV components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note : \n",
    "\n",
    "Getting this build locally and push up to the cloud you need the following...\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "pip install pandas\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "pip install beautifulsoup4\n",
    "pip install Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from  functools import lru_cache\n",
    "import requests\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "\n",
    "import time\n",
    "from PIL import Image,  ImageDraw  # https://pillow.readthedocs.io/en/4.3.x/\n",
    "import io\n",
    "import base64\n",
    "import sys\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "import cvsupport\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"imgAna_OpenCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    \"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a local file\n",
    "Using OpenCv so some  processing on a local file.\n",
    "* do some image manipulation\n",
    "* find the face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw = cv2.imread('../datasets/baby1.png')\n",
    "print(type(img_raw))\n",
    "print(img_raw.shape)\n",
    "plt.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToRGB(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(convertToRGB(img_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToBW(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(convertToBW(img_raw), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_cascade_face = cv2.CascadeClassifier('../datasets/haarcascade_frontalface_default.xml')\n",
    "haar_cascade_face\n",
    "\n",
    "faces_rects = haar_cascade_face.detectMultiScale(convertToBW(img_raw), scaleFactor = 1.2, minNeighbors = 5);\n",
    "\n",
    "# Let us print the no. of faces found\n",
    "print(\"Faces found: {} faces within regions:{}\".format(len(faces_rects), faces_rects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize \n",
    "* Find faces\n",
    "* Draw a box around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image, cascade, scaleFactor = 1.1):\n",
    "    # create a copy of the image to prevent any changes to the original one.\n",
    "    image_copy = image.copy()\n",
    "\n",
    "    #convert the test image to gray scale as opencv face detector expects gray images\n",
    "    gray_image = convertToBW(image_copy)\n",
    "\n",
    "    # Applying the haar classifier to detect faces\n",
    "    image_rects = cascade.detectMultiScale(gray_image, scaleFactor=scaleFactor, minNeighbors=5)\n",
    "    return image_rects\n",
    "\n",
    "def rects_render(image, rects):\n",
    "    image_copy = image.copy()\n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(image_copy, (x, y), (x+w, y+h), (0, 255, 0), 15)\n",
    "    return image_copy\n",
    "    \n",
    "## Render the image with the faces_rects\n",
    "if len(faces_rects)> 0:\n",
    "    img_rects = rects_render(convertToRGB(img_raw), faces_rects)\n",
    "    #print(img_rects)\n",
    "    plt.imshow(img_rects)\n",
    "else:\n",
    "    print(\"On image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another image....\n",
    "img_raw = cv2.imread('../datasets/imgClassify.jpg')\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "img_rects = rects_render(img_rgb, rects)\n",
    "plt.imshow(img_rects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch a image from the web\n",
    "\n",
    "\n",
    "Use the the URLs for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and image from the web. \n",
    "urls = [\"https://media.wired.com/photos/5c5354d391d0df22c1dee493/master/w_2560%2Cc_limit/Backchannel-Lena-Final.jpg\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/1/1e/Christopher_de_Paus.JPG\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/a/a5/Anne_Bethel_Spencer_in_her_wedding_dress.jpg\",\n",
    "       \"https://upload.wikimedia.org/wikipedia/commons/f/f7/Barbara_Anderson_1969.JPG\"]\n",
    "response = requests.get(urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV locate face region : face_rects\n",
    "- OpenCV to fetch process image, locate the faces.\n",
    "- OpenCV to render the results.\n",
    "\n",
    "This processing will be pushed to Streams.\n",
    "\n",
    "\n",
    "#### face_rects[] is a list of retanges that the face is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bts_to_img(bts):\n",
    "    '''\n",
    "    :param bts: results from image_to_bts\n",
    "    '''\n",
    "    buff = np.fromstring(bts, np.uint8)\n",
    "    buff = buff.reshape(1, -1)\n",
    "    img = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "img_raw = bts_to_img(response.content)\n",
    "print(\"Size of image to process : \",img_raw.shape)\n",
    "img_rgb = convertToRGB(img_raw)\n",
    "face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "print(\"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects))\n",
    "img_rects = rects_render(img_rgb, face_rects)\n",
    "plt.imshow(img_rects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render with face_rects[] - reduce dependency on numpy\n",
    "Using image from web add the rects. \n",
    "\n",
    "- Render in a widget, in a browser. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_box(ele):\n",
    "    (x,y,w,h)=ele\n",
    "    return (x,y, x+w,y, x+w,y+h, x,y+w, x,y)\n",
    "    \n",
    "\n",
    "def inscribe_rect(bin_image, detection_box):\n",
    "    \"\"\"Inscribe box on image\n",
    "    \n",
    "    This is updating the image passed in.\n",
    "    \n",
    "    Args:\n",
    "        bin_image : binary image\n",
    "        detection_box : region to put box around\n",
    "    Return:\n",
    "        return image - \n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(bin_image) \n",
    "    box_width = 10 \n",
    "    draw.line(line_box(detection_box), fill=\"yellow\", width=box_width)\n",
    "    return bin_image\n",
    "   \n",
    "\n",
    "def encode_img(img):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        contents = output.getvalue() \n",
    "    return base64.b64encode(contents).decode('ascii')\n",
    "\n",
    "def decode_img(bin64):\n",
    "    \"\"\"must be easier way\"\"\"\n",
    "    img = Image.open(io.BytesIO(base64.b64decode(bin64)))\n",
    "    return img\n",
    "\n",
    "def resize_image(bin_image, basewidth=None, baseheight=None):\n",
    "    \"\"\"Resize image proportional to the base, make it fit in cell\"\"\"\n",
    "    if basewidth is not None:\n",
    "        wpercent = (basewidth/float(bin_image.size[0]))\n",
    "        hsize = int((float(bin_image.size[1])*float(wpercent)))\n",
    "        return bin_image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "    wpercent = (baseheight/float(bin_image.size[1]))\n",
    "    wsize = int((float(bin_image.size[0])*float(wpercent)))\n",
    "    return bin_image.resize((wsize,baseheight), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Widget to display image\n",
    "demo_widget = widgets.Output(layout={'border': '1px solid green'})\n",
    "display(demo_widget)\n",
    "with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "    for rect in face_rects:\n",
    "        inscribe_rect(bin_image, rect)\n",
    "    with demo_widget:\n",
    "        display(bin_image)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images from Wiki via Streams\n",
    "\n",
    "At this point...\n",
    "- we can fetch images local/web\n",
    "- render the image in the browser\n",
    "- locate faces using the OpenCV facility.\n",
    "- render the image inscribed with 'suspected' face regions. \n",
    "\n",
    "Before we push this processing up to Streams we'll process the types of images the Streams applicagtion will be dealing with locally. \n",
    "If you have not you need to bring up the the Streams application composed in the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb) notebook. \n",
    "This application's view (soupActive) includes the URL of the image being submitted to WikiPedia, the image that we \n",
    "are going preform face OpenCV face detection.\n",
    "\n",
    "We'll do the processing on the Streams, results will be send back. Below were going \n",
    "the processing in the browser to work out the processing to do. \n",
    "\n",
    "This assumes that you have started the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# Setup\n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICPD setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you using ICP4D insert your creds here...\n",
    "#from icpd_core import icpd_util\n",
    "#cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing Streams Instance & views\n",
    "\n",
    "def get_instance():\n",
    "    \"\"\"Setup to access your Streams instance.\n",
    "\n",
    "    ..note::The notebook is work within Cloud and ICP4D. \n",
    "            Refer to the 'Setup' cells above.              \n",
    "    Returns:\n",
    "        instance : Access to Streams instance, used for submitting and rendering views.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from icpd_core import icpd_util\n",
    "        import urllib3\n",
    "        global cfg\n",
    "        cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        instance = rest.Instance.of_service(cfg)\n",
    "        print(\"Within ICP4D\")\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    except ImportError:\n",
    "        cfg = None\n",
    "        print(\"Outside ICP4D\")\n",
    "        import credential  \n",
    "        sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', \n",
    "                                               vcap_services=credential.vcap_conf)\n",
    "        instance = sc.get_instances()[0]\n",
    "    return instance,cfg\n",
    "\n",
    "instance,cfg = get_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch the view that has the image URLs.\n",
    "_view = instance.get_views(name=\"soupActive\")[0]\n",
    "_view.start_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_tuples = _view.fetch_tuples(max_tuples=100, timeout=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(view_tuple, image_region=None, rect_region=None, title_region=None, status_region=None, url_region=None):\n",
    "    img_url = view_tuple['img_desc'][0]['img']\n",
    "    response = requests.get(img_url)\n",
    "    \n",
    "    img_raw = bts_to_img(response.content)\n",
    "    if img_raw is None:\n",
    "        print(\"img_url:{} bts_img() failed\".format(img_url))\n",
    "        return\n",
    "    img_rgb = convertToRGB(img_raw)\n",
    "    face_rects = detect_faces(img_rgb, haar_cascade_face)\n",
    "    status_region.value = \"Found: {} potential faces. \\n {}\".format(len(face_rects),  face_rects)\n",
    "    # img_rects = rects_render(img_rgb, face_rects)\n",
    "    url_region.value = img_url\n",
    "    title_region.value = view_tuple['title']\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        with image_region:\n",
    "            display(resize_image(bin_image, baseheight=300))\n",
    "            clear_output(wait=True)\n",
    "            for rect in face_rects:\n",
    "                inscribe_rect(bin_image, rect)\n",
    "            with rect_region:\n",
    "                if len(face_rects) is not 0:\n",
    "                    display(resize_image(bin_image, baseheight=300))\n",
    "                    clear_output(wait=True)\n",
    "                else:\n",
    "                    clear_output()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render in 'Dashboard' images that processed locally using OpenCV. Best to get things working \n",
    "## locally before pushing it up to Streams. \n",
    "## \n",
    "## Images are coming from a Wikipedia via a Streams view.\n",
    "##\n",
    "status_local = widgets.Label(value=\"Status\", layout={'border': '1px solid green','width':'600%'})\n",
    "url_local = widgets.Label(value=\"Img URL\", layout={'border': '1px solid green','width':'60%'})\n",
    "image_local = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'300pt'})\n",
    "rect_local = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'300pt'})\n",
    "hbox_local = widgets.HBox([image_local, rect_local])\n",
    "title_local = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'60%'})\n",
    "dashboard = widgets.VBox([status_local, hbox_local, title_local, url_local])\n",
    "display(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is going to populate the dashboard above.\n",
    "## The left pane has the image that arrived, the right \n",
    "## show the faces demarked, if none found no image.\n",
    "## \n",
    "## - Re-execute the cell to fetch/process/render images.\n",
    "## - Note the debug statements below.\n",
    "\n",
    "@catchInterrupt\n",
    "def server_soup(count=1):\n",
    "    \"\"\"Fetch and display images from view.\n",
    "    Args::\n",
    "        count: number of iterations to fetch images, count<0\n",
    "        is infinite\n",
    "    \"\"\"\n",
    "    while count != 0:\n",
    "        count -= 1\n",
    "        view_tuples = _view.fetch_tuples(max_tuples=12, timeout=2)\n",
    "        print(\"remaining cycles:{} tuples:{}\".format(count, len(view_tuples)))\n",
    "        for soup_tuple in view_tuples:\n",
    "            display_image(soup_tuple, image_region=image_local, rect_region=rect_local, title_region=title_local, status_region=status_local, url_region=url_local)\n",
    "\n",
    "server_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streams support - move out....\n",
    "def list_jobs(_instance=None, cancel=False):\n",
    "    \"\"\"\n",
    "    Interactive selection of jobs to cancel.\n",
    "    \n",
    "    Prompts with SelectMultiple widget, if thier are no jobs, your presente with a blank list.\n",
    "    \n",
    "    \"\"\"\n",
    "    active_jobs = { \"{}:{}\".format(job.name, job.health):job for job in _instance.get_jobs()}\n",
    "\n",
    "    selectMultiple_jobs = widgets.SelectMultiple(\n",
    "        options=active_jobs.keys(),\n",
    "        value=[],\n",
    "        rows=len(active_jobs),\n",
    "        description = \"Cancel jobs(s)\" if cancel else \"Active job(s):\",\n",
    "        layout=Layout(width='60%')\n",
    "    )\n",
    "    cancel_jobs = widgets.ToggleButton(\n",
    "        value=False,\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Delete selected jobs',\n",
    "        icon=\"stop\"\n",
    "    )\n",
    "    def on_value_change(change):\n",
    "        for job in selectMultiple_jobs.value:\n",
    "            print(\"canceling job:\", job, active_jobs[job].cancel())\n",
    "        cancel_jobs.disabled = True\n",
    "        selectMultiple_jobs.disabled = True\n",
    "\n",
    "    cancel_jobs.observe(on_value_change, names='value')\n",
    "    if cancel:\n",
    "        return HBox([selectMultiple_jobs, cancel_jobs])\n",
    "    else:\n",
    "        return HBox([selectMultiple_jobs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   To Processing Operators. \n",
    "Above we created code to fetch a image from the web and find faces, which is composed a number of functions. \n",
    "To push it up to Streams we've rolled up the processing into two functions.\n",
    "\n",
    "The cvsupport.py in [scripts](../scripts) class that are used below to compose the application.\n",
    "- ImageFetch[] - web request that puts image in tuple. The most time consuming portion of the processing is fetching the images using the URL. Retrieve the image from and push the image data into the Stream where it will be processed by downstream operator.\n",
    "- FaceRegions[] - locates faces in image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the ImageFetch[] & FaceRegions[] before pushing up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://media.wired.com/photos/5c5354d391d0df22c1dee493/master/w_2560%2Cc_limit/Backchannel-Lena-Final.jpg\"\n",
    "#url = \"https://upload.wikimedia.org/wikipedia/commons/0/0a/Great_Western_Road%2C_53-100_Kelvin_Court%2C_Glasgow.jpg\"\n",
    "tuple1 = {'img_desc':[{'img':url}]}   # simulate the tuple to process\n",
    "image_fetch = cvsupport.ImageFetch()\n",
    "\n",
    "# simulate the Stream.\n",
    "tuple2 = image_fetch(tuple1)\n",
    "with cvsupport.FaceRegions(haar_file=\"../datasets/haarcascade_frontalface_default.xml\") as fr:\n",
    "    print(type(fr))\n",
    "    tuple3 = fr(tuple2)\n",
    "#face_region = cvsupport.FaceRegions(haar_file=\"../datasets/haarcascade_frontalface_default.xml\")\n",
    "#tuple3 = face_region(tuple2)\n",
    "\n",
    "# render the results.... create a\n",
    "if tuple3 is not None:\n",
    "    print(\" URL:{}\\n had {} regions \\n ... {}\".format(tuple3['img_desc'][0]['img'], len(tuple3['face_regions']), tuple3['face_regions']))\n",
    "    verify_widget = widgets.Output(layout={'border': '1px solid green'})\n",
    "    display(verify_widget)\n",
    "\n",
    "    response = requests.get(tuple3['img_desc'][0]['img'])  # fetch the image\n",
    "\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        for rect in tuple3['face_regions']:\n",
    "            inscribe_rect(bin_image, rect)\n",
    "        with verify_widget:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "else:\n",
    "    print(\"No faces located in image....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose and Submit the application.\n",
    "\n",
    "The Streams application that will get Tuples by subscribing a feed\n",
    "that is composed in the [WikiRecentPhase3](./imgAna_3.jupyter-py36.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compose the findFaces application\n",
    "def findFacesApplication():\n",
    "    \"\"\"Compose the application\n",
    "    \n",
    "       - Subscribe to stream with images urls.\n",
    "       - fetch the image into 'img_string'\n",
    "       - finds faces into 'face_regions' \n",
    "       - \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"findFaces\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "    \n",
    "    soup_active = topo.subscribe(topic=\"soup_active\")\n",
    "    active_image = soup_active.map(cvsupport.ImageFetch(), name=\"image_fetch\")\n",
    "    active_image.view(name=\"image_fetch\", description=\"encoded binary image\")\n",
    "    face_regions = active_image.map(cvsupport.FaceRegions(), name=\"face_regions\")\n",
    "    face_trimmed = face_regions.map(lambda t: {\n",
    "        'user':t['user'],\n",
    "        'title':t['title'],\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'face_regions':t['face_regions'] \n",
    "    }, name=\"trimmed_view\")\n",
    "    face_trimmed.view(name=\"trimmed_view\", description=\"drop image\")\n",
    "    return topo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submit the findFaces application\n",
    "topo = findFacesApplication()\n",
    "\n",
    "if cfg is not None:\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    submission_result = context.submit(\"DISTRIBUTED\",topo, config=cfg)\n",
    "\n",
    "if cfg is None:\n",
    "    import credential\n",
    "    cloud = {\n",
    "        context.ConfigParams.VCAP_SERVICES: credential.vcap_conf,\n",
    "        context.ConfigParams.SERVICE_NAME: \"Streaming3Turbine\",\n",
    "        context.ContextTypes.STREAMING_ANALYTICS_SERVICE:\"STREAMING_ANALYTIC\",\n",
    "        context.ConfigParams.FORCE_REMOTE_BUILD: True,\n",
    "    }\n",
    "    submission_result = context.submit(\"STREAMING_ANALYTICS_SERVICE\",topo,config=cloud)\n",
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    print(\"JobId: \", submission_result['id'] , \"Name: \", submission_result['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the processing results of the findFaces application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start getting \n",
    "_trimmed = instance.get_views(name=\"trimmed_view\")[0]\n",
    "_trimmed.start_data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the results.... only images with regions will arrive via this view\n",
    "\n",
    "title_face = widgets.Label(value=\"Title\", layout={'border': '1px solid green','width':'50%'})\n",
    "rect_face = widgets.Output(layout={'border': '1px solid red','width':'50%','height':'300pt'})\n",
    "url_face = widgets.Label(value=\"URL\", layout={'border': '1px solid green','width':'50%'})\n",
    "dashboard = widgets.VBox([title_face, rect_face, url_face])\n",
    "display(dashboard)\n",
    "trimmed = trimmed_tuples[1]\n",
    "\n",
    "# Fetch the tuples from the view\n",
    "trimmed_tuples = _trimmed.fetch_tuples(max_tuples=100, timeout=2)\n",
    "\n",
    "for trimmed in trimmed_tuples:\n",
    "    response = requests.get(trimmed['url'])  # fetch the image\n",
    "    title_face.value = trimmed['title']\n",
    "    url_face.value = trimmed['url']\n",
    "    with Image.open(io.BytesIO(response.content)) as bin_image:\n",
    "        for rect in trimmed['face_regions']:\n",
    "            inscribe_rect(bin_image, rect)\n",
    "        with rect_face:\n",
    "            display(resize_image(bin_image, baseheight=600))\n",
    "            clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
