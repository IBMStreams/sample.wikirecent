{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiRecentFinale\n",
    "\n",
    "For the impatient (like me). \n",
    "\n",
    "This is notebook is functional equivilant to the imgAna4 notebook, it has fewer cells since\n",
    "support code has been moved into script files. The other notebooks in this directory\n",
    "walk through building up this application in detail. \n",
    "\n",
    "\n",
    "\n",
    "## Overview \n",
    "This application locates, crops and performs emotion classification of photos containing people submitted to WikiPedia. The application is composed, built and submitted looks like...\n",
    "\n",
    "![stillPhase5.jpg](images/stillPhase4.jpg)\n",
    "\n",
    "Events are flowing from left to right:\n",
    "- WikiPedia sends out events via a SSE connection, they enter the stream on the far left.\n",
    "- Many WikiPedia events are generated by 'robots' that check for 'undesirable' content, robot actions are dropped from this stream.\n",
    "- For this application WikiPedia sends inconseqential fields, these are pared down in this next operation. \n",
    "- WikiPedia gets data in many languages spread over many servers, the next operation aggregates the language. The result of this processing is not rendered in this notebook, refer to a prior notebooks for rendering. \n",
    "- Some events refer to images, the next operator leverages the [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to discover referenced images. If an image is not found, the event is dropped. \n",
    "- Some of the images submitted to WikiPedia may have faces, we use the [Facial Recognizer](https://developer.ibm.com/exchanges/models/all/max-facial-recognizer/) (NeuralNet) to extract  faces from images. If no potential face is not found, the event is dropped. All potential faces are cropped from images and sent to the next operator.\n",
    "- The last operator in this stream, on the far right, uses the [Facial Emotion Classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/)(NeuralNet)\n",
    "to classify the cropped images. \n",
    "-  Images with potential faces, cropped and the scored are rendered below. \n",
    "\n",
    "Execute the following cells to setup, compose, submit and render the live WikiPedia data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# Setup\n",
    "### Add credentials for the IBM Streams service\n",
    "This notebook has been tested using the ICP4D and Cloud instances. ICP4D uses the systems notebook's. I've submitted to Cloud from Jupyter notebook deployed from an Anaconda install.\n",
    "#### ICPD setup\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud setup\n",
    "\n",
    "To use Streams instance running in the cloud setup a [credential.py](setup_credential.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Show me\n",
    "After doing the 'Setup' above you can use Menu 'Cell' | 'Run All' to compose, build, submit and start the rendering of the live Wikidata. Go to [Show me now](#showMeNow) for the rendering - be patient, a significant amount of processing/communicating is being done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install components\n",
    "!pip install sseclient\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import support code that was embeded\n",
    "from streams_render import list_jobs\n",
    "from streams_render import display_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the server : ICP4D or Cloud instance.¶\n",
    "\n",
    "Attempt to import if fails the cfg will not be defined we know were using Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside ICP4D\n"
     ]
    }
   ],
   "source": [
    "def get_instance(cfg=None):\n",
    "    \"\"\"Setup to access your Streams instance.\n",
    "\n",
    "    ..note::The notebook is work within Cloud and ICP4D. \n",
    "            Refer to the 'Setup' cells above.              \n",
    "    Returns:\n",
    "        instance : Access to Streams instance, used for submitting and rendering views.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from icpd_core import icpd_util\n",
    "        import urllib3\n",
    "        cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        instance = rest.Instance.of_service(cfg)\n",
    "        print(\"Within ICP4D\")\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    except ImportError:\n",
    "        cfg = None\n",
    "        print(\"Outside ICP4D\")\n",
    "        import credential  \n",
    "        sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', \n",
    "                                               vcap_services=credential.vcap_conf)\n",
    "        instance = sc.get_instances()[0]\n",
    "    return instance,cfg\n",
    "\n",
    "try:\n",
    "    cfg\n",
    "except NameError:\n",
    "    cfg = None\n",
    "instance,cfg = get_instance(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List jobs and cancel....\n",
    "\n",
    "This page will submit a job named 'WikiPhase4'. If it's running you'll want to cancel it before submitting a new version. If it is running, no need to cancel/submit you can just procede to the [Viewing data section](#viewingData).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b6bee39a8147fa8be5935b660035a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(SelectMultiple(description='Cancel jobs(s)', layout=Layout(width='60%'), options=(), rows=0, value=()), ToggleButton(value=False, button_style='warning', description='Cancel', icon='stop', tooltip='Delete selected jobs')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='composeSubmit'></a>\n",
    "## Compose, build and submit the Streams application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the operations that will be composed.\n",
    "from streams_operations import get_events\n",
    "from streams_operations import sum_aggregation\n",
    "from streams_operations import tally_fields\n",
    "from streams_operations import wiki_lang\n",
    "from streams_operations import soup_image_extract\n",
    "from streams_operations import facial_image\n",
    "from streams_operations import emotion_image\n",
    "\n",
    "## Compose the Flow\n",
    "def WikiRecentFinale(jobName=None, wiki_lang_fname=None):\n",
    "    \"\"\"\n",
    "    Compose topology. \n",
    "    -- wiki_lang : csv file mapping database name to langauge\n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    ### make sure we sseclient in Streams environment.\n",
    "    topo.add_pip_package('sseclient')\n",
    "    topo.add_pip_package('bs4')\n",
    "\n",
    "    ## wiki events\n",
    "    wiki_events = topo.source(get_events, name=\"wikiEvents\")\n",
    "    ## select events generated by humans\n",
    "    human_filter = wiki_events.filter(lambda x: x['type']=='edit' and x['bot'] is False, name='humanFilter')\n",
    "    # pare down the humans set of columns\n",
    "    pared_human= human_filter.map(lambda x : {'timestamp':x['timestamp'],\n",
    "                                              'new_len':x['length']['new'],\n",
    "                                              'old_len':x['length']['old'], \n",
    "                                              'delta_len':x['length']['new'] - x['length']['old'],\n",
    "                                              'wiki':x['wiki'],'user':x['user'],\n",
    "                                              'title':x['title']}, \n",
    "                        name=\"paredHuman\")\n",
    "    pared_human.view(buffer_time=1.0, sample_size=200, name=\"paredEdits\", description=\"Edits done by humans\")\n",
    "\n",
    "    ## Define window(count)& aggregate\n",
    "    sum_win = pared_human.last(100).trigger(20)\n",
    "    sum_aggregate = sum_win.aggregate(sum_aggregation(sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }), name=\"sumAggregate\")\n",
    "    sum_aggregate.view(buffer_time=1.0, sample_size=200, name=\"aggEdits\", description=\"Aggregations of human edits\")\n",
    "\n",
    "    ## Define window(count) & tally edits\n",
    "    tally_win = pared_human.last(100).trigger(10)\n",
    "    tally_top = tally_win.aggregate(tally_fields(fields=['user', 'title'], top_count=10), name=\"talliesTop\")\n",
    "    tally_top.view(buffer_time=1.0, sample_size=200, name=\"talliesCount\", description=\"Top count tallies: user,titles\")\n",
    "\n",
    "    ## augment filterd/pared edits with language\n",
    "    if cfg is None:        \n",
    "        lang_augment = pared_human.map(wiki_lang(fname='../datasets/wikimap.csv'), name=\"langAugment\")\n",
    "    else:\n",
    "        lang_augment = pared_human.map(wiki_lang(fname=os.environ['DSX_PROJECT_DIR']+'/datasets/wikimap.csv'), name=\"langAugment\")\n",
    "\n",
    "    lang_augment.view(buffer_time=1.0, sample_size=200, name=\"langAugment\", description=\"Language derived from wiki\")\n",
    "\n",
    "    ## Define window(time) & tally language\n",
    "    time_lang_win = lang_augment.last(datetime.timedelta(minutes=2)).trigger(5)\n",
    "    time_lang = time_lang_win.aggregate(tally_fields(fields=['language'], top_count=10), name=\"timeLang\")\n",
    "    time_lang.view(buffer_time=1.0, sample_size=200, name=\"talliesTime\", description=\"Top timed tallies: language\")\n",
    "\n",
    "    ## attempt to extract image using beautifulsoup add img_desc[{}] field\n",
    "    soup_image = lang_augment.map(soup_image_extract(field_name=\"title\", url_base=\"https://www.wikidata.org/wiki/\"),name=\"imgSoup\")\n",
    "    soup_active = soup_image.filter(lambda x: x['img_desc'] is not None and len(x['img_desc']) > 0, name=\"soupActive\")\n",
    "    soup_active.view(buffer_time=1.0, sample_size=200, name=\"soupActive\", description=\"Image extracted via Bsoup\")\n",
    "    \n",
    "    ## facial extraction  - \n",
    "    facial_images = soup_active.map(facial_image(field_name='img_desc'),name=\"facialImgs\")\n",
    "    face_image = facial_images.flat_map(name=\"faceImg\")\n",
    "    face_image.view(buffer_time=10.0, sample_size=20, name=\"faceImg\", description=\"Face image analysis/extraction\")\n",
    "\n",
    "    ## emotion anaylsis on image - \n",
    "    face_emotion = face_image.map(emotion_image(), name=\"faceEmotion\")\n",
    "    face_emotion.view(buffer_time=10.0, sample_size=20, name=\"faceEmotion\", description=\"Factial emotion analysis\")\n",
    "       \n",
    "    return ({\"topo\":topo,\"view\":{ }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Submit : ICP or Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd713d296067418f9e7134eec14aee5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId:  2 Name:  ipythoninput779cc0244094f::WikiRecentFinale_2\n"
     ]
    }
   ],
   "source": [
    "resp = WikiRecentFinale(jobName=\"WikiRecentFinale\")\n",
    "if cfg is not None:\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    submission_result = context.submit(\"DISTRIBUTED\",resp['topo'], config=cfg)\n",
    "\n",
    "if cfg is None:\n",
    "    import credential\n",
    "    cloud = {\n",
    "        context.ConfigParams.VCAP_SERVICES: credential.vcap_conf,\n",
    "        context.ConfigParams.SERVICE_NAME: \"Streaming3Turbine\",\n",
    "        context.ContextTypes.STREAMING_ANALYTICS_SERVICE:\"STREAMING_ANALYTIC\",\n",
    "        context.ConfigParams.FORCE_REMOTE_BUILD: True,\n",
    "    }\n",
    "    submission_result = context.submit(\"STREAMING_ANALYTICS_SERVICE\",resp['topo'],config=cloud)\n",
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    print(\"JobId: \", submission_result['id'] , \"Name: \", submission_result['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viewingData'></a>\n",
    "## Viewing data \n",
    "\n",
    "The running application has a number of views, a view enables observation of the data moving through the stream. The following cell will fetch the views' queue and display it's data when selected. \n",
    "\n",
    "| view name | description of data is the view | bot |\n",
    "|---------|-------------|--------------|\n",
    "|aggEdits  | summarised fields | False |\n",
    "|langAugment | mapped augmented fields | False |\n",
    "|paredEdits | seleted fields | False |\n",
    "|talliesCount | last 100 messages tallied | False | \n",
    "|talliesTimes | 2 minute windowed | False |\n",
    "|soupActive | extracted images links| False |\n",
    "|faceImg | analyse image for faces and extract | False |\n",
    "|faceEmotion | emotional analysis of facial images | False | \n",
    "\n",
    "\n",
    "\n",
    "You want to stop the the fetching the view data when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data that is flowing.....\n",
    "display_views(instance, \"WikiRecentFinale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![phase4_1.gif](attachment:phase5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Views / Render Views UI\n",
    "\n",
    "From the server this is getting the cropped images. Streams is passing the image through the \n",
    "IBM Facial Recognizer that extracts the coordinates of potential faces. A new tuple is generated\n",
    "for each potential face consisting of the \n",
    "- input tuple, this include a url image being analyzed\n",
    "- face dict() consisting of ...\n",
    "- - probability : probabilty that it's an face\n",
    "- - image_percentage : % of image original image the found face occupies\n",
    "- - bytes_PIL_b64 : binary image version of found image\n",
    "- - detection_box : region within the original image the face was detected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='showMeNow'></a>\n",
    "### Show me now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fd075b1ddd4cbbb170c9ecead1350e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border='1px solid red', height='300pt', width='100%')), HBox(children=(VBox(children=(Label(value='prop:0', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:1', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:2', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:3', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:4', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:5', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt')))), VBox(children=(Label(value='prop:6', layout=Layout(border='1px solid blue', width='100pt')), Label(value='image %', layout=Layout(border='1px solid blue', width='100pt')), Output(layout=Layout(border='1px solid blue', height='120pt', width='100pt')), Output(layout=Layout(border='1px solid black', height='100pt', width='100pt'))))))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Setup the 'Dashboard' - Display the images sent to Wikipedia, result of facial extraction followed by emotion (pie chart) analysis\n",
    "##                         Next cell populates the 'Dashboard'.....\n",
    "from streams_render import render_emotions\n",
    "crops_bar = list()  # setup in layout section.\n",
    "bar_cells = 7\n",
    "full_widget = widgets.Output(layout={'border': '1px solid red','width':'100%','height':'300pt'})\n",
    "vbox_bar = list()\n",
    "for idx in range(bar_cells):\n",
    "    vbox = {\n",
    "        'probability' : widgets.Label(value=\"prop:{}\".format(idx), layout={'border': '1px solid blue','width':'100pt'}),\n",
    "        'image_percent' : widgets.Label(value=\"image %\", layout={'border': '1px solid blue','width':'100pt'}),\n",
    "        'image' : widgets.Output(layout={'border': '1px solid blue','width':'100pt','height':'120pt'}),\n",
    "        'pie' : widgets.Output(layout={'border': '1px solid black','width':'100pt','height':'100pt'})\n",
    "    }\n",
    "    crops_bar.append(vbox)\n",
    "    vbox_bar.append(widgets.VBox([vbox['probability'], vbox['image_percent'], vbox['image'], vbox['pie']]))\n",
    "    \n",
    "display(widgets.VBox([full_widget,widgets.HBox(vbox_bar)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Populate the dashboard - If you want this to run longer set cnt higher\n",
    "cnt = 40\n",
    "_view = instance.get_views(name=\"faceEmotion\")[0]\n",
    "_view.start_data_fetch()\n",
    "for idx in range(10):\n",
    "    emotion_tuples = _view.fetch_tuples(max_tuples=10, timeout=20)\n",
    "    print(\"Count of tuples\", len(emotion_tuples))\n",
    "    render_emotions(emotion_tuples, full_widget, crops_bar, bar_cells)\n",
    "_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel jobs when your done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d29bf19d9c4c3a8a34688a94b411c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(SelectMultiple(description='Cancel jobs(s)', layout=Layout(width='60%'), options=('ipythoninput779cc0244094f::WikiRecentFinale_2:healthy',), rows=1, value=()), ToggleButton(value=False, button_style='warning', description='Cancel', icon='stop', tooltip='Delete selected jobs')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook wrap up¶\n",
    "\n",
    "In this notebook we composed and deployed a Streams application that processes live Wikipedia events on a server. It \n",
    "extended the previous application to using the extracted image that we applied \n",
    "deep learning image processing models to derive insight into the images submitted.\n",
    "\n",
    "## Extentions\n",
    "The processing on the stream can continue on, gaining more insights into the\n",
    "events occuring on the Wikipedia servers.\n",
    "\n",
    "Possible explorations\n",
    "- Continue the face analysis stream by applying the [Facial Age Estimator](https://developer.ibm.com/exchanges/models/all/max-facial-age-estimator/)\n",
    "- For for smaller cropped facial apply the [Image Resolution Enahance](https://developer.ibm.com/exchanges/models/all/max-image-resolution-enhancer/) before proceding to the emotion analysis.\n",
    "- Use the image [Image Caption Generator](https://developer.ibm.com/exchanges/models/all/max-image-caption-generator/) to generate captions for the images\n",
    "- Use the result of the 'Image Caption Generator' to verify captions provided by the submitted, check the translation of captions submitted outside english speaking countries. \n",
    "- Check the sentiment of submitted updated.\n",
    "- On images that faces do not appear use the [Object Detector](https://developer.ibm.com/exchanges/models/all/m[beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) beatuful soup to extract the text being sumbitted and apply \n",
    "[Sentiment Classifier](https://developer.ibm.com/exchanges/models/all/max-text-sentiment-classifier/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
