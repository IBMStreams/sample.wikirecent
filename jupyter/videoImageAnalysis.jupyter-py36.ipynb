{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    print(\"JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"])\n",
    "except KeyError as err:\n",
    "    os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home\"\n",
    "    print(\"Had to set JAVA_HOME is set to\",os.environ[\"JAVA_HOME\"],\"\\n   --- what is going on here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Analysis of Video Frames\n",
    "\n",
    "\n",
    "This is the image processing portion of the [wikiImageAnalysis](wikiImageAnalysis.jupyter-py36.ipynb), deploying only the image collection/analysis portion. The wikiImageAnalysis project walks through the processes of \n",
    "deriving images from the wiki feed, a multistep process. In the instances that you have a supply of images, \n",
    ",video stream or mpeg files, their is no derivation process. \n",
    "\n",
    "This notebook composes and submits three applications that\n",
    "- Receive image/video frames via Kafka.\n",
    "- Uses a Face Detection model locate faces in the frames. \n",
    "- Uses a Object Detection mode to locates objects (person, car, bicycle, boat...)\n",
    "\n",
    "A rendering accompanies the each of the models. \n",
    "\n",
    "The [videoToAnalysis](videoToAnalysis.jupyter-py36.ipynb) notebook publishes video frames to the Kafka\n",
    "to complete the example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='DIRECTORY'>Directory</a>\n",
    "- [Overview ](#OVERVIEW)\n",
    "- [Setup - Helper functions connect to service](#SETUP)\n",
    "    - [Setup Imports](#setupImports)\n",
    "    - [Setup Credentials](#setupCredentials)\n",
    "- [Recieve Video Frames via Kafka - VideoRcvKafka](#PHASE5)\n",
    "- [Image Analysis - Face ](#PHASE4.1)\n",
    "    -  [Render Face ](#PHASE5.1)\n",
    "- [Image Analysis - Object ](#PHASE4.2)\n",
    "    -  [Render Object ](#PHASE5.2)\n",
    "\n",
    "\n",
    "-  [Back to Directroy](#DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"Overview  - Continious processing with Streams\">OVERVIEW</a>\n",
    "![graph of application](images/imageOnlyApplications.png)\n",
    "\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "- [Topology](https://streamsxtopology.readthedocs.io/en/latest/index.html) Streams Topology documentation\n",
    "- [Widgets](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Basics.html) Notebook Widgets documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='SETUP'>Setup</a>\n",
    "\n",
    "## <a id='setupImports'>Setup Imports</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Install components\n",
    "#!pip --user install SSEClient===0.0.22 --upgrade\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports are in support of accessing Streams with Wikipedia data, \n",
    "subsequent are in support of rendering the results as they flow back from \n",
    "Streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "\n",
    "from  functools import lru_cache\n",
    "from statistics import mean\n",
    "import collections\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sseclient import SSEClient as EventSource\n",
    "import requests\n",
    "\n",
    "import streamsx\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "if '../scripts' not in sys.path:\n",
    "    sys.path.insert(0, '../scripts')\n",
    "    \n",
    "import ipynb\n",
    "\n",
    "import streams_aid as aid\n",
    "import streams_render as render\n",
    "import credential\n",
    "import cvsupport\n",
    "print(\"streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions used in interfacing  to Streams  \n",
    "\n",
    "Their are number of helper functions to make it aid in the development of Streams applicatons, refer to scripts/streams_aid.py or utilize.\n",
    "\n",
    "For details on the avaliable aid functions, use the help command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='setupCredentials'>Setup Credentials</a>\n",
    "\n",
    "The service that you use defines what you need to setup.\n",
    "\n",
    "- Describe CP4D\n",
    "- Describe Cloud \n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICP4D setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud setup\n",
    "To use Streams instance running in the cloud setup a [credential.py](setup_credential.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# If your using the cloud set the name of the Cloud Streams instance you will be using. \n",
    "# Credential data \n",
    "SERVICE_NAME='Streaming3Turbine'\n",
    "#SERVICE_NAME='StreamingHourly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Connect to the server :  ICP4D or Cloud instance -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# The code is setup to run in the Cloud and CP4D. \n",
    "# If in the Cloud, you'll see a message.\n",
    "# If in CP4D, you'll need the service credential, which what the above link explains. \n",
    "#    Delete this cell and use the above instructions if you only using CP4D.\n",
    "\n",
    "try:\n",
    "    from icpd_core import icpd_util\n",
    "except ModuleNotFoundError as e:  # get all exceptions\n",
    "    instance,cfg = aid.get_instance(service_name=SERVICE_NAME)\n",
    "\n",
    "else:   # runs when no exception occurs\n",
    "    cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Connected\n",
    "When you're up to this point you have established connection to the Streams instance. The cell below shows, if any, the applications that are running. For the applications that are running thier corresponding rendering codes could be executed without going through the process of submitting the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid.list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next compose and submit the first appication or back to the [Directory](#DIRECTORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thier may not be any jobs currently running. \n",
    "\n",
    "Continue on to compose an application or [back to Directory](#DIRECTORY) \n",
    "\n",
    "<tr style=\"border-bottom: 1px solid #000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View into the live stream\n",
    "The [view](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.view) enables access to live stream at runtime. We spread them liberaly throughout the application to observe how the processing is procedeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing live data:\n",
    "- constructing the view object : https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html?highlight=view#streamsx.topology.topology.Stream.view\n",
    "- methods on the view object : https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html?highlight=view#streamsx.topology.topology.View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Notebook support\n",
    "\n",
    "def render_image(image_url=None, output_region=None):\n",
    "    \"\"\"Write the image into a output region.\n",
    "    \n",
    "    Args::\n",
    "        url: image\n",
    "        output_region: output region\n",
    "        \n",
    "    .. note:: The creation of the output 'stage', if this is not done the image is rendered in the page and\n",
    "        the output region. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        stage = widgets.Output(layout={'border': '1px solid green'})\n",
    "    except:\n",
    "        print(\"Error on request : \", image_url)\n",
    "    else:\n",
    "        if response.status_code == 200:\n",
    "            with output_region:\n",
    "                stage.append_display_data(widgets.Image(\n",
    "                    value=response.content,\n",
    "                    #format='jpg',\n",
    "                    width=300,\n",
    "                    height=400,\n",
    "                ))\n",
    "            output_region.clear_output(wait=True) \n",
    "\n",
    "ana_stage = list()\n",
    "def display_image(tup, image_region=None, title_region=None, url_region=None):\n",
    "    if tup['img_desc'] is not None and len(tup['img_desc']) > 0:\n",
    "        display_desc = tup['img_desc'][0]\n",
    "        ana_stage.append(display_desc)\n",
    "        title_region.value = \"Img Title:{}\".format(display_desc['title'] )\n",
    "        url_region.value = \"{}\".format(display_desc['img'])\n",
    "        render_image(image_url=display_desc['img'], output_region=image_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='PHASE5'>Recieve Video Frames via Kafka - VideoRcvKafka</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import streamsx.eventstreams as eventstreams\n",
    "from streamsx.topology.schema import CommonSchema\n",
    "\n",
    "def VideoRcvKafka():\n",
    "    \"\"\"Recieve video frame on topic and publish to 'image_string'. \n",
    "    \n",
    "    Notes:\n",
    "        - The script VideoSndKafka.py pushed video frame onto the topic.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    topo = Topology(\"VideoRcvKafka\")\n",
    "    topo.add_pip_package('opencv-contrib-python')\n",
    "\n",
    "    video_chunk = eventstreams.subscribe(topo, schema=CommonSchema.Json, \n",
    "                                         credentials=json.loads(credential.magsEventStream),\n",
    "                                         topic='VideoFrame' )\n",
    "    kafka_frame = video_chunk.map(cvsupport.BuildVideoFrame(), name=\"kafka_frame\")\n",
    "    kafka_frame.view(name=\"frame_kafka\", description=\"frame from kafka image\")\n",
    "    kafka_frame.publish(topic=\"image_active\", name=\"pubImageActive\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, VideoRcvKafka(), credential) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Directory](#DIRECTORY) \n",
    "## <a id='PHASE4.1'>Compose and submit 'FaceAnalysis' application</a>\n",
    "![graph of application](images/faceAnalysis.png)\n",
    "\n",
    "- Find faces in images\n",
    "- Less sophisticated Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## compose the findImageElements application\n",
    "import cvsupport\n",
    "\n",
    "def FaceAnalysis():\n",
    "    \"\"\"Analyse the images with various tools. \n",
    "       - Subscribe to 'image_active' with encoded images in 'image_string'\n",
    "       - process 'image_string' with FaceRegions into 'face_regions' \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"FaceAnalysis\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    #topo.add_file_dependency('../datasets/yolov3.weights', 'etc')\n",
    "    #topo.add_file_dependency('../datasets/yolov3.cfg', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')   \n",
    "    \n",
    "    image_active = topo.subscribe(topic=\"image_active\", name=\"subImageActive\")\n",
    "    \n",
    "    # Find faces analysis ....\n",
    "    face_regions = image_active.map(cvsupport.FaceRegions(), name=\"face_regions\")\n",
    "    face_trimmed = face_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'face_regions':t['face_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"faces_trimmed\")\n",
    "    face_trimmed.view(name=\"faces_view\", description=\"faces regions\")\n",
    "    return topo\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, FaceAnalysis(), credential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Directory](#DIRECTORY) \n",
    "## <a id='PHASE4.2'>Compose and submit 'ObjectAnalysis' application</a>\n",
    "![graph of application](images/objectAnalysis.png)\n",
    "\n",
    "- Analysis operator that does analysis: findObject.\n",
    "- More sophisticated model\n",
    "- Bigger longer to submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def ObjectAnalysis():\n",
    "    \"\"\"Analyse the images with various tools. \n",
    "       - Subscribe to 'image_active' with encoded images in 'image_string'\n",
    "       - process 'image_string' with ObjectRegions into 'object_regions'\n",
    "       - \n",
    "       \n",
    "    \"\"\"\n",
    "    topo = Topology(\"ObjectAnalysis\")\n",
    "    topo.add_file_dependency('../datasets/haarcascade_frontalface_default.xml', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.weights', 'etc')\n",
    "    topo.add_file_dependency('../datasets/yolov3.cfg', 'etc')\n",
    "    topo.add_pip_package('opencv-contrib-python')    \n",
    "    image_active = topo.subscribe(topic=\"image_active\", name=\"subImageActive\")\n",
    "    \n",
    "    # Find objects analysis ...\n",
    "    object_regions = image_active.map(cvsupport.ObjectRegions(classes=\"../datasets/yolov3.txt\"), name=\"object_fetch\")\n",
    "    object_trimmed = object_regions.map(lambda t: {\n",
    "        'url':t['img_desc'][0]['img'], \n",
    "        'object_regions':t['object_regions'],\n",
    "        'image_string':t['image_string']\n",
    "    }, name=\"objects_trimmed\")\n",
    "    object_trimmed.view(name=\"objects_view\", description=\"object regions\")\n",
    "    return topo\n",
    "\n",
    "\n",
    "## Compose and submit the findElements Application \n",
    "\n",
    "\n",
    "aid.cloudSubmit(instance, SERVICE_NAME, ObjectAnalysis(), credential) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Directory](#DIRECTORY)\n",
    "## <a id='PHASE5.1'>View the ImageAnalysis application's FacesRegions results - </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SHOW Found Faces\n",
    "face_dash = render.faceDashboard(instance, sleep=2)\n",
    "display(face_dash.dashboard)\n",
    "face_dash.ignition(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## Fetch FacesRegions start/get/stop\n",
    "faces_view = instance.get_views(name=\"faces_view\")[0]\n",
    "faces_view.start_data_fetch()\n",
    "faces_tuples = faces_view.fetch_tuples(max_tuples=100, timeout=10)\n",
    "print(\"Number of faces_tuples:\", len(faces_tuples))\n",
    "faces_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Back to Directory](#DIRECTORY)\n",
    "## <a id='PHASE5.2'>View the ImageAnalysis application's ObjectRegions results</a>\n",
    "Display the images and the objects they found. Run in a thread to make is easier to stop.\n",
    "\n",
    "On the Streams side it's processing the images to find objects, if not no objects are \n",
    "found they are not pushed to the view and will not be rendered here.\n",
    "\n",
    "The object detection maybe CPU intensive (you don't say) you may want to distribute this across hardware, which\n",
    "is left as an exercise to the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "object_dash = render.objectDashboard(instance, sleep=2.0)\n",
    "display(object_dash.dashboard)\n",
    "object_dash.ignition(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Back to Directory](#DIRECTORY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}