{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiRecentPhase2\n",
    "\n",
    "## Streams processing + windowing\n",
    "[WikiRecentPhase1](imgAna_1.jupyter-py36.ipynb) illustrated accessing a Wikipedia continuously with Streams job Using\n",
    "a view to look at the live data and building a UI to fetch view Wikipedia activity. This notebook brings\n",
    "Streams to bear on more of the processing. \n",
    "\n",
    "\n",
    "## Overview \n",
    "**About the sample** \n",
    "\n",
    "The appliction recieves Wikipedia updates as events via SSE, processes events that were edited by humans. \n",
    "The events are staged into windows in order that aggregations can be done over time and events counts.\n",
    "Wikipedia accepts content in variety of language, before doing the time based aggregation the contents\n",
    "declared language is mapped. \n",
    "\n",
    "Since the Streams application runs on the server independent of an open Juypter session it analyzes data\n",
    "over larger windows or time and events. This enables monitoring the events over hours and days to dervive\n",
    "insights into how Wikipedia is modified over time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"setup\"></a>\n",
    "# Setup\n",
    "### Add credentials for the IBM Streams service\n",
    "\n",
    "#### ICPD setup\n",
    "\n",
    "With the cell below selected, click the \"Connect to instance\" button in the toolbar to insert the credentials for the service.\n",
    "\n",
    "<a target=\"blank\" href=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/02/connect_icp4d.gif\">See an example</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from icpd_core import icpd_util\n",
    "cfg=icpd_util.get_service_instance_details(name='zen-sample-icp1-blitz-env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud setup\n",
    "\n",
    "To use Streams instance running in the cloud setup a [credential.py](setup_credential.ipynb)\n",
    "\n",
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Install components\n",
    "!pip install --user SSEClient===0.0.22 --upgrade\n",
    "!pip install --user --upgrade streamsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "from ipywidgets import Button, HBox, VBox, Layout\n",
    "\n",
    "from streamsx.topology.topology import *\n",
    "import streamsx.rest as rest\n",
    "from streamsx.topology import context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def catchInterrupt(func):\n",
    "    \"\"\"decorator : when interupt occurs the display is lost if you don't catch it\n",
    "       TODO * <view>.stop_data_fetch()  # stop\n",
    "       \n",
    "    .\"\"\"\n",
    "    def catch_interrupt(*args, **kwargs):\n",
    "        try: \n",
    "            func(*args, **kwargs)\n",
    "        except (KeyboardInterrupt): pass\n",
    "    return catch_interrupt\n",
    "\n",
    "#\n",
    "# Support for locating/rendering views.\n",
    "def display_view_stop(eventView, period=2):\n",
    "    \"\"\"Wrapper for streamsx.rest_primitives.View.display() to have button. \"\"\"\n",
    "    button =  widgets.Button(description=\"Stop Updating\")\n",
    "    display(button)\n",
    "    eventView.display(period=period) \n",
    "    def on_button_clicked(b):\n",
    "        eventView.stop_data_fetch()\n",
    "        b.description = \"Stopped\"\n",
    "    button.on_click(on_button_clicked)\n",
    "\n",
    "def view_events(views):\n",
    "    \"\"\"\n",
    "    Build interface to display a list of views and \n",
    "    display view when selected from list.\n",
    "     \n",
    "    \"\"\"\n",
    "    view_names = [view.name for view in views]\n",
    "    nameView = dict(zip(view_names, views))    \n",
    "    select = widgets.RadioButtons(\n",
    "        options = view_names,\n",
    "        value = None,\n",
    "        description = 'Select view to display',\n",
    "        disabled = False\n",
    "    )\n",
    "    def on_change(b):\n",
    "        if (b['name'] == 'label'):\n",
    "            clear_output(wait=True)\n",
    "            [view.stop_data_fetch() for view in views ]\n",
    "            display(select)\n",
    "            display_view_stop(nameView[b['new']], period=2)\n",
    "    select.observe(on_change)\n",
    "    display(select)\n",
    "\n",
    "def find_job(instance, job_name=None):\n",
    "    \"\"\"locate job within instance\"\"\"\n",
    "    for job in instance.get_jobs():    \n",
    "        if job.applicationName.split(\"::\")[-1] == job_name:\n",
    "            return job\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def display_views(instance, job_name):\n",
    "    \"Locate/promote and display all views of a job\"\n",
    "    job = find_job(instance, job_name=job_name)\n",
    "    if job is None:\n",
    "        print(\"Failed to locate job\")\n",
    "    else:\n",
    "        views = job.get_views()\n",
    "        view_events(views)\n",
    "\n",
    "def list_jobs(_instance=None, cancel=False):\n",
    "    \"\"\"\n",
    "    Interactive selection of jobs to cancel.\n",
    "    \n",
    "    Prompts with SelectMultiple widget, if thier are no jobs, your presente with a blank list.\n",
    "    \n",
    "    \"\"\"\n",
    "    active_jobs = { \"{}:{}\".format(job.name, job.health):job for job in _instance.get_jobs()}\n",
    "\n",
    "    selectMultiple_jobs = widgets.SelectMultiple(\n",
    "        options=active_jobs.keys(),\n",
    "        value=[],\n",
    "        rows=len(active_jobs),\n",
    "        description = \"Cancel jobs(s)\" if cancel else \"Active job(s):\",\n",
    "        layout=Layout(width='60%')\n",
    "    )\n",
    "    cancel_jobs = widgets.ToggleButton(\n",
    "        value=False,\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Delete selected jobs',\n",
    "        icon=\"stop\"\n",
    "    )\n",
    "    def on_value_change(change):\n",
    "        for job in selectMultiple_jobs.value:\n",
    "            print(\"canceling job:\", job, active_jobs[job].cancel())\n",
    "        cancel_jobs.disabled = True\n",
    "        selectMultiple_jobs.disabled = True\n",
    "\n",
    "    cancel_jobs.observe(on_value_change, names='value')\n",
    "    if cancel:\n",
    "        return HBox([selectMultiple_jobs, cancel_jobs])\n",
    "    else:\n",
    "        return HBox([selectMultiple_jobs])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Support functions are pushed to Streams\n",
    "Details of these functions can be found in previous notesbooks of this suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"fetch recent changes from wikievents site using SSE\"\"\"\n",
    "    for change in EventSource('https://stream.wikimedia.org/v2/stream/recentchange'):\n",
    "        if len(change.data):\n",
    "            try:\n",
    "                obj = json.loads(change.data)\n",
    "            except json.JSONDecodeError as err:\n",
    "                print(\"JSON l1 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            except json.decoder.JSONDecodeError as err:\n",
    "                print(\"JSON l2 error:\", err, \"Invalid JSON:\", change.data)\n",
    "            else:\n",
    "                yield(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the server :  ICP4D or Cloud instance. \n",
    "Attempt to import if fails the cfg will not be defined we know were using \n",
    "Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_instance():\n",
    "    \"\"\"Setup to access your Streams instance.\n",
    "\n",
    "    ..note::The notebook is work within Cloud and ICP4D. \n",
    "            Refer to the 'Setup' cells above.              \n",
    "    Returns:\n",
    "        instance : Access to Streams instance, used for submitting and rendering views.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from icpd_core import icpd_util\n",
    "        import urllib3\n",
    "        global cfg\n",
    "        cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "        instance = rest.Instance.of_service(cfg)\n",
    "        print(\"Within ICP4D\")\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    except ImportError:\n",
    "        cfg = None\n",
    "        print(\"Outside ICP4D\")\n",
    "        import credential  \n",
    "        sc = rest.StreamingAnalyticsConnection(service_name='Streaming3Turbine', \n",
    "                                               vcap_services=credential.vcap_conf)\n",
    "        instance = sc.get_instances()[0]\n",
    "    return instance,cfg\n",
    "\n",
    "instance,cfg = get_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List jobs and cancel....\n",
    "This page will submit a job named 'WikiPhase2'. If it's running you'll want to cancel it before submitting a new version. If it is running, no need to cancel/submit you can just procede to the [Viewing data section](#viewingData)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing the Streams application\n",
    "The following functions that will be executing within the deployed Streams application. \n",
    "The functions a [composed](#composeBuildSubmit) into an application that is compiled and submitted to the Streams instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating tuples\n",
    "The [aggregation](http://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#streamsx.topology.topology.Window.aggregate) which is performed\n",
    "accross a list of tuples. The list is defined by a [window](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Window) which can be count or timed based with \n",
    "various options. \n",
    "in general the function is invoked when the window's\n",
    "critera are met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class sum_aggregation():\n",
    "    def __init__(self, sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }):\n",
    "        \"\"\"\n",
    "        Summation of column(s) over a window's tuples. \n",
    "        Args::\n",
    "            sum_map :  specfify tuple columns to be summed and the result field. \n",
    "            tuples : at run time, list of tuples will flow in. Sum each fields\n",
    "        \"\"\"\n",
    "        self.sum_map = sum_map\n",
    "    def __call__(self, tuples)->dict: \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tuples : list of tuples constituting a window, over all the tuples sum using the sum_map key/value \n",
    "                     to specify the input and result field.\n",
    "        Returns:\n",
    "            dictionary of fields summations over tuples\n",
    "            \n",
    "        \"\"\"\n",
    "        summaries = dict()\n",
    "        for summary_field,result_field in self.sum_map.items():\n",
    "            summation = sum([ele[summary_field] for ele in tuples])\n",
    "            summaries.update({result_field : summation})\n",
    "        return(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tallying fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "class tally_fields(object):\n",
    "    def __init__(self, top_count=3, fields=['user', 'wiki', 'title']):\n",
    "        \"\"\"\n",
    "        Tally fields of a list of tuples.\n",
    "        Args::\n",
    "            fields :  fields of tuples that are to be tallied\n",
    "        \"\"\"\n",
    "        self.fields = fields\n",
    "        self.top_count = top_count\n",
    "    def __call__(self, tuples)->dict:\n",
    "        \"\"\"\n",
    "        Args::\n",
    "            tuples : list of tuples tallying to perform. \n",
    "        return::\n",
    "            dict of tallies\n",
    "        \"\"\"\n",
    "        tallies = dict()\n",
    "        for field in self.fields:\n",
    "            stage = [tuple[field] for tuple in tuples if tuple[field] is not None]\n",
    "            tallies[field] = collections.Counter(stage).most_common(self.top_count)\n",
    "        return tallies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrating testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping tuples\n",
    "\n",
    "The event's 'wiki' field is the database that events pertains to. The database has a primary language which muliple databases share a common language. We'll use aht wikimap.csv to reconcile the events message using the augment_lang\n",
    "function using [map](https://streamsxtopology.readthedocs.io/en/latest/streamsx.topology.topology.html#streamsx.topology.topology.Stream.map) to drive the processing.\n",
    "\n",
    "When defining a class be aware that the __init__() is executed locally at compile invocation when the self value is pickeld. Before call is __call__() invoked at runtime the self is depickeled. In the augmentation_lang.__init__() code \n",
    "below, csv file is read into dict that maps from 'wiki' to 'language' and saved to *self*. At runtime the *self* with it's wiki/language mapping reconsile the language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "class wiki_lang():\n",
    "    \"\"\"\n",
    "    Augment the tuple to include language wiki event.\n",
    "    \n",
    "    Mapping is loaded at build time and utilized at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fname=\"wikimap.csv\"):\n",
    "        self.wiki_map = dict()\n",
    "        with open(fname, mode='r') as csv_file:\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            for row in csv_reader:\n",
    "                self.wiki_map[row['dbname']] = row\n",
    "\n",
    "    def __call__(self, tuple):\n",
    "        \"\"\"using 'wiki' field to look pages code, langauge and native\n",
    "        Args:\n",
    "            tuple: tuple (dict) with a 'wiki' fields\n",
    "        Returns:'\n",
    "            input tuple with  'code', 'language, 'native' fields added to the input tuple.\n",
    "        \"\"\"\n",
    "        if tuple['wiki'] in self.wiki_map:\n",
    "            key = tuple['wiki']\n",
    "            tuple['code'] = self.wiki_map[key]['code']\n",
    "            tuple['language'] = self.wiki_map[key]['in_english']\n",
    "            tuple['native'] = self.wiki_map[key]['name_language']\n",
    "        else:\n",
    "            tuple['code'] = tuple['language'] = tuple['native'] = None\n",
    "        return tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='composeBuildSubmit'></a>\n",
    "## Compose, build and submit the Streams application.\n",
    "The following Code cell composed the Streams application depicted here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stillPhase2.jpg](images/stillPhase2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is application is built upon the previous, I'll only discuss operators beyond 'paredHuman' details of this operator and prior can be found in the previous [notebook](./imgAna_1.ipynb).\n",
    "\n",
    "The events output by the map named 'paredHuman' are limited to those with of 'type' i'edit' and bot is 'False', \n",
    "and a reduced set fields. \n",
    "\n",
    "Three streams and thier views are composed from here. \n",
    "\n",
    "1) The aggregate method named 'talliesTop' is preceded by a windowing operation that stages 100 tuples in increments \n",
    "of 10. 'talliesTop' tuples are processed by the 'tallies_field()' object that tallies the 'user' and 'titles' fields and returns the results in dict. The dict is forwarded onto the view named 'talliesTop'.\n",
    "\n",
    "2) The aggregate method named 'sumAggregation' is preceded by windowing that stages 100 with increment of 20. 'sumAggregation' tuples are processed by 'sum_aggregation()' that returns a dict the view named 'sumAggregate' publishes.\n",
    "\n",
    "3) The map method named 'langAugment' invokes wiki_lang() with each tuple. The wiki_lang() method expands the tuple \n",
    "by three fields 'native', 'code', 'language' keying off the 'wiki' field. The resulting tuple is sent onto aggregate\n",
    "named 'timeLang' is preceded by a windoing operation that stages 2 minutes of tuples in increments of 5. \n",
    "'langAugment' tuples are processed by the 'tallies_fields()' object that tallies the 'langugage' fields and returns\n",
    "the results in a dict. The dict is forwarded onto a view named 'langAugment'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def WikiPhase2(jobName=None):\n",
    "    \"\"\"\n",
    "    UPDATED \n",
    "    -- wiki_lang : csv file mapping database name to langauge\n",
    "\n",
    "    \"\"\"\n",
    "    topo = Topology(name=jobName)\n",
    "    ### make sure we sseclient in Streams environment.\n",
    "    topo.add_pip_package('SSEClient===0.0.22')\n",
    "    \n",
    "    ## wiki events\n",
    "    wiki_events = topo.source(get_events, name=\"wikiEvents\")\n",
    "    ## select events generated by humans\n",
    "    human_filter = wiki_events.filter(lambda x: x['type']=='edit' and x['bot'] is False, name='humanFilter')\n",
    "    # pare down the humans set of columns\n",
    "    pared_human= human_filter.map(lambda x : {'timestamp':x['timestamp'],\n",
    "                                              'new_len':x['length']['new'],\n",
    "                                              'old_len':x['length']['old'], \n",
    "                                              'delta_len':x['length']['new'] - x['length']['old'],\n",
    "                                              'wiki':x['wiki'],'user':x['user'],\n",
    "                                              'title':x['title']}, \n",
    "                        name=\"paredHuman\")\n",
    "    pared_human.view(buffer_time=1.0, sample_size=200, name=\"paredEdits\", description=\"Edits done by humans\")\n",
    "\n",
    "    ## Define window(count)& aggregate\n",
    "    sum_win = pared_human.last(100).trigger(20)\n",
    "    sum_aggregate = sum_win.aggregate(sum_aggregation(sum_map={'new_len':'newSum','old_len':'oldSum','delta_len':'deltaSum' }), name=\"sumAggregate\")\n",
    "    sum_aggregate.view(buffer_time=1.0, sample_size=200, name=\"aggEdits\", description=\"Aggregations of human edits\")\n",
    "\n",
    "    ## Define window(count) & tally edits\n",
    "    tally_win = pared_human.last(100).trigger(10)\n",
    "    tally_top = tally_win.aggregate(tally_fields(fields=['user', 'title'], top_count=10), name=\"talliesTop\")\n",
    "    tally_top.view(buffer_time=1.0, sample_size=200, name=\"talliesCount\", description=\"Top count tallies: user,titles\")\n",
    "\n",
    "    ## augment filterd/pared edits with language\n",
    "    if cfg is None:        \n",
    "        lang_augment = pared_human.map(wiki_lang(fname='../datasets/wikimap.csv'), name=\"langAugment\")\n",
    "    else:\n",
    "        lang_augment = pared_human.map(wiki_lang(fname=os.environ['DSX_PROJECT_DIR']+'/datasets/wikimap.csv'), name=\"langAugment\")\n",
    "    lang_augment.view(buffer_time=1.0, sample_size=200, name=\"langAugment\", description=\"Language derived from wiki\")\n",
    "\n",
    "    ## Define window(time) & tally language\n",
    "    time_lang_win = lang_augment.last(datetime.timedelta(minutes=2)).trigger(5)\n",
    "    time_lang = time_lang_win.aggregate(tally_fields(fields=['language'], top_count=10), name=\"timeLang\")\n",
    "    time_lang.view(buffer_time=1.0, sample_size=200, name=\"talliesTime\", description=\"Top timed tallies: language\")\n",
    "   \n",
    "    return ({\"topo\":topo,\"view\":{ }})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting job : ICP or Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "resp = WikiPhase2(jobName=\"WikiPhase2\")\n",
    "if cfg is not None:\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    submission_result = context.submit(\"DISTRIBUTED\",resp['topo'], config=cfg)\n",
    "\n",
    "if cfg is None:\n",
    "    import credential\n",
    "    cloud = {\n",
    "        context.ConfigParams.VCAP_SERVICES: credential.vcap_conf,\n",
    "        context.ConfigParams.SERVICE_NAME: \"Streaming3Turbine\",\n",
    "        context.ContextTypes.STREAMING_ANALYTICS_SERVICE:\"STREAMING_ANALYTIC\",\n",
    "        context.ConfigParams.FORCE_REMOTE_BUILD: True,\n",
    "    }\n",
    "    submission_result = context.submit(\"STREAMING_ANALYTICS_SERVICE\",resp['topo'],config=cloud)\n",
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    print(\"JobId: \", submission_result['id'] , \"Name: \", submission_result['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viewingData'></a>\n",
    "## Viewing data \n",
    "\n",
    "The running application has number of views to see what what data is moving through the stream. The following \n",
    "cell will fetch the views' queue and dipslay it's data when selected. \n",
    "\n",
    "|view name | description of data is the view | bot |\n",
    "|---------|-------------|------|\n",
    "|aggEdits  | summarised fields | False |\n",
    "|langAugment | mapped augmented fields | False |\n",
    "|paredEdits | seleted fields | False |\n",
    "|talliesCount | last 100 messages tallied | False | \n",
    "|talliesTimes | 2 minute windowed | False |\n",
    "\n",
    "\n",
    "You want to stop the the fetching the view data when done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acces Views / Render Views UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Render the views.....\n",
    "display_views(instance, job_name=\"WikiPhase2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render\n",
    "The Streams application is sending out 5 views which you can seen using the wiget above. The following show the data in more favorable light. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tallied languages\n",
    "\n",
    "We extreact the languge data and tallied it in Streams. This is showing the submitted events pertaining to article written in various languages in the last 2 minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO Cart\n",
    "\n",
    "@catchInterrupt\n",
    "def tally_pie(view=None, tally_field='language', count=10):\n",
    "    \"\"\"Pie chart of language that edits by humans being done in. \n",
    "    Args:\n",
    "        view: Streams view that data will be fetched from\n",
    "        tally_field: fields within view to get data.\n",
    "        count: number of times to fetch data, < 0 until interrupt \n",
    "    \n",
    "    \"\"\"\n",
    "    while (count != 0):\n",
    "        count -= 1\n",
    "        tuples = view.fetch_tuples(max_tuples=10, timeout=10 )\n",
    "        if len(tuples) is 0:\n",
    "            print(\"No new tuples @{}\".format(count))\n",
    "            time.sleep(3)\n",
    "            next\n",
    "        language= [lst[0] for lst in tuples[-1][tally_field]]\n",
    "        counts = [lst[-1] for lst in tuples[-1][tally_field]]\n",
    "        percent = counts[0]/sum(counts) * 100\n",
    "        print(\"[{2}]{0:4.2f}% of the events are in {1}, {1} is  dropped from the piechart.\".format(percent,language[0],\"+*\"[count%2]))\n",
    "        df = pd.DataFrame({'counts': counts[1:]}, index=language[1:])\n",
    "        df.plot.pie(y='counts')    \n",
    "        plt.show()\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "view = instance.get_views(name=\"talliesTime\")[0]\n",
    "view.start_data_fetch()\n",
    "tally_pie(view, tally_field='language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tally users\n",
    "Show the users who have submitted the most events in the last 200 events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "@catchInterrupt\n",
    "def tally_table(view=None, tally_field=\"title\", count=10):\n",
    "    \"\"\"Display a title data in a table\n",
    "    Args:\n",
    "        view: Streams view that data will be fetched from\n",
    "        tally_field: fields within view to get data.\n",
    "        count: number of times to fetch data, < 0 until interrupt \n",
    " \n",
    "    \"\"\"\n",
    "    while (count != 0):\n",
    "        count = count - 1\n",
    "        tallies = view.fetch_tuples(max_tuples=10, timeout=6)\n",
    "        if tallies is not None and len(tallies) != 0:\n",
    "            title_tallies = tallies[0][tally_field]\n",
    "            title = [ele[0] for ele in title_tallies]\n",
    "            cnt = [ele[1] for ele in title_tallies]\n",
    "            tbl = [(tally_field, title),('count', cnt)]\n",
    "            df = pd.DataFrame.from_items(tbl)\n",
    "            display(df)\n",
    "        else:\n",
    "            display(\"Fetch Fail count down value ... {}\".format(count))\n",
    "            time.sleep(3)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "view = instance.get_views(name=\"talliesCount\")[0]\n",
    "view.start_data_fetch()\n",
    "tally_table(view, tally_field=\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tally titles\n",
    "Show the most updated titles within the last 200 events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "view = instance.get_views(name=\"talliesCount\")[0]\n",
    "view.start_data_fetch()\n",
    "tally_table(view, tally_field=\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel jobs when you're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "list_jobs(instance, cancel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook wrap up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook composed and deployed a Streams application that processes live Wikipedia events on a server. We \n",
    "mapped definition and created windows of were aggreagated and pushed out to views where they are rendered. \n",
    "\n",
    "This being a demonstration the windows are contrained. Letting the application run for longer \n",
    "periods and expanding the windows may provide insights. The code in the vicinity of 'window(count)' and 'window(time)' is where window size is manipulated. \n",
    "\n",
    "Since Streams application is running on server, it is not necessary to notebook open. Close the notebook and executing\n",
    "the cells up to 'List jobs and cancel....' will restablish a connection views. Accessing extended views \n",
    "will show active trends. \n",
    "\n",
    "In the next notebook we will continue to build out the notebook, by rendering images as they are subitted to Wikipedia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix \n",
    "\n",
    "## Two tables and a graph.....\n",
    "\n",
    "The goal of this code is to illustrate a dashboard within a notebook realtime rendering Wikipedia activity. \n",
    "This spawns three threads that independently fetchs views and renders them using widgets. \n",
    "Due to the inconsistencies of among browsers, hardware and data connection I've moved this to an appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def tally_out(*args, out=None, tally_field='title'):\n",
    "    \"\"\"\n",
    "    Render tally data: json format of order list of lists. \n",
    "```    \n",
    "{'language': [['English', 1178],\n",
    "   ['French', 39],\n",
    "   ['German', 38],\n",
    "   ['Spanish', 38],\n",
    "   ['Italian', 32],\n",
    "   ['Portuguese', 28],\n",
    "   ['Russian', 27],\n",
    "   ['Swedish', 13],\n",
    "   ['Dutch', 11],\n",
    "   ['Hebrew', 10]]}]\n",
    "```\n",
    "    \"\"\"\n",
    "    tallies = args[0]\n",
    "    assert tally_field in tallies[0], \"Field {} does not exist in input tuples, keys found : {}\".format(tally_field, tallies[0].keys()) \n",
    "    title_tallies = tallies[0][tally_field]\n",
    "    title = [ele[0] for ele in title_tallies]\n",
    "    count = [ele[1] for ele in title_tallies]\n",
    "    tbl = [(tally_field, title),('count', count)]\n",
    "    out.append_display_data(pd.DataFrame.from_dict(dict(tbl)))\n",
    "    out.clear_output(wait=True)\n",
    "\n",
    "def default_out(*args, out=None):\n",
    "    tuples = args[0]\n",
    "    out.append_display_data(pd.DataFrame(tuples))\n",
    "    out.clear_output(wait=True)\n",
    "\n",
    "def pie_out(*args, out=None, tally_field=\"language\", drop_top=True):\n",
    "    \"\"\"Render piechart into Output\n",
    "    drop_top: do not display the highest value element. \n",
    "    \n",
    "    \"\"\"\n",
    "    tuples = args[0]\n",
    "    if drop_top:\n",
    "        start = -1\n",
    "    else:\n",
    "        start = 0\n",
    "    assert tally_field in tuples[0], \"Field {} does not exist in input tuples, keys found : {}\".format(tally_field, tuples[0].keys()) \n",
    "    language= [lst[0] for lst in tuples[start][tally_field]]\n",
    "    counts = [lst[-1] for lst in tuples[start][tally_field]]\n",
    "    percent = counts[0]/sum(counts) * 100\n",
    "    #print(\"[{2}]{0:4.2f}% of the events are in {1},\\n {1} will be dropped from the piechart.\".format(percent,language[0],\"+*\"[idx%2]))\n",
    "    df = pd.DataFrame({'counts': counts[1:]}, index=language[1:])\n",
    "    with out:\n",
    "        plt.show(df.plot.pie(y='counts'))\n",
    "        clear_output(wait=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def graph_display_out(self, duration, period, active, label, lock, transform_func):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import IPython\n",
    "    tqueue = self.start_data_fetch()\n",
    "    end = time.time() + float(duration) if duration is not None else None\n",
    "    max_rows = pd.options.display.max_rows\n",
    "    max_rows = 10\n",
    "    last = 0\n",
    "    idx = 0\n",
    "    try:\n",
    "        while self._data_fetcher and (duration is None or time.time() < end):\n",
    "            idx += 1\n",
    "            # Slow down pace when view is busy\n",
    "            gap = time.time() - last\n",
    "            label.value = \"{0} wait:{1:4.2}\".format(\"-|\"[idx%2],period - gap)\n",
    "            if gap < period:\n",
    "                time.sleep(period - gap)\n",
    "            # Display latest tuples by removing earlier ones\n",
    "            # Avoids display falling behind live data with\n",
    "            # large view buffer\n",
    "            tqs = tqueue.qsize()\n",
    "            if tqs > max_rows:\n",
    "                tqs -= max_rows\n",
    "                for _ in range(tqs):\n",
    "                    try:\n",
    "                        tqueue.get(block=False)\n",
    "                    except queue.Empty:\n",
    "                        break\n",
    "            tuples = self.fetch_tuples(max_rows, None)\n",
    "            if not tuples:\n",
    "                if not self._data_fetcher:\n",
    "                    break\n",
    "                #out.append_stdout('No tuples')\n",
    "            else:\n",
    "                lock.acquire()\n",
    "                transform_func(tuples) ### make call to modfield tally_test\n",
    "                #out.clear_output(wait=True)\n",
    "                lock.release()\n",
    "                #out.append_display_data(pd.DataFrame(tuples))\n",
    "            #out.clear_output(wait=True)\n",
    "            last = time.time()\n",
    "    except Exception as e:\n",
    "        self.stop_data_fetch()\n",
    "        label.value = str(e)\n",
    "        active.value=False\n",
    "        raise e\n",
    "    label.value = \"-\"\n",
    "    self.stop_data_fetch()\n",
    "    active.value=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    view_talliesUser.stop_data_fetch()\n",
    "except NameError as e:\n",
    "    print (\"Views not defined.\", e)\n",
    "\n",
    "try:\n",
    "    view_talliesTitle.stop_data_fetch()\n",
    "except NameError as e:\n",
    "    print (\"Views not defined.\", e)\n",
    "    \n",
    "try:\n",
    "    view_talliesLanguage.stop_data_fetch()\n",
    "except NameError as e:\n",
    "    print (\"Views not defined.\", e)\n",
    "\n",
    "view_talliesUser = instance.get_views(name=\"talliesCount\")[0]\n",
    "print(view_talliesUser.start_data_fetch())\n",
    "view_talliesTitle = instance.get_views(name=\"talliesCount\")[0]\n",
    "print(view_talliesTitle.start_data_fetch())\n",
    "view_talliesLanguage  = instance.get_views(name=\"talliesTime\")[0]\n",
    "print(view_talliesLanguage.start_data_fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import threading\n",
    "from  functools import partial\n",
    "import time\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "self1 = view_talliesUser\n",
    "self2 = view_talliesTitle\n",
    "self3 = view_talliesLanguage\n",
    "\n",
    "duration = 60.0\n",
    "period = 2.0\n",
    "\n",
    "topline = widgets.Text(value=self1.description, description=\"Testing\", disabled=True)\n",
    "active1 = widgets.Valid(value=True, description='users', readout='Stopped')\n",
    "label1 = widgets.Label(value=\"starting\", description=\"status\")\n",
    "active2 = widgets.Valid(value=True, description='titles', readout='Stopped')\n",
    "label2 = widgets.Label(value=\"starting\")\n",
    "active3 = widgets.Valid(value=True, description='languages', readout='Stopped')\n",
    "label3 = widgets.Label(value=\"starting\")\n",
    "user_region = widgets.Output(layout={'border': '1px solid red','width':'30%','height':'270pt'})\n",
    "title_region = widgets.Output(layout={'border': '1px solid black','width':'30%','height':'270pt'})\n",
    "chart_region = widgets.Output(layout={'border': '3px solid orange','width':'60%', \"height\":\"270pt\"})\n",
    "user_region.clear_output(wait=True)\n",
    "title_region.clear_output(wait=True)\n",
    "chart_region.clear_output(wait=True)\n",
    "status = widgets.HBox([topline])\n",
    "activity = widgets.HBox([active1, label1, active2, label2, active3, label3])\n",
    "tables = widgets.HBox([user_region,title_region])\n",
    "piechart = widgets.HBox([chart_region])\n",
    "dashboard = widgets.VBox([status, activity, tables, piechart])\n",
    "display(dashboard)\n",
    "\n",
    "self1._display_thread = threading.Thread(target=lambda: graph_display_out(self1, duration, period, active1, label1, lock, partial(tally_out, tally_field=\"user\", out=user_region) ))\n",
    "self2._display_thread = threading.Thread(target=lambda: graph_display_out(self2, duration, period, active2, label2, lock, partial(tally_out, tally_field=\"title\", out=title_region)))\n",
    "self3._display_thread = threading.Thread(target=lambda: graph_display_out(self3, duration, period, active3, label3, lock, partial(pie_out, out=chart_region)))\n",
    "\n",
    "self1._display_thread.start()\n",
    "self2._display_thread.start()\n",
    "self3._display_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
